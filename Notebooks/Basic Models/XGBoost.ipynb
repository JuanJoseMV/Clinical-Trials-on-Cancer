{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "import qgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCSV(pathSamples, pathMatrix):\n",
    "    df_loaded = pd.read_table(pathMatrix, sep=',')\n",
    "    data = pd.read_table(pathSamples, sep=',')\n",
    "    clin_trial_values = df_loaded.values\n",
    "    columns = df_loaded.columns\n",
    "    \n",
    "    Y = data['Eligible']\n",
    "    Y = Y.astype(int)\n",
    "    X = clin_trial_values[:, :]\n",
    "    return X,Y, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, columns = loadCSV(\"../../Dataset/10k_1Col_NoCarEsp_LSA.csv\", \"../../Tables/docsTopicsLSA1500.csv\") #Cargar SCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=.2) # Modificar metodología de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def xgBoost(X,Y,learning_rate=0.1,  colsample_bytree = 0.3, max_depth = 5, n_estimators = 20, reg_alpha = 10):\n",
    "    \n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=.2)\n",
    "    time_finish = time.time()\n",
    "    # cargamos las 4 combinaciones de las compuertas XOR\n",
    "    training_data = Xtrain\n",
    "\n",
    "    # y estos son los resultados que se obtienen, en el mismo orden\n",
    "    target_data = Ytrain\n",
    "    \n",
    "    model = xgboost.XGBClassifier(objective ='binary:logistic', colsample_bytree = colsample_bytree, learning_rate = learning_rate,\n",
    "                    max_depth = max_depth, n_estimators = n_estimators, booster = 'gbtree', reg_alpha = reg_alpha)\n",
    "\n",
    "\n",
    "    model.fit(training_data, target_data, eval_metric='logloss')\n",
    "    \n",
    "    preds = model.predict(Xtest)\n",
    "    pred_train = model.predict(training_data)\n",
    "    \n",
    "    accuracyTrain = accuracy_score(target_data.values, pred_train)\n",
    "    \n",
    "    accuracyTest = accuracy_score(Ytest.values, preds)\n",
    "\n",
    "    return model, str(time.time() - time_finish), accuracyTrain, accuracyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy train</th>\n",
       "      <th>Accuracy test</th>\n",
       "      <th>Tiempo de ejecución</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.20</th>\n",
       "      <th>0.4</th>\n",
       "      <th>12</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <td>89.25</td>\n",
       "      <td>75.25</td>\n",
       "      <td>7.038994550704956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <th>10</th>\n",
       "      <th>40</th>\n",
       "      <th>100</th>\n",
       "      <td>80.825</td>\n",
       "      <td>71.35</td>\n",
       "      <td>8.253281354904175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <th>0.6</th>\n",
       "      <th>3</th>\n",
       "      <th>10</th>\n",
       "      <th>50</th>\n",
       "      <td>68.425</td>\n",
       "      <td>67.45</td>\n",
       "      <td>1.327953815460205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <th>0.3</th>\n",
       "      <th>4</th>\n",
       "      <th>20</th>\n",
       "      <th>0</th>\n",
       "      <td>82.85</td>\n",
       "      <td>71.5</td>\n",
       "      <td>1.5586626529693604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                Accuracy train  \\\n",
       "learning_rate colsample_bytree max_depth n_estimators reg_alpha                  \n",
       "0.20          0.4              12        40           50                 89.25   \n",
       "              0.8              10        40           100               80.825   \n",
       "0.15          0.6              3         10           50                68.425   \n",
       "0.30          0.3              4         20           0                  82.85   \n",
       "\n",
       "                                                                Accuracy test  \\\n",
       "learning_rate colsample_bytree max_depth n_estimators reg_alpha                 \n",
       "0.20          0.4              12        40           50                75.25   \n",
       "              0.8              10        40           100               71.35   \n",
       "0.15          0.6              3         10           50                67.45   \n",
       "0.30          0.3              4         20           0                  71.5   \n",
       "\n",
       "                                                                Tiempo de ejecución  \n",
       "learning_rate colsample_bytree max_depth n_estimators reg_alpha                      \n",
       "0.20          0.4              12        40           50          7.038994550704956  \n",
       "              0.8              10        40           100         8.253281354904175  \n",
       "0.15          0.6              3         10           50          1.327953815460205  \n",
       "0.30          0.3              4         20           0          1.5586626529693604  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "import time\n",
    "import qgrid\n",
    "\n",
    "randn = np.random.randn\n",
    "df_types = pd.DataFrame({\n",
    "    'learning_rate' : pd.Series([0.2,0.2,0.15,0.3]), \n",
    "    'colsample_bytree' : pd.Series([0.4,0.8,0.6,0.3]), \n",
    "    'max_depth' : pd.Series([12,10,3,4]),\n",
    "    'reg_alpha' : pd.Series([50,100,50,0]),\n",
    "    'n_estimators' : pd.Series([40,40,10,20])})\n",
    "df_types[\"Accuracy train\"] = \"\"\n",
    "df_types[\"Accuracy test\"] = \"\"\n",
    "df_types[\"Tiempo de ejecución\"] = \"\"\n",
    "df_types.set_index(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'reg_alpha'], inplace=True)\n",
    "\n",
    "for learning_rate, colsample_bytree, max_depth, n_estimators, reg_alpha in df_types.index:\n",
    "    modelo, time_finish, accuracyTrain, accuracyTest = xgBoost(X,Y,learning_rate=learning_rate,colsample_bytree = colsample_bytree,\n",
    "                                                              max_depth = max_depth, n_estimators = n_estimators, reg_alpha = reg_alpha)   \n",
    "    \n",
    "    \n",
    "    df_types[\"Accuracy test\"][learning_rate, colsample_bytree, max_depth, n_estimators, reg_alpha] = (accuracyTest * 100.0)\n",
    "    df_types[\"Tiempo de ejecución\"][learning_rate, colsample_bytree, max_depth,n_estimators, reg_alpha] = time_finish\n",
    "    df_types[\"Accuracy train\"][learning_rate, colsample_bytree, max_depth, n_estimators, reg_alpha] = (accuracyTrain * 100.0)\n",
    "    \n",
    "qgrid_widget = qgrid.show_grid(df_types, show_toolbar=False)\n",
    "qgrid_widget.get_changed_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(parameters):\n",
    "    xgb = xgboost.XGBClassifier(objective ='binary:logistic', booster = 'gbtree')\n",
    "    clf = GridSearchCV(xgb, parameters, scoring='f1', n_jobs=-1, return_train_score=True, verbose=1)\n",
    "    clf.fit(X,Y)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search(clf, parameters):\n",
    "    # Get Test Scores Mean and std for each grid search\n",
    "    scores_mean = clf.cv_results_['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(parameters['metric']),len(parameters['K']))\n",
    "\n",
    "    scores_sd = clf.cv_results_['std_test_score']\n",
    "    scores_sd = np.array(scores_sd).reshape(len(parameters['metric']),len(parameters['K']))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1,1, figsize=(20, 10))\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(parameters['metric']):\n",
    "        ax.plot(parameters['K'], scores_mean[idx,:], '-o', label= 'metric' + ': ' + str(val))\n",
    "\n",
    "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel('K', fontsize=16)\n",
    "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "    ax.legend(loc=\"best\", fontsize=15)\n",
    "    ax.grid('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(clf, name):\n",
    "    path = \"../../Models/\" + name + \".pkl\"\n",
    "    joblib.dump(clf, path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   51.5s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed: 39.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed: 67.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed: 92.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  1.97065387,   1.96031594,   1.79910722,   2.58922844,\n",
       "          2.57011857,   2.32763562,   3.20142446,   3.12878194,\n",
       "          2.91485991,   3.49475207,   3.45403371,   3.3169765 ,\n",
       "          2.03805785,   1.87535238,   1.93069558,   2.75424085,\n",
       "          2.62703142,   2.43460207,   3.58398829,   3.39394627,\n",
       "          3.18785815,   4.2417006 ,   3.92867861,   3.63931856,\n",
       "          2.00514731,   1.95610609,   1.81447716,   2.92851944,\n",
       "          2.75983448,   2.51359286,   3.97510695,   3.68075185,\n",
       "          3.40618234,   4.81743469,   4.26180191,   3.96272917,\n",
       "          1.66512527,   1.77743883,   1.75094123,   2.37503247,\n",
       "          2.42062736,   2.29611526,   3.09083991,   3.03744369,\n",
       "          2.87955971,   3.5104918 ,   3.44348211,   3.30484719,\n",
       "          1.90485282,   1.84755673,   1.8232955 ,   2.75135841,\n",
       "          2.54276538,   2.42592487,   3.52051892,   3.37840843,\n",
       "          3.136905  ,   4.17883086,   3.85129509,   3.53527613,\n",
       "          1.95562968,   1.89262671,   1.81584268,   2.92605524,\n",
       "          2.7335485 ,   2.5241569 ,   4.01273422,   3.5673501 ,\n",
       "          3.34854565,   4.71707878,   4.24033632,   3.68995266,\n",
       "          1.65709057,   1.7206574 ,   1.71852117,   2.33856559,\n",
       "          2.37354207,   2.25397358,   3.03792973,   2.98412418,\n",
       "          2.81580267,   3.48627281,   3.51676722,   3.2666492 ,\n",
       "          1.94693208,   1.87041044,   1.8086338 ,   2.6903501 ,\n",
       "          2.57475901,   2.38021307,   3.52582474,   3.33255825,\n",
       "          3.07357316,   4.18904438,   3.86932287,   3.4559226 ,\n",
       "          1.95726061,   1.93055906,   1.84899526,   2.89355817,\n",
       "          2.76291113,   2.43951473,   3.90731668,   3.57700458,\n",
       "          3.21669383,   4.5531002 ,   4.1920496 ,   3.6043788 ,\n",
       "          1.67848597,   1.724016  ,   1.70619082,   2.3476335 ,\n",
       "          2.41816096,   2.23664942,   3.07679029,   2.98864417,\n",
       "          2.808464  ,   3.49844446,   3.46836934,   3.2152163 ,\n",
       "          1.92068672,   1.88152471,   1.79016943,   2.69794898,\n",
       "          2.58054757,   2.30908084,   3.50166092,   3.31170626,\n",
       "          3.02814674,   4.15968027,   3.78906097,   3.45381556,\n",
       "          1.96221232,   1.89963202,   1.82030187,   2.84525447,\n",
       "          2.73951163,   2.36389189,   3.71935468,   3.55479908,\n",
       "          3.11730347,   4.28501639,   4.14507761,   3.40592356,\n",
       "          5.83737288,   5.87278962,   5.63491054,  12.68749347,\n",
       "         12.95417852,  12.29726372,  19.57748957,  20.08990393,\n",
       "         18.87274408,  24.06786933,  24.73704381,  23.21055822,\n",
       "          9.41088386,   9.08125863,   8.40750999,  21.78136039,\n",
       "         20.85362744,  19.01040716,  34.08292632,  32.65147233,\n",
       "         29.60435605,  42.26007853,  40.55599308,  36.64069037,\n",
       "         11.791503  ,  11.2346591 ,  10.4483943 ,  27.71808472,\n",
       "         26.13816023,  23.81718197,  43.63881888,  41.13169756,\n",
       "         37.14990716,  54.15892749,  50.96323991,  46.00424104,\n",
       "          5.79337997,   5.87293205,   5.62367935,  12.70691385,\n",
       "         12.8176724 ,  12.0863625 ,  19.7333353 ,  19.74413042,\n",
       "         18.4584383 ,  24.30295324,  24.23697114,  21.9452765 ,\n",
       "          9.60621724,   9.08175235,   8.37444797,  21.9241014 ,\n",
       "         20.18178535,  18.55417237,  33.7476923 ,  30.58888197,\n",
       "         28.11110015,  41.3399251 ,  37.52688422,  30.64923282,\n",
       "         11.82802024,  11.05755811,  10.26878071,  26.89164329,\n",
       "         24.59045191,  22.92164001,  40.65079632,  37.50124087,\n",
       "         34.26423063,  49.07429357,  45.56751895,  35.58477201,\n",
       "          5.78591614,   5.80251942,   5.53139911,  12.72376366,\n",
       "         12.67447371,  11.78882446,  19.55729609,  19.40893288,\n",
       "         14.60395422,  24.15218215,  23.74188251,  15.57089806,\n",
       "          9.43956676,   8.87488766,   8.14355073,  21.2367547 ,\n",
       "         19.25835199,  17.04178567,  32.03292012,  29.10453782,\n",
       "         19.35954361,  38.66397552,  35.70125117,  20.38990445,\n",
       "         11.5696908 ,  10.75479965,   9.88589301,  25.11710734,\n",
       "         23.25937181,  20.32397027,  36.60584302,  35.32852349,\n",
       "         22.75833645,  43.60878501,  41.04390721,  23.89653115,\n",
       "          5.79813747,   5.8216948 ,   5.50331435,  12.76332226,\n",
       "         12.42315602,   9.57559962,  19.69881711,  18.83925023,\n",
       "         11.14781995,  24.2757453 ,  22.29624124,  12.09320703,\n",
       "          9.32952747,   8.49061522,   8.15950127,  19.65853047,\n",
       "         18.60375175,  13.56879358,  28.14014401,  25.58450255,\n",
       "         14.95208468,  33.01644392,  26.52773771,  15.98154607,\n",
       "         10.96706328,  10.19804578,   9.90221148,  21.63790665,\n",
       "         22.43826647,  15.11189523,  29.93385625,  28.74209275,\n",
       "         16.59214563,  34.80839095,  29.87980471,  17.54498873,\n",
       "          7.30190301,   7.45350256,   7.24116201,  16.46657977,\n",
       "         16.87357736,  16.04012976,  25.42584958,  26.30206299,\n",
       "         24.89926677,  31.53525276,  32.58814769,  30.66111956,\n",
       "         12.16948252,  11.86991243,  11.04697638,  28.66844158,\n",
       "         27.86446676,  25.67719598,  45.25580783,  43.80030012,\n",
       "         39.69526949,  55.7080431 ,  54.01313033,  49.18894482,\n",
       "         15.24599466,  14.77611933,  13.82659116,  36.32180896,\n",
       "         34.80122128,  32.00696459,  57.36687803,  54.95400429,\n",
       "         50.02436256,  71.61096125,  68.13682561,  62.34042635,\n",
       "          7.26279154,   7.3319097 ,   7.07127266,  16.36706896,\n",
       "         16.4303534 ,  15.50994253,  25.37930121,  25.49426932,\n",
       "         23.82420206,  31.49241309,  31.40915227,  27.58305039,\n",
       "         12.13146801,  11.48124661,  10.78655267,  28.2251318 ,\n",
       "         26.17529335,  24.23865523,  43.68865767,  39.84784861,\n",
       "         35.11551447,  53.71470003,  48.77495289,  36.58874092,\n",
       "         15.29650898,  14.27762403,  13.16146159,  34.88240147,\n",
       "         32.28567181,  29.81329031,  52.94793649,  49.12717991,\n",
       "         42.63436756,  64.01352243,  60.21633973,  44.72835774,\n",
       "          7.22994094,   7.29503431,   6.98316798,  16.35557818,\n",
       "         16.26882429,  15.14419408,  25.3465559 ,  25.01809831,\n",
       "         17.98550506,  31.30138893,  30.70676537,  19.13069534,\n",
       "         12.06088147,  11.33720226,  10.52090278,  27.48682179,\n",
       "         25.04843378,  20.71541905,  41.45952492,  37.78633237,\n",
       "         22.60422869,  50.13209825,  45.42216692,  23.73241186,\n",
       "         14.85996256,  13.73162413,  12.98981071,  32.45052519,\n",
       "         30.46180124,  23.04334702,  47.35751843,  46.29705625,\n",
       "         24.93894453,  56.28477821,  51.68325028,  26.09951134,\n",
       "          7.18268743,   7.22639527,   6.85479131,  16.43107386,\n",
       "         16.12330317,  11.49253159,  25.24929762,  24.56208982,\n",
       "         13.30926261,  31.30838966,  28.93354454,  14.49628949,\n",
       "         11.88270078,  10.92424531,   9.95497561,  25.39776745,\n",
       "         23.73650045,  15.11802588,  36.32537708,  31.17647371,\n",
       "         16.84121451,  42.69564018,  32.36361732,  18.01898785,\n",
       "         13.90149693,  13.1073976 ,  11.80247769,  27.86714721,\n",
       "         27.99237819,  17.58829145,  38.89102645,  32.11950517,\n",
       "         19.86743627,  45.19792604,  33.40844393,  21.11076527,\n",
       "         10.29395914,  10.62168298,  10.1450254 ,  23.90634866,\n",
       "         24.66850491,  23.61598172,  37.81126347,  38.36504068,\n",
       "         37.20266733,  46.55878286,  47.68588386,  46.32523761,\n",
       "         17.490242  ,  17.29129019,  16.22158709,  41.81344147,\n",
       "         41.17940416,  38.32231832,  67.09970579,  65.03485866,\n",
       "         60.32329612,  82.61559658,  80.58729253,  75.077037  ,\n",
       "         22.08089886,  21.78040414,  20.39349318,  53.40689883,\n",
       "         52.3451045 ,  48.71260986,  84.90032706,  82.37253881,\n",
       "         76.71528625, 105.51024666, 102.33557687,  95.68038716,\n",
       "         10.14435282,  10.35412245,   9.93617086,  23.36919851,\n",
       "         23.73787913,  22.4073348 ,  36.96432791,  37.14845777,\n",
       "         34.9830173 ,  45.62823076,  46.31771569,  39.47874684,\n",
       "         17.52282228,  16.84677601,  16.0271009 ,  41.21242623,\n",
       "         38.23241673,  35.59497871,  64.37832146,  59.38652735,\n",
       "         48.50928569,  79.10201416,  72.65488062,  49.55187445,\n",
       "         21.99651937,  20.77139883,  19.3533442 ,  51.18640504,\n",
       "         46.78505082,  43.6365098 ,  77.58700099,  72.60581493,\n",
       "         58.36363134,  95.6064877 ,  87.70557742,  60.11094575,\n",
       "         10.41452451,  10.48332577,   9.69984727,  23.8472507 ,\n",
       "         23.5343328 ,  21.37095284,  37.10686769,  36.42682347,\n",
       "         24.40946331,  46.09546223,  44.83344369,  25.84959435,\n",
       "         17.35243044,  16.19878192,  14.78817544,  39.97528191,\n",
       "         36.45932837,  26.50100808,  60.58479843,  55.51100235,\n",
       "         29.07169175,  73.12901464,  62.23509254,  30.72872243,\n",
       "         21.53874378,  20.01287341,  18.24529586,  47.42035737,\n",
       "         44.84966402,  33.23858576,  69.36991825,  67.66737089,\n",
       "         35.85308747,  82.09546762,  70.59024453,  37.56273851,\n",
       "         10.10402327,  10.17452846,   9.59999232,  23.60841012,\n",
       "         23.02266245,  14.17718692,  36.87076416,  34.92966967,\n",
       "         16.74811673,  45.56708822,  37.80235353,  18.43120251,\n",
       "         16.81526828,  15.49002018,  14.47114263,  36.61004829,\n",
       "         34.68825293,  18.98540797,  52.76611886,  43.70641537,\n",
       "         21.58484578,  61.72031107,  45.9299087 ,  23.43473773,\n",
       "         20.20179334,  18.86824193,  17.20130973,  40.34764686,\n",
       "         42.78931403,  22.55614634,  55.92467237,  49.27809825,\n",
       "         25.14775295,  63.96607981,  50.46969719,  23.80775747]),\n",
       " 'std_fit_time': array([0.06075735, 0.03496272, 0.08358721, 0.03763908, 0.05338476,\n",
       "        0.07308833, 0.03786577, 0.08408381, 0.11276263, 0.04503471,\n",
       "        0.07792609, 0.06251055, 0.13478571, 0.06812707, 0.16361578,\n",
       "        0.14832526, 0.02014365, 0.08474645, 0.03560758, 0.05463875,\n",
       "        0.06429384, 0.1051732 , 0.0935182 , 0.09910494, 0.04436225,\n",
       "        0.04722316, 0.04136835, 0.04238779, 0.05248677, 0.01587425,\n",
       "        0.06270825, 0.06228551, 0.04722183, 0.02561581, 0.05602836,\n",
       "        0.11143597, 0.01567339, 0.02273535, 0.01784078, 0.06130893,\n",
       "        0.04109733, 0.04305828, 0.02110577, 0.05334829, 0.02863877,\n",
       "        0.02941338, 0.02269095, 0.08006788, 0.0404409 , 0.04478383,\n",
       "        0.02191158, 0.0318054 , 0.07652823, 0.0354691 , 0.04095685,\n",
       "        0.05122682, 0.04575645, 0.07782721, 0.08203961, 0.07474442,\n",
       "        0.04185103, 0.03635465, 0.02815569, 0.03236068, 0.04448961,\n",
       "        0.03109312, 0.05872964, 0.07098509, 0.02939027, 0.03427567,\n",
       "        0.06796619, 0.06174101, 0.04568417, 0.0308311 , 0.04612998,\n",
       "        0.03316591, 0.05102885, 0.01656851, 0.04677039, 0.06930543,\n",
       "        0.04377602, 0.02590409, 0.03461619, 0.08394999, 0.04953463,\n",
       "        0.05201357, 0.00628079, 0.01459155, 0.03656563, 0.03083662,\n",
       "        0.01988712, 0.06543127, 0.06620162, 0.08047918, 0.11521345,\n",
       "        0.10135261, 0.03542569, 0.04480585, 0.02573407, 0.05018972,\n",
       "        0.04146696, 0.01217255, 0.01807722, 0.07693206, 0.05700721,\n",
       "        0.02479121, 0.07382916, 0.07001003, 0.03666537, 0.04670513,\n",
       "        0.04709054, 0.05321939, 0.05896131, 0.04844621, 0.03096227,\n",
       "        0.0712866 , 0.04358784, 0.04905868, 0.04400973, 0.0824808 ,\n",
       "        0.02822197, 0.03978856, 0.03145315, 0.03357596, 0.03369652,\n",
       "        0.03458485, 0.0372113 , 0.08848084, 0.07663714, 0.07145485,\n",
       "        0.07154584, 0.11525163, 0.0240156 , 0.01430488, 0.02246446,\n",
       "        0.04242925, 0.04329411, 0.03598901, 0.02913573, 0.03812999,\n",
       "        0.09092818, 0.03085008, 0.04570639, 0.06987174, 0.02709293,\n",
       "        0.02448938, 0.04822119, 0.06327395, 0.06088363, 0.09193786,\n",
       "        0.11946945, 0.04927013, 0.13236207, 0.09759105, 0.04562207,\n",
       "        0.19041194, 0.02479264, 0.03687912, 0.09638225, 0.12677193,\n",
       "        0.13160537, 0.19804552, 0.16941685, 0.24922231, 0.40977639,\n",
       "        0.0910497 , 0.11009073, 0.39687652, 0.04620761, 0.07436566,\n",
       "        0.10951006, 0.07380899, 0.09554384, 0.33053962, 0.06897594,\n",
       "        0.10381415, 0.42507534, 0.09006348, 0.20408377, 0.4746469 ,\n",
       "        0.02764043, 0.02741185, 0.04360177, 0.09538553, 0.03390466,\n",
       "        0.11240704, 0.0936517 , 0.09650959, 0.13315904, 0.17153825,\n",
       "        0.08187122, 0.77044756, 0.06449824, 0.03233343, 0.10629235,\n",
       "        0.06487265, 0.05245773, 0.12936279, 0.1743037 , 0.32112829,\n",
       "        0.44975441, 0.08301738, 0.47424197, 1.42629213, 0.08174841,\n",
       "        0.11945237, 0.10623433, 0.08163987, 0.1590105 , 0.21522347,\n",
       "        0.15327358, 0.25268758, 0.63832595, 0.21240089, 0.34663327,\n",
       "        1.00392049, 0.02406046, 0.02545324, 0.02738944, 0.02687091,\n",
       "        0.06761608, 0.07824205, 0.06150171, 0.09891742, 0.55105791,\n",
       "        0.03739337, 0.09491985, 0.49601554, 0.06788057, 0.03392225,\n",
       "        0.05371941, 0.09304166, 0.16832755, 0.79436443, 0.22396996,\n",
       "        0.22237388, 1.16353226, 0.13330261, 0.15311352, 1.15949599,\n",
       "        0.04742679, 0.11720374, 0.21697137, 0.13126943, 0.42960178,\n",
       "        0.71148574, 0.14040628, 0.60038032, 1.88821288, 0.0565372 ,\n",
       "        0.97096657, 2.12302103, 0.08117399, 0.07790626, 0.02135279,\n",
       "        0.16778449, 0.05265106, 0.42676259, 0.2061001 , 0.06310407,\n",
       "        0.45378235, 0.23841258, 0.41119049, 0.41076611, 0.13495587,\n",
       "        0.06408743, 0.11600816, 0.25565185, 0.2693378 , 1.05328884,\n",
       "        0.1709658 , 0.93894509, 0.98350668, 0.20495517, 0.93227419,\n",
       "        0.94687198, 0.05682813, 0.10104299, 0.12994373, 0.26732595,\n",
       "        0.11089326, 2.01485838, 0.25325781, 2.04615094, 2.09087043,\n",
       "        0.16288568, 2.02894854, 2.02162627, 0.01420787, 0.02515666,\n",
       "        0.07822367, 0.10907085, 0.05659817, 0.13837354, 0.09663376,\n",
       "        0.20369774, 0.16117072, 0.09150919, 0.10956821, 0.19530516,\n",
       "        0.07007182, 0.10483703, 0.09796444, 0.19479469, 0.27470411,\n",
       "        0.2666299 , 0.35192282, 0.66685345, 0.31866491, 0.19321382,\n",
       "        0.35301543, 0.35866958, 0.05392549, 0.08991048, 0.15300619,\n",
       "        0.24234619, 0.1637869 , 0.49662403, 0.27888492, 0.46362337,\n",
       "        0.55567779, 0.33961343, 0.36437215, 0.54724699, 0.03308226,\n",
       "        0.04184951, 0.0462446 , 0.14492966, 0.09492845, 0.06174416,\n",
       "        0.14220459, 0.10079049, 0.06780735, 0.24019654, 0.14141563,\n",
       "        0.98404223, 0.07088868, 0.04804956, 0.08571764, 0.0411086 ,\n",
       "        0.12960127, 0.12311667, 0.06140078, 0.33163512, 0.70749426,\n",
       "        0.04781186, 0.20216178, 0.84839219, 0.0626694 , 0.06859694,\n",
       "        0.18305711, 0.0434288 , 0.20778017, 0.31248138, 0.22824323,\n",
       "        0.3642831 , 2.04960106, 0.19653802, 0.41300122, 2.52934132,\n",
       "        0.05017513, 0.0178111 , 0.04675897, 0.11896947, 0.07723906,\n",
       "        0.26248149, 0.09899715, 0.13122946, 1.00801081, 0.06036741,\n",
       "        0.12003365, 0.99020648, 0.07941599, 0.06758989, 0.07388162,\n",
       "        0.18162104, 0.09840194, 0.656413  , 0.08279861, 0.16078564,\n",
       "        0.73486191, 0.31921306, 0.79035311, 0.76530152, 0.05129575,\n",
       "        0.10919098, 0.22809717, 0.08180992, 0.23947559, 1.42613722,\n",
       "        0.27720057, 0.23793815, 1.38558034, 0.31869136, 2.58716922,\n",
       "        1.41861025, 0.05143112, 0.06098683, 0.06893595, 0.13855619,\n",
       "        0.13810849, 1.06239567, 0.1265308 , 0.31163874, 1.06513058,\n",
       "        0.12034035, 1.10717721, 1.11874936, 0.11421695, 0.20390736,\n",
       "        0.51692469, 0.37432109, 0.1128405 , 1.69279281, 0.14569076,\n",
       "        0.88955326, 1.82978604, 0.33037088, 0.91718787, 1.78574526,\n",
       "        0.10969889, 0.20328659, 0.84992644, 0.21738092, 0.2005138 ,\n",
       "        2.3801289 , 0.55216215, 0.23515547, 2.83927889, 0.57442169,\n",
       "        0.25464113, 2.64176874, 0.1043063 , 0.12617915, 0.08208697,\n",
       "        0.25200064, 0.40123336, 0.13267671, 0.41960727, 0.17703908,\n",
       "        0.50586241, 0.60767595, 0.25404252, 0.82614829, 0.04862583,\n",
       "        0.08629127, 0.13049443, 0.10085758, 0.18763299, 0.33720916,\n",
       "        1.05694346, 0.55455343, 0.52977644, 0.26690722, 0.49563887,\n",
       "        0.48354556, 0.04629575, 0.17083887, 0.21496429, 0.18878044,\n",
       "        0.35708802, 0.39903101, 0.39461748, 0.40733045, 0.36084578,\n",
       "        0.12446059, 0.39936129, 0.55642441, 0.03497764, 0.06668054,\n",
       "        0.05845048, 0.05287283, 0.131891  , 0.07726787, 0.46326755,\n",
       "        0.13851895, 0.53914574, 0.16148859, 0.71161259, 1.35120261,\n",
       "        0.06112885, 0.3603821 , 0.07992238, 0.12969152, 0.62666695,\n",
       "        0.60873426, 0.84903695, 0.93124209, 2.42120899, 1.06393001,\n",
       "        1.21158884, 2.28110099, 0.10117417, 0.12382377, 0.39686042,\n",
       "        0.11466855, 0.16166868, 0.53619631, 0.776936  , 0.93270324,\n",
       "        4.55933146, 1.75720014, 0.38331218, 3.85773676, 0.15166109,\n",
       "        0.16599387, 0.07557482, 0.54242562, 0.05436236, 0.52125113,\n",
       "        0.46113371, 0.09913662, 0.83412374, 0.37887712, 0.03701636,\n",
       "        0.86977402, 0.0680714 , 0.05670424, 0.14207059, 0.07487699,\n",
       "        0.29148846, 1.27342723, 0.15316813, 0.47232569, 1.35703194,\n",
       "        0.24452563, 1.8669368 , 1.29801241, 0.1228777 , 0.13281302,\n",
       "        0.22226524, 0.15341021, 0.16523929, 1.82507211, 0.3060374 ,\n",
       "        0.51500147, 1.80739849, 0.15551103, 2.08483892, 1.84158101,\n",
       "        0.03646309, 0.03839335, 0.07659793, 0.1871657 , 0.12102892,\n",
       "        0.76050731, 0.25022306, 0.84043082, 0.71871088, 0.14694884,\n",
       "        1.56599138, 0.75948582, 0.07204508, 0.18021442, 0.1523412 ,\n",
       "        0.14314773, 0.46863046, 1.12075052, 0.63326793, 4.0122503 ,\n",
       "        1.2073598 , 0.66845621, 3.78153145, 1.16283522, 0.25833342,\n",
       "        0.21229835, 1.14808969, 0.28633342, 0.54481793, 0.91384517,\n",
       "        0.42723745, 1.87202186, 1.03112277, 1.14649042, 1.47218161,\n",
       "        2.56265145]),\n",
       " 'mean_score_time': array([0.08596539, 0.08309441, 0.07311091, 0.06962733, 0.07294378,\n",
       "        0.05653453, 0.06153345, 0.06126161, 0.06177359, 0.062465  ,\n",
       "        0.06347437, 0.06132574, 0.07649317, 0.05098705, 0.0609499 ,\n",
       "        0.0644248 , 0.05670056, 0.07151136, 0.06906343, 0.06877656,\n",
       "        0.0603404 , 0.07111897, 0.062185  , 0.05482049, 0.06304984,\n",
       "        0.06707001, 0.07024846, 0.06798272, 0.06740575, 0.06359735,\n",
       "        0.07696781, 0.06624613, 0.05510855, 0.07845082, 0.07002172,\n",
       "        0.06345534, 0.06232243, 0.05896945, 0.07609301, 0.06520004,\n",
       "        0.06938138, 0.06777682, 0.06031618, 0.06357183, 0.06082416,\n",
       "        0.0626018 , 0.06306696, 0.06118741, 0.06248288, 0.06565518,\n",
       "        0.05184388, 0.06919804, 0.06854267, 0.05469699, 0.06897607,\n",
       "        0.06807728, 0.0601676 , 0.06804099, 0.06173964, 0.06107669,\n",
       "        0.06330109, 0.05661197, 0.06004505, 0.06488085, 0.06621914,\n",
       "        0.05457902, 0.08050237, 0.06707525, 0.05977159, 0.06882248,\n",
       "        0.07779274, 0.05832219, 0.05998955, 0.05959477, 0.06791935,\n",
       "        0.06256046, 0.06435924, 0.06427746, 0.05815825, 0.06364851,\n",
       "        0.0538857 , 0.05800734, 0.05793152, 0.05680552, 0.05825319,\n",
       "        0.04882092, 0.04787641, 0.06455145, 0.08872266, 0.07543659,\n",
       "        0.07027359, 0.07469282, 0.05570869, 0.06753855, 0.06567278,\n",
       "        0.05619621, 0.0664669 , 0.05292711, 0.05625467, 0.05992198,\n",
       "        0.06076102, 0.05490847, 0.07021055, 0.06325893, 0.05537195,\n",
       "        0.066607  , 0.06525335, 0.05925045, 0.05632119, 0.06026301,\n",
       "        0.06967173, 0.06177144, 0.0638092 , 0.05542727, 0.0600049 ,\n",
       "        0.06608224, 0.05444551, 0.0565959 , 0.05640912, 0.05703435,\n",
       "        0.05375872, 0.0547349 , 0.05104318, 0.05705466, 0.05865798,\n",
       "        0.05947938, 0.07638369, 0.06712003, 0.05941377, 0.06930861,\n",
       "        0.06664209, 0.05675707, 0.06716542, 0.05607061, 0.05976067,\n",
       "        0.06525426, 0.06568356, 0.0641386 , 0.06952724, 0.06818624,\n",
       "        0.05890975, 0.06116877, 0.06150088, 0.0543819 , 0.05567803,\n",
       "        0.05189385, 0.05349908, 0.05613236, 0.05784698, 0.05594888,\n",
       "        0.05734749, 0.05744309, 0.05656171, 0.05792503, 0.05865102,\n",
       "        0.0557137 , 0.05780888, 0.05571728, 0.05364084, 0.05593729,\n",
       "        0.05502343, 0.05722075, 0.06071734, 0.05673871, 0.05613861,\n",
       "        0.06389856, 0.05887275, 0.05939217, 0.05604672, 0.05682597,\n",
       "        0.05344243, 0.0606256 , 0.05734634, 0.05843706, 0.06359448,\n",
       "        0.05830112, 0.05862002, 0.06493673, 0.06360416, 0.05896463,\n",
       "        0.05518088, 0.05577302, 0.05493002, 0.05725722, 0.0591435 ,\n",
       "        0.05559511, 0.05809979, 0.05947719, 0.05496044, 0.06093593,\n",
       "        0.05559359, 0.05673761, 0.05803308, 0.05898952, 0.06451883,\n",
       "        0.06242418, 0.05195656, 0.06258726, 0.06217098, 0.05739045,\n",
       "        0.05744328, 0.06454535, 0.05773797, 0.05540714, 0.05711045,\n",
       "        0.0587585 , 0.05640001, 0.05948567, 0.05653701, 0.05633616,\n",
       "        0.06261959, 0.06008563, 0.05774112, 0.06500273, 0.06025963,\n",
       "        0.05888381, 0.05392027, 0.05489426, 0.05487061, 0.05739665,\n",
       "        0.06004977, 0.0553823 , 0.05971742, 0.05863729, 0.05593185,\n",
       "        0.05845556, 0.05905023, 0.05738025, 0.05426903, 0.05285053,\n",
       "        0.05486155, 0.05839181, 0.05720577, 0.05791669, 0.06079202,\n",
       "        0.05791965, 0.05598469, 0.06291103, 0.05969148, 0.05695834,\n",
       "        0.0564765 , 0.0572402 , 0.05629954, 0.06186008, 0.05624018,\n",
       "        0.05566955, 0.0660121 , 0.0591238 , 0.05692997, 0.06108761,\n",
       "        0.0600625 , 0.05599852, 0.0555541 , 0.05581617, 0.05380201,\n",
       "        0.05854082, 0.05480566, 0.05802822, 0.05954413, 0.0590034 ,\n",
       "        0.05612941, 0.05337696, 0.05881195, 0.05277886, 0.05619226,\n",
       "        0.05601559, 0.05514026, 0.06042185, 0.0556076 , 0.05703244,\n",
       "        0.06080689, 0.05538716, 0.05605979, 0.06324863, 0.05764184,\n",
       "        0.05705729, 0.0615212 , 0.05592837, 0.05430541, 0.05633173,\n",
       "        0.05821257, 0.05571804, 0.06209903, 0.06034403, 0.05578489,\n",
       "        0.06406603, 0.05859551, 0.05695038, 0.05623765, 0.05492454,\n",
       "        0.05630341, 0.05552135, 0.05530949, 0.05630403, 0.05876536,\n",
       "        0.05643945, 0.05729489, 0.0559186 , 0.05817428, 0.05792518,\n",
       "        0.05538945, 0.05600219, 0.05501394, 0.05607295, 0.0581151 ,\n",
       "        0.05413666, 0.06143103, 0.05936174, 0.05757852, 0.06300063,\n",
       "        0.05624151, 0.05703864, 0.05319033, 0.05515723, 0.05537114,\n",
       "        0.05946774, 0.05910077, 0.05654774, 0.06192994, 0.06357026,\n",
       "        0.05992713, 0.06668248, 0.06095662, 0.06042571, 0.05614243,\n",
       "        0.05670333, 0.0596365 , 0.06027412, 0.05413618, 0.05324187,\n",
       "        0.05851994, 0.05570121, 0.05835843, 0.05630455, 0.05746036,\n",
       "        0.05770502, 0.05711126, 0.0586132 , 0.05534539, 0.05951338,\n",
       "        0.05886827, 0.05852184, 0.06236043, 0.06040878, 0.05884209,\n",
       "        0.06063814, 0.06167545, 0.05942945, 0.05917544, 0.05639224,\n",
       "        0.05641398, 0.06111717, 0.05679855, 0.05722995, 0.06268983,\n",
       "        0.05818949, 0.05837183, 0.06514964, 0.06146212, 0.06023965,\n",
       "        0.05497918, 0.05459266, 0.05619631, 0.05736399, 0.05609651,\n",
       "        0.05827432, 0.05882864, 0.05910473, 0.05589614, 0.05673289,\n",
       "        0.05970693, 0.05611405, 0.05614996, 0.05674925, 0.05301609,\n",
       "        0.06042666, 0.05464129, 0.0553833 , 0.05860662, 0.0569397 ,\n",
       "        0.05437317, 0.06492863, 0.06626463, 0.05726824, 0.05548306,\n",
       "        0.05601392, 0.0545752 , 0.05820723, 0.05835209, 0.05677285,\n",
       "        0.06446872, 0.05971103, 0.05556931, 0.06064777, 0.06107364,\n",
       "        0.057406  , 0.0545064 , 0.05334091, 0.05364537, 0.05699744,\n",
       "        0.05610986, 0.0556406 , 0.05855584, 0.05978904, 0.05675473,\n",
       "        0.0602562 , 0.05665541, 0.05971389, 0.05840654, 0.05645556,\n",
       "        0.05374527, 0.0579855 , 0.05885005, 0.0545248 , 0.06012616,\n",
       "        0.05778289, 0.05502853, 0.06195002, 0.05797882, 0.05650454,\n",
       "        0.05638161, 0.05470777, 0.05443988, 0.05917864, 0.05718732,\n",
       "        0.0559248 , 0.0607882 , 0.06354871, 0.05626774, 0.06584463,\n",
       "        0.05740728, 0.05339932, 0.05488596, 0.05228748, 0.05351048,\n",
       "        0.05569553, 0.06813269, 0.0558341 , 0.05498409, 0.05720825,\n",
       "        0.05531464, 0.05803905, 0.05736108, 0.0580368 , 0.05533347,\n",
       "        0.05509615, 0.05491238, 0.05699229, 0.05774608, 0.05389223,\n",
       "        0.06070366, 0.06146588, 0.05782871, 0.06324816, 0.06218758,\n",
       "        0.05907707, 0.05728755, 0.05630455, 0.0542551 , 0.05865817,\n",
       "        0.05718303, 0.05568657, 0.06068139, 0.05954356, 0.05919666,\n",
       "        0.06437635, 0.06209846, 0.06063361, 0.05453563, 0.05373578,\n",
       "        0.05391297, 0.05469799, 0.05603728, 0.05434251, 0.05748796,\n",
       "        0.05847731, 0.05469413, 0.05795379, 0.05840669, 0.05311832,\n",
       "        0.05476742, 0.05536184, 0.05382152, 0.05903583, 0.05883183,\n",
       "        0.05606322, 0.06086893, 0.05993567, 0.05746489, 0.06117229,\n",
       "        0.05886927, 0.05793419, 0.05761766, 0.05747714, 0.05407906,\n",
       "        0.05879045, 0.05810595, 0.05609655, 0.06220589, 0.05928674,\n",
       "        0.05803351, 0.06486483, 0.06325207, 0.05757689, 0.05626779,\n",
       "        0.05564499, 0.05555735, 0.05701089, 0.05953164, 0.05650744,\n",
       "        0.0583447 , 0.06026058, 0.057093  , 0.05990248, 0.05963559,\n",
       "        0.05555944, 0.05709071, 0.05628567, 0.05591912, 0.05991163,\n",
       "        0.05764794, 0.05675931, 0.05949616, 0.0586309 , 0.05532403,\n",
       "        0.06299362, 0.05863743, 0.05560241, 0.05300894, 0.05406551,\n",
       "        0.0568243 , 0.06008492, 0.05536876, 0.05660601, 0.06021852,\n",
       "        0.05972252, 0.05567389, 0.06330299, 0.0601254 , 0.05725875,\n",
       "        0.05416732, 0.05545993, 0.05694842, 0.05551419, 0.05736704,\n",
       "        0.05566864, 0.05511298, 0.06122408, 0.05680623, 0.05979261,\n",
       "        0.05773125, 0.05507307, 0.05422416, 0.05490403, 0.05414066,\n",
       "        0.05811548, 0.05545244, 0.05434976, 0.06066594, 0.05931258,\n",
       "        0.0546206 , 0.06157327, 0.05819893, 0.05401444, 0.05582151,\n",
       "        0.0555903 , 0.05412107, 0.05703654, 0.05726995, 0.05419221,\n",
       "        0.06025381, 0.05935407, 0.05638123, 0.04690905, 0.05550222,\n",
       "        0.04653635]),\n",
       " 'std_score_time': array([0.01577325, 0.01161338, 0.0088734 , 0.00301786, 0.0042371 ,\n",
       "        0.00446513, 0.00563543, 0.00653301, 0.00751607, 0.00431268,\n",
       "        0.00773071, 0.01309073, 0.02278079, 0.00278674, 0.00485808,\n",
       "        0.00417386, 0.00408434, 0.02354859, 0.00447067, 0.01019393,\n",
       "        0.00218443, 0.00369964, 0.00600226, 0.00458845, 0.00366763,\n",
       "        0.00606203, 0.01431464, 0.00716524, 0.00077275, 0.00790899,\n",
       "        0.00780418, 0.0041446 , 0.00686082, 0.00781418, 0.0068293 ,\n",
       "        0.00315931, 0.00612873, 0.00664468, 0.01764967, 0.00674631,\n",
       "        0.00572562, 0.00881783, 0.00702379, 0.0090603 , 0.00727701,\n",
       "        0.00461625, 0.00995938, 0.01038673, 0.00918102, 0.01657149,\n",
       "        0.00803065, 0.02148978, 0.01075624, 0.00461577, 0.00511935,\n",
       "        0.01041119, 0.00539313, 0.00384957, 0.00748026, 0.0064415 ,\n",
       "        0.00351838, 0.00425089, 0.00520381, 0.00782723, 0.00336564,\n",
       "        0.00441582, 0.00964638, 0.01094802, 0.00244482, 0.00462779,\n",
       "        0.02887265, 0.00770107, 0.00410608, 0.00646341, 0.01008285,\n",
       "        0.00486742, 0.00498928, 0.00600078, 0.00544001, 0.00708207,\n",
       "        0.00475847, 0.00493637, 0.00572215, 0.00904038, 0.00775097,\n",
       "        0.00097855, 0.00223894, 0.0041797 , 0.02540762, 0.0228102 ,\n",
       "        0.00311233, 0.01379406, 0.00385137, 0.00136543, 0.00804517,\n",
       "        0.00976385, 0.0041606 , 0.00596519, 0.00703154, 0.00349199,\n",
       "        0.00668546, 0.0047779 , 0.00322662, 0.00661316, 0.00622395,\n",
       "        0.00421484, 0.00970796, 0.00519002, 0.00843146, 0.01105961,\n",
       "        0.01022215, 0.007393  , 0.00672747, 0.00640398, 0.00326786,\n",
       "        0.008453  , 0.00453776, 0.00427796, 0.00726209, 0.00829524,\n",
       "        0.00770835, 0.00775504, 0.00468206, 0.00525896, 0.00698806,\n",
       "        0.00651847, 0.00537962, 0.01001829, 0.00554543, 0.0043419 ,\n",
       "        0.01050497, 0.00717621, 0.00463053, 0.00629812, 0.01008666,\n",
       "        0.00386077, 0.00742025, 0.00743306, 0.00438526, 0.01058556,\n",
       "        0.0021016 , 0.00437439, 0.00658625, 0.003682  , 0.00231987,\n",
       "        0.0033903 , 0.00394027, 0.00117901, 0.00059057, 0.00157124,\n",
       "        0.00092348, 0.00182767, 0.00137662, 0.00170754, 0.00128562,\n",
       "        0.00391779, 0.00359709, 0.00167195, 0.00126964, 0.00385242,\n",
       "        0.00428151, 0.0030692 , 0.00146949, 0.00564459, 0.00400073,\n",
       "        0.002636  , 0.00049121, 0.00202106, 0.00186639, 0.00302216,\n",
       "        0.00453116, 0.00512733, 0.00516518, 0.00294788, 0.00191552,\n",
       "        0.00395223, 0.00107698, 0.00278643, 0.00148265, 0.00449985,\n",
       "        0.00273718, 0.00107735, 0.00279202, 0.00181746, 0.00357232,\n",
       "        0.00486213, 0.00153427, 0.00298526, 0.00330359, 0.00241776,\n",
       "        0.00364492, 0.00427176, 0.00251617, 0.00285663, 0.01661034,\n",
       "        0.002357  , 0.0038331 , 0.00466789, 0.00192725, 0.00456356,\n",
       "        0.0017283 , 0.00695054, 0.00309616, 0.0037225 , 0.00139682,\n",
       "        0.00811227, 0.00276741, 0.00163622, 0.00468448, 0.00464669,\n",
       "        0.00419069, 0.00212204, 0.00094182, 0.00289355, 0.00115296,\n",
       "        0.00064558, 0.00086759, 0.00161989, 0.0028866 , 0.00198017,\n",
       "        0.00287267, 0.00424931, 0.00311809, 0.00222309, 0.00113403,\n",
       "        0.00152919, 0.00372665, 0.00208433, 0.00341202, 0.00485988,\n",
       "        0.00548138, 0.00144335, 0.00265586, 0.00147126, 0.00268572,\n",
       "        0.00114566, 0.000738  , 0.00465568, 0.0052636 , 0.00117434,\n",
       "        0.00235447, 0.00331325, 0.00237323, 0.00304151, 0.00400462,\n",
       "        0.00126828, 0.00357272, 0.00537475, 0.00128943, 0.00394169,\n",
       "        0.00459004, 0.0036844 , 0.00149227, 0.00126341, 0.00400266,\n",
       "        0.00284603, 0.00353499, 0.00223011, 0.00228707, 0.00104894,\n",
       "        0.00505262, 0.00446326, 0.00040787, 0.00408761, 0.001691  ,\n",
       "        0.001383  , 0.00237901, 0.00480779, 0.00351828, 0.00285921,\n",
       "        0.00542297, 0.00447469, 0.00278985, 0.00201225, 0.00163935,\n",
       "        0.0017545 , 0.00249585, 0.00196726, 0.00495121, 0.00342559,\n",
       "        0.00307574, 0.00156248, 0.0020806 , 0.00197525, 0.00118068,\n",
       "        0.00172433, 0.00124768, 0.0023069 , 0.00318228, 0.00359834,\n",
       "        0.00255458, 0.00079495, 0.00375771, 0.00142796, 0.0019561 ,\n",
       "        0.00413576, 0.0021705 , 0.00417412, 0.00099666, 0.00199119,\n",
       "        0.00143092, 0.0024541 , 0.00068571, 0.00082883, 0.00220178,\n",
       "        0.00474861, 0.00272092, 0.00187905, 0.00634979, 0.00182272,\n",
       "        0.00370558, 0.0031727 , 0.00375098, 0.00134067, 0.0015326 ,\n",
       "        0.00305664, 0.00366342, 0.00101885, 0.00514054, 0.00483547,\n",
       "        0.00403994, 0.00252814, 0.00045448, 0.00164928, 0.00267307,\n",
       "        0.00372316, 0.01025492, 0.0050506 , 0.00301914, 0.00334455,\n",
       "        0.00288394, 0.00468371, 0.00144781, 0.00448593, 0.00496118,\n",
       "        0.00480459, 0.00255707, 0.00462401, 0.00143396, 0.0037837 ,\n",
       "        0.00422619, 0.00455085, 0.00538701, 0.00271571, 0.00070753,\n",
       "        0.00480433, 0.00196187, 0.0016216 , 0.00250369, 0.00174449,\n",
       "        0.00076467, 0.00269924, 0.00134856, 0.00202441, 0.00160872,\n",
       "        0.00373776, 0.00086512, 0.00138133, 0.00128377, 0.00225775,\n",
       "        0.00199166, 0.00204594, 0.0031964 , 0.00248809, 0.00169439,\n",
       "        0.00063198, 0.00233541, 0.00134777, 0.00142453, 0.00368132,\n",
       "        0.00213449, 0.00118471, 0.00555032, 0.00294445, 0.00448767,\n",
       "        0.00353846, 0.0033044 , 0.00411719, 0.00362664, 0.00432972,\n",
       "        0.00438127, 0.00330378, 0.01100577, 0.00100075, 0.00351906,\n",
       "        0.00151356, 0.00437956, 0.00076308, 0.00135188, 0.00348696,\n",
       "        0.00285624, 0.00174614, 0.00207178, 0.00364701, 0.00304582,\n",
       "        0.0020924 , 0.00157234, 0.00390322, 0.00318961, 0.00084692,\n",
       "        0.00409186, 0.00119144, 0.00451977, 0.00222432, 0.00130806,\n",
       "        0.00074412, 0.00485578, 0.0036204 , 0.00371829, 0.00079145,\n",
       "        0.00468583, 0.00177594, 0.00174443, 0.00045521, 0.00165959,\n",
       "        0.00128008, 0.00125872, 0.00114642, 0.00212298, 0.0015011 ,\n",
       "        0.00193231, 0.00135463, 0.00078385, 0.00172714, 0.00191005,\n",
       "        0.00223308, 0.00269238, 0.0161099 , 0.00129133, 0.01257743,\n",
       "        0.00153359, 0.00379338, 0.00124815, 0.00429545, 0.00248481,\n",
       "        0.00148009, 0.0242312 , 0.00125672, 0.00344922, 0.00532448,\n",
       "        0.00454493, 0.00137684, 0.00122562, 0.00129511, 0.00029702,\n",
       "        0.00111865, 0.00411672, 0.00047823, 0.00134126, 0.00328138,\n",
       "        0.00136874, 0.00780771, 0.00064937, 0.00115961, 0.00626365,\n",
       "        0.00165986, 0.00332731, 0.00295693, 0.00441555, 0.00098995,\n",
       "        0.00504872, 0.00068437, 0.00371534, 0.00127689, 0.00173548,\n",
       "        0.00193993, 0.0014718 , 0.00094751, 0.00137538, 0.00070891,\n",
       "        0.00117962, 0.00160524, 0.00233915, 0.00412146, 0.00079429,\n",
       "        0.00140898, 0.00389277, 0.00114136, 0.0012508 , 0.00465304,\n",
       "        0.00090158, 0.00092018, 0.0037925 , 0.0021245 , 0.00427201,\n",
       "        0.0002728 , 0.0021938 , 0.00189434, 0.00097615, 0.00207713,\n",
       "        0.00180973, 0.00133293, 0.00281099, 0.00358266, 0.00095728,\n",
       "        0.00286709, 0.0025617 , 0.0013873 , 0.00502068, 0.00137325,\n",
       "        0.00098829, 0.00260006, 0.00181788, 0.00453159, 0.00252107,\n",
       "        0.00235019, 0.00112426, 0.00099437, 0.00221557, 0.00251822,\n",
       "        0.001357  , 0.00298112, 0.0019532 , 0.00129565, 0.00190364,\n",
       "        0.00063199, 0.00189042, 0.00335008, 0.00273478, 0.0020543 ,\n",
       "        0.001037  , 0.00246616, 0.00369492, 0.00424983, 0.00141389,\n",
       "        0.00293966, 0.00189472, 0.00138764, 0.00459003, 0.00511976,\n",
       "        0.005671  , 0.00250143, 0.00408597, 0.00087326, 0.00151164,\n",
       "        0.00065864, 0.00059641, 0.00259579, 0.00341849, 0.00276679,\n",
       "        0.00096267, 0.00221951, 0.00293509, 0.00134385, 0.00296612,\n",
       "        0.00071772, 0.00331532, 0.00377503, 0.00143529, 0.00142683,\n",
       "        0.00094721, 0.00120333, 0.00167063, 0.00408694, 0.0009353 ,\n",
       "        0.00441857, 0.00059724, 0.0014062 , 0.00194732, 0.00543792,\n",
       "        0.00088902, 0.00114681, 0.00213208, 0.00442557, 0.00199243,\n",
       "        0.00271099, 0.00132794, 0.00142047, 0.00157557, 0.00106547,\n",
       "        0.00137089, 0.00377232, 0.00157232, 0.00050038, 0.0068046 ,\n",
       "        0.00764063]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
       "                    0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
       "                    0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
       "                    0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
       "                    0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
       "                    0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
       "                    0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
       "                    0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
       "                    0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
       "                    0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
       "                    0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
       "                    0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
       "                    0.9, 0.9, 0.9, 0.9, 0.9],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 12, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 12, 12, 12, 12, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 12, 12, 12,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 12, 12, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 12, 12,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 12, 12, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 12, 12,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 12, 12],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[10, 10, 10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10,\n",
       "                    10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25,\n",
       "                    25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25,\n",
       "                    40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40,\n",
       "                    40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40, 40, 50,\n",
       "                    50, 50, 10, 10, 10, 25, 25, 25, 40, 40, 40, 50, 50, 50,\n",
       "                    10, 10, 10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10,\n",
       "                    10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25,\n",
       "                    25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25,\n",
       "                    40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40,\n",
       "                    40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40, 40, 50,\n",
       "                    50, 50, 10, 10, 10, 25, 25, 25, 40, 40, 40, 50, 50, 50,\n",
       "                    10, 10, 10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10,\n",
       "                    10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25,\n",
       "                    25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25,\n",
       "                    40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40,\n",
       "                    40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40, 40, 50,\n",
       "                    50, 50, 10, 10, 10, 25, 25, 25, 40, 40, 40, 50, 50, 50,\n",
       "                    10, 10, 10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10,\n",
       "                    10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25,\n",
       "                    25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25,\n",
       "                    40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40,\n",
       "                    40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40, 40, 50,\n",
       "                    50, 50, 10, 10, 10, 25, 25, 25, 40, 40, 40, 50, 50, 50,\n",
       "                    10, 10, 10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10,\n",
       "                    10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25,\n",
       "                    25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25,\n",
       "                    40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40,\n",
       "                    40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40, 40, 50,\n",
       "                    50, 50, 10, 10, 10, 25, 25, 25, 40, 40, 40, 50, 50, 50,\n",
       "                    10, 10, 10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10,\n",
       "                    10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25,\n",
       "                    25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25,\n",
       "                    40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40,\n",
       "                    40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40, 40, 50,\n",
       "                    50, 50, 10, 10, 10, 25, 25, 25, 40, 40, 40, 50, 50, 50,\n",
       "                    10, 10, 10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10,\n",
       "                    10, 25, 25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25,\n",
       "                    25, 25, 40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25,\n",
       "                    40, 40, 40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40,\n",
       "                    40, 50, 50, 50, 10, 10, 10, 25, 25, 25, 40, 40, 40, 50,\n",
       "                    50, 50],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_reg_alpha': masked_array(data=[0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50,\n",
       "                    100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0, 50, 100, 0,\n",
       "                    50, 100, 0, 50, 100, 0, 50, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.01,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.4,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 10,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 25,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 100},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 50},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.9,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 100}],\n",
       " 'split0_test_score': array([0.61423594, 0.5933162 , 0.44667503, 0.60549558, 0.58235919,\n",
       "        0.39809134, 0.6230315 , 0.61073476, 0.46405229, 0.63583252,\n",
       "        0.60504202, 0.43843031, 0.58961303, 0.59679767, 0.47549909,\n",
       "        0.62068966, 0.60525012, 0.44794953, 0.63972736, 0.63362488,\n",
       "        0.50484262, 0.65830116, 0.63499758, 0.49264706, 0.59307577,\n",
       "        0.60760713, 0.48223646, 0.61523146, 0.60979157, 0.45714286,\n",
       "        0.63565132, 0.63636364, 0.52250146, 0.63347975, 0.6462553 ,\n",
       "        0.51960211, 0.59691081, 0.60620996, 0.51176133, 0.61615655,\n",
       "        0.61802155, 0.56781003, 0.64028056, 0.65837321, 0.62735382,\n",
       "        0.64881253, 0.6682442 , 0.63142438, 0.58596134, 0.61020212,\n",
       "        0.55392954, 0.61623246, 0.62705314, 0.5958523 , 0.64606181,\n",
       "        0.66852368, 0.67047619, 0.65232616, 0.67570093, 0.667295  ,\n",
       "        0.57494867, 0.61664262, 0.55176094, 0.60712494, 0.63208453,\n",
       "        0.60665362, 0.64848785, 0.68566091, 0.66823974, 0.6471464 ,\n",
       "        0.68196721, 0.67384183, 0.60040161, 0.6159102 , 0.55578947,\n",
       "        0.62682069, 0.63049853, 0.59189723, 0.64902646, 0.66987952,\n",
       "        0.65800866, 0.65371201, 0.68023256, 0.65304136, 0.5761194 ,\n",
       "        0.61656295, 0.57696228, 0.59970015, 0.6272772 , 0.6023166 ,\n",
       "        0.6318927 , 0.65507246, 0.65466604, 0.62685094, 0.66894866,\n",
       "        0.6481037 , 0.57604851, 0.61041466, 0.58025922, 0.58935743,\n",
       "        0.6408417 , 0.60626186, 0.612286  , 0.66380952, 0.65611913,\n",
       "        0.6222664 , 0.66985189, 0.65917603, 0.57027708, 0.60784314,\n",
       "        0.57881399, 0.60273973, 0.63470094, 0.59750623, 0.61836735,\n",
       "        0.65592972, 0.65096154, 0.61709661, 0.65481481, 0.64209505,\n",
       "        0.59266601, 0.58870565, 0.577     , 0.58563536, 0.61038961,\n",
       "        0.60717658, 0.60050633, 0.64348678, 0.64790076, 0.61398176,\n",
       "        0.64036419, 0.63762376, 0.57591093, 0.59446093, 0.57910448,\n",
       "        0.60764588, 0.61252446, 0.61516588, 0.62184024, 0.6552901 ,\n",
       "        0.64652278, 0.61866126, 0.66404715, 0.64609258, 0.7046683 ,\n",
       "        0.68442211, 0.67227723, 0.72321429, 0.70665323, 0.68774704,\n",
       "        0.72291563, 0.70546914, 0.68588469, 0.72862823, 0.71779449,\n",
       "        0.69690927, 0.7000993 , 0.68285281, 0.6696562 , 0.73973974,\n",
       "        0.71270161, 0.69616519, 0.75522388, 0.70870871, 0.69975186,\n",
       "        0.75872383, 0.71943888, 0.70489372, 0.698941  , 0.68117409,\n",
       "        0.67789891, 0.74288567, 0.71191554, 0.70472441, 0.75772682,\n",
       "        0.71856287, 0.70430906, 0.76403378, 0.72989076, 0.70820969,\n",
       "        0.71751129, 0.689     , 0.68351871, 0.73908629, 0.72453588,\n",
       "        0.70623742, 0.75276382, 0.72303797, 0.71158754, 0.75806452,\n",
       "        0.73336719, 0.71573604, 0.72905439, 0.70553064, 0.67932273,\n",
       "        0.75233875, 0.73594378, 0.71500504, 0.76564052, 0.74085213,\n",
       "        0.72854992, 0.76945959, 0.75037369, 0.73173173, 0.69878296,\n",
       "        0.70582379, 0.69732188, 0.73073073, 0.74092491, 0.7235116 ,\n",
       "        0.74254473, 0.74513716, 0.73642252, 0.74987618, 0.74974975,\n",
       "        0.73642252, 0.70804369, 0.71598808, 0.70433905, 0.72327672,\n",
       "        0.75608544, 0.72216547, 0.73841555, 0.76277188, 0.73150963,\n",
       "        0.74837419, 0.75921252, 0.73150963, 0.69156991, 0.70227498,\n",
       "        0.70747617, 0.71585244, 0.72754491, 0.73778664, 0.7310757 ,\n",
       "        0.74225774, 0.73841555, 0.74077767, 0.74462769, 0.73841555,\n",
       "        0.68036072, 0.71220492, 0.68820513, 0.72417252, 0.74874875,\n",
       "        0.72699542, 0.72481203, 0.74611918, 0.73483709, 0.72981057,\n",
       "        0.74874875, 0.73483709, 0.68746855, 0.71450075, 0.70060484,\n",
       "        0.693857  , 0.73741903, 0.72236118, 0.70493454, 0.7415507 ,\n",
       "        0.72236118, 0.71104387, 0.7379891 , 0.72236118, 0.66229177,\n",
       "        0.71340839, 0.70647265, 0.69416499, 0.72635815, 0.7215377 ,\n",
       "        0.70776942, 0.7347145 , 0.7215377 , 0.71108901, 0.7347145 ,\n",
       "        0.7215377 , 0.67159035, 0.69944247, 0.70747617, 0.70891089,\n",
       "        0.72718154, 0.71782178, 0.7185332 , 0.72653885, 0.71782178,\n",
       "        0.73449132, 0.72653885, 0.71782178, 0.70296548, 0.67261905,\n",
       "        0.65969108, 0.71076012, 0.68625498, 0.680372  , 0.71576866,\n",
       "        0.6935725 , 0.69103314, 0.7184853 , 0.70054808, 0.69317624,\n",
       "        0.70344828, 0.67661692, 0.6703629 , 0.74139626, 0.69695457,\n",
       "        0.69189189, 0.74478649, 0.70798215, 0.69803922, 0.74613466,\n",
       "        0.71513944, 0.7007371 , 0.70919881, 0.67601402, 0.67405542,\n",
       "        0.74576271, 0.70114371, 0.6935167 , 0.75460428, 0.71009448,\n",
       "        0.70068695, 0.75571003, 0.71980198, 0.70049261, 0.71731623,\n",
       "        0.6899696 , 0.68172485, 0.74584801, 0.7235116 , 0.7093199 ,\n",
       "        0.75524476, 0.73165829, 0.72295248, 0.76080402, 0.74208145,\n",
       "        0.72800809, 0.70919136, 0.71336314, 0.69246646, 0.73678963,\n",
       "        0.74170854, 0.71385238, 0.74628345, 0.75336323, 0.73165829,\n",
       "        0.74662669, 0.75775776, 0.73165829, 0.71782178, 0.71044776,\n",
       "        0.68870804, 0.74025974, 0.73705179, 0.726     , 0.75036928,\n",
       "        0.75763547, 0.73542601, 0.7573201 , 0.76289683, 0.73752495,\n",
       "        0.71514544, 0.70310933, 0.7035533 , 0.73830846, 0.73532338,\n",
       "        0.71943888, 0.73467337, 0.74862017, 0.71987952, 0.74572864,\n",
       "        0.75388471, 0.71987952, 0.69945355, 0.71940299, 0.70814365,\n",
       "        0.73971244, 0.74606299, 0.73006439, 0.75333663, 0.75450902,\n",
       "        0.73006439, 0.75632754, 0.75948104, 0.73006439, 0.70576639,\n",
       "        0.727     , 0.70564718, 0.74604743, 0.7501237 , 0.73772102,\n",
       "        0.74875374, 0.75099206, 0.73772102, 0.74712068, 0.75247525,\n",
       "        0.73772102, 0.69234617, 0.70773931, 0.70558882, 0.70025189,\n",
       "        0.72507553, 0.7148635 , 0.7111334 , 0.73245168, 0.7148635 ,\n",
       "        0.71558704, 0.73370389, 0.7148635 , 0.67628046, 0.70763132,\n",
       "        0.7119321 , 0.69858871, 0.72202884, 0.72131148, 0.71039354,\n",
       "        0.72845042, 0.72131148, 0.71709531, 0.72845042, 0.72131148,\n",
       "        0.67862481, 0.70020534, 0.70594109, 0.69402229, 0.71262038,\n",
       "        0.71548738, 0.70951417, 0.71406805, 0.71548738, 0.71787149,\n",
       "        0.71406805, 0.71548738, 0.69839728, 0.67061728, 0.66339066,\n",
       "        0.70437777, 0.68639053, 0.67804878, 0.70291646, 0.69548134,\n",
       "        0.68029197, 0.70489372, 0.69522403, 0.68898593, 0.71139615,\n",
       "        0.67562189, 0.66930302, 0.7300343 , 0.69009901, 0.68226121,\n",
       "        0.74041298, 0.69955599, 0.68228404, 0.7453294 , 0.69930762,\n",
       "        0.69361909, 0.70394408, 0.68355688, 0.66897919, 0.73637265,\n",
       "        0.69698468, 0.68397844, 0.74740484, 0.70182356, 0.68945312,\n",
       "        0.75037369, 0.70291646, 0.69463415, 0.70136433, 0.69994957,\n",
       "        0.68324873, 0.74059207, 0.72846348, 0.7073922 , 0.74810702,\n",
       "        0.73811931, 0.71975498, 0.76014021, 0.74418605, 0.72230653,\n",
       "        0.71996028, 0.7119321 , 0.69499241, 0.7504931 , 0.73146293,\n",
       "        0.7145749 , 0.7627035 , 0.74379345, 0.72269753, 0.77029021,\n",
       "        0.75249501, 0.72269753, 0.71904525, 0.71400199, 0.68894602,\n",
       "        0.73751854, 0.72998508, 0.7248731 , 0.75306824, 0.74913065,\n",
       "        0.73536768, 0.7621383 , 0.75332349, 0.73536768, 0.70229008,\n",
       "        0.7248996 , 0.68277635, 0.71399799, 0.74294355, 0.72124561,\n",
       "        0.72634791, 0.75178026, 0.72124561, 0.73387097, 0.75076297,\n",
       "        0.72124561, 0.71039354, 0.71817732, 0.70485934, 0.73573574,\n",
       "        0.74592995, 0.72745491, 0.75325978, 0.75498008, 0.72745491,\n",
       "        0.765555  , 0.760199  , 0.72745491, 0.72080201, 0.71871872,\n",
       "        0.71363865, 0.74282888, 0.75112219, 0.72197084, 0.75173784,\n",
       "        0.75875876, 0.72197084, 0.75222993, 0.76223776, 0.72197084,\n",
       "        0.67777778, 0.70014918, 0.69226882, 0.69791136, 0.72052183,\n",
       "        0.70822943, 0.70516717, 0.74823766, 0.70822943, 0.70957932,\n",
       "        0.74352463, 0.70822943, 0.68917577, 0.71385542, 0.70806209,\n",
       "        0.69504256, 0.73386296, 0.70629371, 0.70912738, 0.73263368,\n",
       "        0.70629371, 0.72261307, 0.73263368, 0.70629371, 0.70192783,\n",
       "        0.71707561, 0.69928644, 0.71080139, 0.73790323, 0.70912738,\n",
       "        0.719202  , 0.74007039, 0.70912738, 0.72718204, 0.74007039,\n",
       "        0.70912738]),\n",
       " 'split1_test_score': array([0.57171923, 0.54545455, 0.45238095, 0.59868753, 0.55224689,\n",
       "        0.36652835, 0.63775259, 0.57051282, 0.39918534, 0.62270699,\n",
       "        0.55888651, 0.3772791 , 0.56673511, 0.57260556, 0.48891552,\n",
       "        0.59817814, 0.5960396 , 0.43576826, 0.62994492, 0.60696517,\n",
       "        0.48750762, 0.62524851, 0.609421  , 0.45574388, 0.56534954,\n",
       "        0.58145363, 0.49853372, 0.59886422, 0.60987415, 0.46239211,\n",
       "        0.62576065, 0.62939759, 0.51917404, 0.62538071, 0.62392344,\n",
       "        0.49638554, 0.6023976 , 0.5928934 , 0.53936348, 0.6219151 ,\n",
       "        0.61067194, 0.55124056, 0.65308151, 0.65432099, 0.63749378,\n",
       "        0.67131474, 0.66761229, 0.63846532, 0.55629139, 0.59502488,\n",
       "        0.56306546, 0.59969864, 0.63614688, 0.59656218, 0.6241206 ,\n",
       "        0.66573296, 0.64454064, 0.64435564, 0.67357513, 0.65967588,\n",
       "        0.58464567, 0.60861151, 0.54457573, 0.61135802, 0.63855992,\n",
       "        0.6121242 , 0.62829268, 0.66791045, 0.65080875, 0.64317181,\n",
       "        0.67450611, 0.65877899, 0.59572301, 0.61417323, 0.55278515,\n",
       "        0.63030303, 0.63405276, 0.58388241, 0.65078561, 0.66539197,\n",
       "        0.64723032, 0.64974619, 0.67054828, 0.65219489, 0.5794948 ,\n",
       "        0.60273973, 0.57418699, 0.61866931, 0.62890062, 0.60848126,\n",
       "        0.64455446, 0.65840877, 0.6548418 , 0.64770459, 0.65089329,\n",
       "        0.66217517, 0.57523434, 0.61329452, 0.57387796, 0.5938431 ,\n",
       "        0.62626263, 0.61830574, 0.62295082, 0.65507246, 0.66003788,\n",
       "        0.63288624, 0.66147102, 0.6647482 , 0.59979424, 0.60194175,\n",
       "        0.57433809, 0.61015228, 0.61417323, 0.59674134, 0.62796567,\n",
       "        0.64171657, 0.64416586, 0.6315256 , 0.6503006 , 0.63168317,\n",
       "        0.57429719, 0.61344944, 0.59804878, 0.60049875, 0.61850569,\n",
       "        0.60916708, 0.60974385, 0.64936337, 0.64655172, 0.62051793,\n",
       "        0.65236908, 0.63695543, 0.55350921, 0.60272639, 0.60291262,\n",
       "        0.57256858, 0.62439497, 0.6169691 , 0.59140859, 0.64691598,\n",
       "        0.66474544, 0.58964143, 0.64933791, 0.66182874, 0.70974058,\n",
       "        0.68658178, 0.65368852, 0.70954973, 0.70147783, 0.68937876,\n",
       "        0.72263132, 0.70201276, 0.6877193 , 0.72485207, 0.71540726,\n",
       "        0.69373434, 0.71807712, 0.68958645, 0.65408163, 0.75012321,\n",
       "        0.708     , 0.69552089, 0.75949367, 0.70582379, 0.68968969,\n",
       "        0.75895925, 0.71764118, 0.70546914, 0.71794872, 0.69219294,\n",
       "        0.65953109, 0.75516224, 0.71812749, 0.69552089, 0.7621383 ,\n",
       "        0.71660917, 0.69543402, 0.7679531 , 0.72951629, 0.70653266,\n",
       "        0.72486513, 0.69373434, 0.66396761, 0.74837419, 0.73382716,\n",
       "        0.7100651 , 0.75212819, 0.74074074, 0.7265429 , 0.75528701,\n",
       "        0.74249138, 0.73219659, 0.70732907, 0.70264966, 0.6740666 ,\n",
       "        0.74201474, 0.73326772, 0.72123016, 0.75933202, 0.74482759,\n",
       "        0.7308642 , 0.76209086, 0.74678536, 0.73710317, 0.71194844,\n",
       "        0.70098039, 0.67641129, 0.7383407 , 0.73182711, 0.72799602,\n",
       "        0.76479218, 0.75260288, 0.7372549 , 0.7655642 , 0.75531389,\n",
       "        0.7372549 , 0.69965017, 0.70740189, 0.694     , 0.71450227,\n",
       "        0.73906634, 0.7283523 , 0.73867596, 0.75085995, 0.72772033,\n",
       "        0.74836765, 0.75149105, 0.72772033, 0.70054808, 0.71864407,\n",
       "        0.70326409, 0.72790467, 0.74902724, 0.73220339, 0.74827246,\n",
       "        0.75654321, 0.73220339, 0.7553816 , 0.76153091, 0.73220339,\n",
       "        0.70594059, 0.72674135, 0.70535714, 0.73283951, 0.74791973,\n",
       "        0.73756098, 0.74446086, 0.75109596, 0.73756098, 0.75540275,\n",
       "        0.75878906, 0.73756098, 0.66832669, 0.71040947, 0.69118388,\n",
       "        0.69672944, 0.71764118, 0.72089625, 0.71323168, 0.73731343,\n",
       "        0.72089625, 0.7275405 , 0.73815461, 0.72089625, 0.67573011,\n",
       "        0.70922693, 0.69451434, 0.70381526, 0.72367771, 0.721     ,\n",
       "        0.70922693, 0.72968981, 0.721     , 0.72175212, 0.72968981,\n",
       "        0.721     , 0.6791498 , 0.72291563, 0.69459869, 0.716     ,\n",
       "        0.73218673, 0.72960949, 0.72943508, 0.73543929, 0.72960949,\n",
       "        0.73386296, 0.73481481, 0.72960949, 0.7061657 , 0.67833917,\n",
       "        0.63742072, 0.7179236 , 0.70211718, 0.66872746, 0.72309198,\n",
       "        0.71039845, 0.68505517, 0.72290338, 0.7109375 , 0.68927681,\n",
       "        0.72789116, 0.68613861, 0.64526316, 0.75061005, 0.70843611,\n",
       "        0.672148  , 0.76181198, 0.71603727, 0.69246231, 0.76915521,\n",
       "        0.71575847, 0.69804707, 0.71933268, 0.6875    , 0.6459758 ,\n",
       "        0.75133301, 0.70512821, 0.67930328, 0.76144109, 0.71754983,\n",
       "        0.6918429 , 0.7631068 , 0.72113503, 0.69839679, 0.71532847,\n",
       "        0.70680372, 0.67753439, 0.74670571, 0.7390873 , 0.71104387,\n",
       "        0.75767918, 0.75579674, 0.72225013, 0.76771654, 0.76279528,\n",
       "        0.72781655, 0.72745098, 0.71176471, 0.68565717, 0.75097656,\n",
       "        0.73895976, 0.72736269, 0.76333657, 0.74780059, 0.73207171,\n",
       "        0.77102577, 0.75960591, 0.73359841, 0.72246696, 0.71705426,\n",
       "        0.68145161, 0.75577396, 0.74721009, 0.72835821, 0.76622743,\n",
       "        0.75918762, 0.74319644, 0.77094431, 0.76450512, 0.74319644,\n",
       "        0.70717131, 0.72035139, 0.6973027 , 0.73604827, 0.74519468,\n",
       "        0.72293942, 0.73836918, 0.75923191, 0.73103448, 0.74125874,\n",
       "        0.76345679, 0.73103448, 0.70711089, 0.72390244, 0.71047431,\n",
       "        0.72817955, 0.74743527, 0.72987654, 0.75383853, 0.76292683,\n",
       "        0.72987654, 0.75831266, 0.76292683, 0.72987654, 0.71286142,\n",
       "        0.72905439, 0.71378795, 0.73558648, 0.75210292, 0.73768044,\n",
       "        0.75184275, 0.75683594, 0.73768044, 0.76082677, 0.75808031,\n",
       "        0.73768044, 0.68324217, 0.71527094, 0.69556552, 0.69925926,\n",
       "        0.73527956, 0.70564718, 0.71421496, 0.73510265, 0.70564718,\n",
       "        0.72816971, 0.735     , 0.70564718, 0.68070175, 0.71363865,\n",
       "        0.70135068, 0.7086224 , 0.73031496, 0.71102284, 0.72530713,\n",
       "        0.72989076, 0.71102284, 0.72513474, 0.72989076, 0.71102284,\n",
       "        0.68665667, 0.72043011, 0.70623742, 0.72258065, 0.73431373,\n",
       "        0.71122195, 0.72556762, 0.73874755, 0.71122195, 0.73741362,\n",
       "        0.73874755, 0.71122195, 0.71011021, 0.68999507, 0.63713981,\n",
       "        0.70942663, 0.7       , 0.65766234, 0.71442495, 0.70377541,\n",
       "        0.67981791, 0.71762415, 0.71117562, 0.68341709, 0.71219512,\n",
       "        0.68363273, 0.64102564, 0.72896282, 0.70323213, 0.6638961 ,\n",
       "        0.74132029, 0.70996094, 0.68056968, 0.742913  , 0.71366067,\n",
       "        0.68362157, 0.71065494, 0.68287841, 0.64289516, 0.73491067,\n",
       "        0.70491006, 0.67010309, 0.74418605, 0.70823186, 0.68255578,\n",
       "        0.75798645, 0.71824818, 0.6875632 , 0.7252208 , 0.70335168,\n",
       "        0.68351871, 0.74347612, 0.73684211, 0.70812437, 0.75994109,\n",
       "        0.74492323, 0.72258065, 0.76620825, 0.74925962, 0.72591857,\n",
       "        0.73129921, 0.71936759, 0.69648241, 0.74059406, 0.73994112,\n",
       "        0.7235732 , 0.75297619, 0.76153091, 0.72119701, 0.75892857,\n",
       "        0.76531112, 0.72119701, 0.72727273, 0.71221945, 0.67581301,\n",
       "        0.7621383 , 0.74059406, 0.72327672, 0.77581121, 0.75392157,\n",
       "        0.73439049, 0.77766798, 0.76548025, 0.73439049, 0.72355289,\n",
       "        0.71835918, 0.69878296, 0.74572864, 0.73956262, 0.73233831,\n",
       "        0.74626866, 0.75024679, 0.73564356, 0.75      , 0.75332349,\n",
       "        0.73564356, 0.70251603, 0.71527094, 0.71478697, 0.74352711,\n",
       "        0.74480712, 0.72420635, 0.76356589, 0.74464908, 0.72420635,\n",
       "        0.7608378 , 0.75296443, 0.72420635, 0.7137948 , 0.72103431,\n",
       "        0.71713147, 0.7422179 , 0.73663366, 0.73168317, 0.74805447,\n",
       "        0.7475442 , 0.73168317, 0.74951267, 0.75405007, 0.73168317,\n",
       "        0.69072682, 0.70111449, 0.71698113, 0.69391825, 0.71728272,\n",
       "        0.72329856, 0.71080139, 0.73194857, 0.72329856, 0.72673267,\n",
       "        0.72916667, 0.72329856, 0.70065623, 0.71703704, 0.71649228,\n",
       "        0.71629779, 0.72158809, 0.71499014, 0.73092369, 0.7375677 ,\n",
       "        0.71499014, 0.73493976, 0.7375677 , 0.71499014, 0.71442688,\n",
       "        0.69918699, 0.70600101, 0.73647984, 0.72826627, 0.71649228,\n",
       "        0.74356436, 0.74019608, 0.71649228, 0.73794132, 0.74019608,\n",
       "        0.71649228]),\n",
       " 'split2_test_score': array([0.5620175 , 0.51565168, 0.42646114, 0.59551122, 0.5254332 ,\n",
       "        0.3325975 , 0.61240695, 0.56151762, 0.38700565, 0.61341223,\n",
       "        0.57447944, 0.35345455, 0.56295525, 0.55328084, 0.46775166,\n",
       "        0.6       , 0.58718331, 0.41492147, 0.611     , 0.6127451 ,\n",
       "        0.47607053, 0.62092794, 0.61553398, 0.45396419, 0.56407617,\n",
       "        0.5560166 , 0.47467609, 0.6       , 0.59470069, 0.44485981,\n",
       "        0.63383085, 0.61442308, 0.49570552, 0.62202381, 0.62332696,\n",
       "        0.47648515, 0.56386293, 0.53805497, 0.48158971, 0.60556962,\n",
       "        0.58492462, 0.54643049, 0.63904144, 0.62895494, 0.61767627,\n",
       "        0.63691073, 0.63203463, 0.61869065, 0.56140351, 0.5761358 ,\n",
       "        0.50890869, 0.59423951, 0.5998062 , 0.56410256, 0.63110668,\n",
       "        0.64214993, 0.62608696, 0.63727455, 0.64686469, 0.63259402,\n",
       "        0.57348407, 0.57742258, 0.5215965 , 0.59162039, 0.60999039,\n",
       "        0.58295512, 0.61660865, 0.64585275, 0.62982373, 0.61697927,\n",
       "        0.65290807, 0.62826718, 0.58236776, 0.5556701 , 0.51872247,\n",
       "        0.59213885, 0.60029571, 0.57172131, 0.61869065, 0.6424066 ,\n",
       "        0.62834876, 0.61164557, 0.64187867, 0.62937743, 0.53656025,\n",
       "        0.6053012 , 0.55172414, 0.589     , 0.61442308, 0.58270865,\n",
       "        0.60250627, 0.65321806, 0.64750236, 0.61439206, 0.66055931,\n",
       "        0.64547206, 0.58358061, 0.58187135, 0.55136809, 0.6041461 ,\n",
       "        0.60614793, 0.59416708, 0.62350598, 0.65030675, 0.65350259,\n",
       "        0.63001486, 0.64722753, 0.64460432, 0.58189217, 0.59393346,\n",
       "        0.57010309, 0.58308157, 0.60330165, 0.5815023 , 0.60978044,\n",
       "        0.65217391, 0.63127131, 0.62246413, 0.65738439, 0.64229249,\n",
       "        0.56748768, 0.58959538, 0.57542458, 0.58038825, 0.59370315,\n",
       "        0.59921415, 0.61094527, 0.63789683, 0.62763916, 0.6119403 ,\n",
       "        0.63348416, 0.62000986, 0.56410256, 0.58569371, 0.56744649,\n",
       "        0.57690368, 0.60761999, 0.58134921, 0.60369815, 0.64440079,\n",
       "        0.62247839, 0.60240964, 0.64941406, 0.62444772, 0.67953276,\n",
       "        0.663286  , 0.66733367, 0.6941896 , 0.67860798, 0.66900702,\n",
       "        0.70826991, 0.66838311, 0.67598602, 0.71234257, 0.67178699,\n",
       "        0.676     , 0.6887218 , 0.67307692, 0.66364095, 0.717     ,\n",
       "        0.68955073, 0.68034017, 0.72736368, 0.68466633, 0.68696955,\n",
       "        0.73232323, 0.68676546, 0.69130869, 0.69134567, 0.67819014,\n",
       "        0.66498994, 0.71956088, 0.68986384, 0.6875    , 0.73678963,\n",
       "        0.69226882, 0.69495756, 0.74439462, 0.68962025, 0.69134567,\n",
       "        0.7100651 , 0.68074409, 0.67347982, 0.73247779, 0.69833249,\n",
       "        0.69050051, 0.74513716, 0.71256281, 0.69507187, 0.74353877,\n",
       "        0.71715145, 0.70311702, 0.71245059, 0.68365817, 0.68539326,\n",
       "        0.71904525, 0.71492761, 0.70398386, 0.7339632 , 0.73141724,\n",
       "        0.71385542, 0.74302789, 0.73006439, 0.71155779, 0.68588469,\n",
       "        0.70144064, 0.68104777, 0.71456693, 0.72931963, 0.71010101,\n",
       "        0.72376238, 0.73503435, 0.72119701, 0.7234886 , 0.74212598,\n",
       "        0.72119701, 0.69578466, 0.69534413, 0.67592593, 0.70435218,\n",
       "        0.72263868, 0.69829809, 0.71543086, 0.72962227, 0.71202046,\n",
       "        0.71464519, 0.72425917, 0.71202046, 0.68324217, 0.70342772,\n",
       "        0.68823832, 0.70783133, 0.72592593, 0.71550426, 0.72936468,\n",
       "        0.7394958 , 0.719202  , 0.73326673, 0.74044665, 0.719202  ,\n",
       "        0.66331152, 0.71619614, 0.69517103, 0.71371371, 0.72529644,\n",
       "        0.71463775, 0.72745391, 0.73895976, 0.71463775, 0.73369836,\n",
       "        0.7408867 , 0.71463775, 0.6822335 , 0.69045226, 0.6875    ,\n",
       "        0.70623441, 0.71312309, 0.70446563, 0.71278195, 0.71471774,\n",
       "        0.70446563, 0.726     , 0.72250252, 0.70446563, 0.65965966,\n",
       "        0.69414101, 0.69786368, 0.68858994, 0.70419753, 0.71111111,\n",
       "        0.7057072 , 0.7017017 , 0.71111111, 0.71152895, 0.7017017 ,\n",
       "        0.71111111, 0.66335157, 0.68608739, 0.71868787, 0.68172888,\n",
       "        0.71365205, 0.7343519 , 0.70168484, 0.72055888, 0.7343519 ,\n",
       "        0.7086224 , 0.72055888, 0.7343519 , 0.67714141, 0.663926  ,\n",
       "        0.64769788, 0.68731269, 0.66632601, 0.66465561, 0.69573935,\n",
       "        0.67243133, 0.66532663, 0.70131181, 0.67376966, 0.67132867,\n",
       "        0.69657258, 0.67867868, 0.6595092 , 0.70481013, 0.68156425,\n",
       "        0.66901408, 0.71906694, 0.68415742, 0.67401102, 0.72718104,\n",
       "        0.68287741, 0.6779661 , 0.69964664, 0.67208122, 0.66564574,\n",
       "        0.71363865, 0.68822638, 0.674     , 0.7337986 , 0.69114688,\n",
       "        0.68      , 0.73170732, 0.69704557, 0.68093579, 0.69447236,\n",
       "        0.68332483, 0.66666667, 0.71915747, 0.7072552 , 0.69129555,\n",
       "        0.73236618, 0.71039354, 0.70594159, 0.74137069, 0.72127872,\n",
       "        0.70570419, 0.69486405, 0.70235118, 0.67350077, 0.72047833,\n",
       "        0.72591857, 0.70105158, 0.73015873, 0.74299754, 0.72436218,\n",
       "        0.73511905, 0.74311024, 0.72436218, 0.7042957 , 0.70856012,\n",
       "        0.68154158, 0.739     , 0.72763419, 0.71521197, 0.7390873 ,\n",
       "        0.73772102, 0.72110182, 0.74191944, 0.73715415, 0.72110182,\n",
       "        0.69114688, 0.68803205, 0.66838046, 0.70718232, 0.72004028,\n",
       "        0.69701568, 0.70987342, 0.72627919, 0.69863705, 0.72936468,\n",
       "        0.72989076, 0.69863705, 0.69354839, 0.69713712, 0.68896655,\n",
       "        0.71357285, 0.71964018, 0.7119321 , 0.7245509 , 0.73339941,\n",
       "        0.7119321 , 0.73163418, 0.73663366, 0.7119321 , 0.69284995,\n",
       "        0.69473684, 0.69315895, 0.71294001, 0.73101421, 0.70570866,\n",
       "        0.72274764, 0.73199412, 0.70570866, 0.7290738 , 0.72709456,\n",
       "        0.70570866, 0.6687054 , 0.68951813, 0.6907994 , 0.68396947,\n",
       "        0.71177945, 0.69379451, 0.69344179, 0.71421446, 0.69379451,\n",
       "        0.69199595, 0.71064468, 0.69379451, 0.67467467, 0.69362993,\n",
       "        0.68625498, 0.67767768, 0.70570571, 0.70674487, 0.68931069,\n",
       "        0.71658291, 0.70674487, 0.69391825, 0.71658291, 0.70674487,\n",
       "        0.66163142, 0.67971888, 0.68619662, 0.68631369, 0.71102284,\n",
       "        0.7017199 , 0.69712016, 0.71506713, 0.69915465, 0.70359281,\n",
       "        0.71506713, 0.69915465, 0.66870229, 0.65826613, 0.64596273,\n",
       "        0.69038076, 0.66293279, 0.66060606, 0.6898288 , 0.66801619,\n",
       "        0.66431452, 0.69661787, 0.6720403 , 0.6670005 , 0.69059656,\n",
       "        0.67546978, 0.65219638, 0.71221945, 0.67740312, 0.66835187,\n",
       "        0.71299094, 0.6737481 , 0.67102616, 0.7239819 , 0.67709384,\n",
       "        0.67367367, 0.69420655, 0.67474747, 0.65210608, 0.70795569,\n",
       "        0.69296056, 0.6733871 , 0.72008012, 0.68858994, 0.67068273,\n",
       "        0.72582253, 0.68706118, 0.67695371, 0.69600405, 0.66009341,\n",
       "        0.66969697, 0.71097684, 0.69868554, 0.68095967, 0.72754491,\n",
       "        0.71147044, 0.70242915, 0.72745491, 0.72534143, 0.70688788,\n",
       "        0.70529965, 0.68490375, 0.67715736, 0.72230502, 0.72188755,\n",
       "        0.70723192, 0.73112975, 0.71557789, 0.71904525, 0.7306163 ,\n",
       "        0.72272272, 0.71904525, 0.70144064, 0.69041646, 0.67174023,\n",
       "        0.73720472, 0.72564612, 0.7100651 , 0.75343811, 0.73      ,\n",
       "        0.71662531, 0.75479115, 0.73109664, 0.71662531, 0.70847968,\n",
       "        0.68121827, 0.67011911, 0.71521197, 0.71658291, 0.69698518,\n",
       "        0.726     , 0.71935968, 0.69698518, 0.739     , 0.72890664,\n",
       "        0.69698518, 0.69129555, 0.69307924, 0.68781986, 0.71071071,\n",
       "        0.72591113, 0.71875   , 0.72492552, 0.71940299, 0.71875   ,\n",
       "        0.73243647, 0.72363818, 0.71875   , 0.68048534, 0.70275689,\n",
       "        0.6779661 , 0.71108901, 0.71335668, 0.712     , 0.72871287,\n",
       "        0.7312469 , 0.712     , 0.73134328, 0.7312469 , 0.712     ,\n",
       "        0.67577137, 0.70547264, 0.69648241, 0.67636729, 0.70980789,\n",
       "        0.71064468, 0.70524233, 0.7193159 , 0.71064468, 0.69925187,\n",
       "        0.71882887, 0.71064468, 0.67833248, 0.68002054, 0.69507864,\n",
       "        0.69131998, 0.71152895, 0.70558631, 0.70594262, 0.70938897,\n",
       "        0.70558631, 0.70865335, 0.70938897, 0.70558631, 0.67864271,\n",
       "        0.68312343, 0.67604219, 0.7060576 , 0.71964018, 0.70064325,\n",
       "        0.70730479, 0.71235618, 0.70064325, 0.70552764, 0.71235618,\n",
       "        0.70064325]),\n",
       " 'split3_test_score': array([0.58038007, 0.55935829, 0.4550527 , 0.60590886, 0.5294772 ,\n",
       "        0.34146341, 0.63883495, 0.58505564, 0.4417831 , 0.63592233,\n",
       "        0.57827476, 0.42682927, 0.56939502, 0.58286301, 0.48033878,\n",
       "        0.60110386, 0.57973922, 0.4150694 , 0.63142292, 0.61318898,\n",
       "        0.49326805, 0.63739517, 0.61425061, 0.48125   , 0.59173387,\n",
       "        0.5841785 , 0.49221557, 0.59850374, 0.59549461, 0.43877551,\n",
       "        0.63743842, 0.61913876, 0.50148544, 0.64748903, 0.61374637,\n",
       "        0.50722892, 0.5744898 , 0.58692347, 0.52654867, 0.61170483,\n",
       "        0.61411192, 0.56494845, 0.64907914, 0.64672897, 0.63736264,\n",
       "        0.65042268, 0.64992894, 0.64525272, 0.57627119, 0.60068594,\n",
       "        0.54458089, 0.62609542, 0.62279447, 0.58870565, 0.64825581,\n",
       "        0.66573816, 0.63602251, 0.64334975, 0.66352498, 0.63517306,\n",
       "        0.56972705, 0.60333007, 0.5498426 , 0.60540541, 0.63779153,\n",
       "        0.59233792, 0.62082515, 0.6737425 , 0.64398882, 0.63076177,\n",
       "        0.66107226, 0.65169569, 0.57329317, 0.58312531, 0.55549819,\n",
       "        0.61507738, 0.6159102 , 0.5955774 , 0.64829659, 0.6531574 ,\n",
       "        0.62894861, 0.64506481, 0.64862252, 0.63197729, 0.56150794,\n",
       "        0.59686888, 0.58475426, 0.60356789, 0.61833893, 0.58926829,\n",
       "        0.63080685, 0.64864865, 0.63653484, 0.63502935, 0.65647396,\n",
       "        0.63843958, 0.55804878, 0.59664694, 0.58881742, 0.59736457,\n",
       "        0.61084221, 0.59855422, 0.61825319, 0.64641671, 0.64812239,\n",
       "        0.62352941, 0.6532567 , 0.64272388, 0.57995951, 0.59017987,\n",
       "        0.57029309, 0.61477442, 0.59729865, 0.58857696, 0.63100545,\n",
       "        0.64035088, 0.64052288, 0.6249379 , 0.64225631, 0.62975445,\n",
       "        0.57171717, 0.58321203, 0.57675331, 0.59237246, 0.60320544,\n",
       "        0.58004866, 0.61317484, 0.6492891 , 0.63623529, 0.61607589,\n",
       "        0.63596704, 0.62854397, 0.57016683, 0.58771499, 0.57864078,\n",
       "        0.59364303, 0.60384805, 0.5923445 , 0.60474117, 0.64247702,\n",
       "        0.64219331, 0.61330762, 0.63095823, 0.63041386, 0.67993874,\n",
       "        0.67272727, 0.66262626, 0.69934967, 0.68979387, 0.68145161,\n",
       "        0.70640835, 0.68245968, 0.67911201, 0.71471173, 0.69      ,\n",
       "        0.68280383, 0.69465267, 0.68565717, 0.66999501, 0.7234252 ,\n",
       "        0.69534767, 0.68402604, 0.73065622, 0.69709127, 0.68376068,\n",
       "        0.74081333, 0.70582379, 0.69265367, 0.6996997 , 0.68591339,\n",
       "        0.67200801, 0.71901235, 0.69980119, 0.68234117, 0.73271211,\n",
       "        0.6983017 , 0.68336673, 0.74168297, 0.70845771, 0.69539376,\n",
       "        0.70547607, 0.68161435, 0.66464952, 0.73947111, 0.71736997,\n",
       "        0.70161685, 0.74092247, 0.73216031, 0.71365205, 0.75      ,\n",
       "        0.7361723 , 0.71937531, 0.694182  , 0.69995105, 0.68470241,\n",
       "        0.71772214, 0.72770853, 0.70225269, 0.73050346, 0.73801453,\n",
       "        0.71965318, 0.74950884, 0.74541948, 0.73076923, 0.69292124,\n",
       "        0.7055336 , 0.67497507, 0.726647  , 0.73628489, 0.7122093 ,\n",
       "        0.74135412, 0.74528758, 0.71934077, 0.74951644, 0.75376396,\n",
       "        0.72858518, 0.70710059, 0.68855721, 0.68837675, 0.71705619,\n",
       "        0.72682446, 0.71028037, 0.73501734, 0.74118231, 0.72423945,\n",
       "        0.73103448, 0.74246169, 0.72423945, 0.68523123, 0.70512821,\n",
       "        0.69642857, 0.72361316, 0.73623188, 0.70930233, 0.72798819,\n",
       "        0.74389052, 0.71463296, 0.73503435, 0.75594372, 0.71463296,\n",
       "        0.68900936, 0.71002445, 0.69813176, 0.72744908, 0.73144531,\n",
       "        0.72034716, 0.73856524, 0.75096525, 0.72437137, 0.74517375,\n",
       "        0.75675676, 0.72437137, 0.67161961, 0.68168462, 0.67924528,\n",
       "        0.68302454, 0.71378795, 0.70390509, 0.69672944, 0.73422752,\n",
       "        0.70390509, 0.70232789, 0.7243083 , 0.70390509, 0.67724343,\n",
       "        0.69917114, 0.68999507, 0.71028037, 0.71002445, 0.71694417,\n",
       "        0.71456693, 0.71442688, 0.71694417, 0.71238283, 0.71442688,\n",
       "        0.71694417, 0.6834817 , 0.69513991, 0.68818272, 0.71102284,\n",
       "        0.71665044, 0.70173697, 0.71893491, 0.72011662, 0.70173697,\n",
       "        0.7232581 , 0.72011662, 0.70173697, 0.68117409, 0.66227848,\n",
       "        0.64141152, 0.7032419 , 0.67937782, 0.65918367, 0.70629024,\n",
       "        0.68790443, 0.67100651, 0.71478003, 0.69284294, 0.67840966,\n",
       "        0.69903895, 0.66801825, 0.65051546, 0.73163565, 0.6888668 ,\n",
       "        0.67410035, 0.73904576, 0.69672944, 0.68204614, 0.73499268,\n",
       "        0.7041273 , 0.68665667, 0.70414201, 0.67273647, 0.64678663,\n",
       "        0.74084919, 0.69101678, 0.6673459 , 0.75204622, 0.7036855 ,\n",
       "        0.689     , 0.75108748, 0.70652174, 0.68931069, 0.70640394,\n",
       "        0.68384539, 0.6720403 , 0.7221135 , 0.71015204, 0.71066998,\n",
       "        0.74463938, 0.71882641, 0.71724138, 0.7453477 , 0.72246696,\n",
       "        0.72361316, 0.70405608, 0.6977887 , 0.67510121, 0.73323544,\n",
       "        0.72604066, 0.70910872, 0.75363725, 0.74492754, 0.72744814,\n",
       "        0.75633528, 0.73910915, 0.72744814, 0.71034483, 0.70425321,\n",
       "        0.67672197, 0.74088478, 0.73963356, 0.71777344, 0.75204622,\n",
       "        0.75572519, 0.73330083, 0.75521086, 0.75538535, 0.73330083,\n",
       "        0.70622854, 0.6990099 , 0.68140704, 0.73317191, 0.73505105,\n",
       "        0.70154459, 0.73333333, 0.7367387 , 0.7       , 0.73493386,\n",
       "        0.73900293, 0.7       , 0.68753105, 0.7079646 , 0.70201276,\n",
       "        0.72771792, 0.72789784, 0.7204667 , 0.73910915, 0.74041727,\n",
       "        0.7204667 , 0.74719922, 0.74357731, 0.7204667 , 0.69641981,\n",
       "        0.70779862, 0.69535114, 0.72718351, 0.72983479, 0.71608527,\n",
       "        0.73756098, 0.73955296, 0.71608527, 0.7459932 , 0.73940575,\n",
       "        0.71608527, 0.67915229, 0.69803922, 0.69788073, 0.70138889,\n",
       "        0.71209877, 0.71301775, 0.70559372, 0.73110893, 0.71301775,\n",
       "        0.70986745, 0.73575639, 0.71301775, 0.67920792, 0.71232877,\n",
       "        0.68759342, 0.6956957 , 0.72862086, 0.71470879, 0.70669975,\n",
       "        0.72816487, 0.71470879, 0.70594059, 0.72816487, 0.71470879,\n",
       "        0.68831169, 0.69980315, 0.71882641, 0.71031746, 0.72568093,\n",
       "        0.71668312, 0.71449777, 0.72415483, 0.71668312, 0.71924603,\n",
       "        0.72415483, 0.71668312, 0.68304915, 0.6554878 , 0.64781491,\n",
       "        0.69428008, 0.672     , 0.66464032, 0.70024691, 0.68605224,\n",
       "        0.6716792 , 0.70646275, 0.69087341, 0.67666167, 0.70189432,\n",
       "        0.66191446, 0.64948454, 0.72878862, 0.69299112, 0.67241379,\n",
       "        0.7374031 , 0.70004909, 0.67667668, 0.73955296, 0.70628684,\n",
       "        0.68431568, 0.70629024, 0.6653082 , 0.64887064, 0.7404878 ,\n",
       "        0.70123457, 0.67371601, 0.74987905, 0.70320988, 0.67969925,\n",
       "        0.74576271, 0.7080256 , 0.68557214, 0.71104536, 0.69491525,\n",
       "        0.6744186 , 0.73731343, 0.72110182, 0.69779116, 0.74841231,\n",
       "        0.73876543, 0.70693959, 0.75097656, 0.73818898, 0.71321696,\n",
       "        0.71143552, 0.7054148 , 0.66869301, 0.73699422, 0.72825553,\n",
       "        0.71266634, 0.7537206 , 0.74041298, 0.72043011, 0.75143954,\n",
       "        0.75195312, 0.72043011, 0.71345029, 0.7040715 , 0.67511404,\n",
       "        0.73603083, 0.74012677, 0.71386139, 0.74733785, 0.74477394,\n",
       "        0.73144531, 0.75264678, 0.75254237, 0.73144531, 0.69371598,\n",
       "        0.70675944, 0.69854051, 0.69865067, 0.73014307, 0.71499503,\n",
       "        0.70808679, 0.73333333, 0.71881188, 0.71703704, 0.74411765,\n",
       "        0.71881188, 0.69535114, 0.70558882, 0.69751244, 0.73535156,\n",
       "        0.73268293, 0.70576735, 0.75121477, 0.75048924, 0.70576735,\n",
       "        0.74660194, 0.75134343, 0.70576735, 0.70478068, 0.70075188,\n",
       "        0.69119841, 0.73108834, 0.73112975, 0.71169848, 0.73864191,\n",
       "        0.74288518, 0.71169848, 0.74475354, 0.74288518, 0.71169848,\n",
       "        0.68904245, 0.6913215 , 0.68197703, 0.69940476, 0.70068695,\n",
       "        0.70652174, 0.71477663, 0.71336951, 0.70652174, 0.72493827,\n",
       "        0.71336951, 0.70652174, 0.67713445, 0.70938897, 0.70465808,\n",
       "        0.7086999 , 0.72825024, 0.70118343, 0.7127451 , 0.7243083 ,\n",
       "        0.70118343, 0.71281296, 0.7243083 , 0.70118343, 0.68678872,\n",
       "        0.69375317, 0.68857432, 0.70536585, 0.7202381 , 0.69219144,\n",
       "        0.71611002, 0.72382851, 0.69219144, 0.71835132, 0.72382851,\n",
       "        0.69219144]),\n",
       " 'split4_test_score': array([0.57301108, 0.54690832, 0.46621203, 0.59365709, 0.54625551,\n",
       "        0.3870076 , 0.61796644, 0.59033613, 0.45079787, 0.61124694,\n",
       "        0.59505003, 0.45040214, 0.58074376, 0.59288933, 0.48928571,\n",
       "        0.601     , 0.59237246, 0.45311517, 0.62537764, 0.62950505,\n",
       "        0.50333535, 0.63443513, 0.62572534, 0.50030581, 0.56323605,\n",
       "        0.60333007, 0.50086755, 0.59546599, 0.60793804, 0.48305085,\n",
       "        0.61840121, 0.62924528, 0.51570836, 0.62506191, 0.62411348,\n",
       "        0.51384797, 0.58408862, 0.57494867, 0.52771619, 0.6079602 ,\n",
       "        0.61056751, 0.58666667, 0.62865642, 0.65393795, 0.65200765,\n",
       "        0.64408444, 0.66124402, 0.65028902, 0.58148893, 0.6       ,\n",
       "        0.54115116, 0.61309524, 0.63923445, 0.60796852, 0.64015709,\n",
       "        0.66417212, 0.66291608, 0.63532905, 0.67100977, 0.66134335,\n",
       "        0.56785178, 0.61102977, 0.55689565, 0.60186549, 0.64282397,\n",
       "        0.62391514, 0.63002944, 0.6642269 , 0.65679926, 0.63328424,\n",
       "        0.67586207, 0.65891112, 0.58263027, 0.58156028, 0.55154091,\n",
       "        0.61454183, 0.62167488, 0.60504202, 0.6447434 , 0.65060241,\n",
       "        0.65111851, 0.65506958, 0.66763425, 0.64550265, 0.583     ,\n",
       "        0.60314342, 0.58917836, 0.59149357, 0.64952381, 0.62175168,\n",
       "        0.62327416, 0.67206864, 0.65734266, 0.63924678, 0.66506024,\n",
       "        0.65311909, 0.55437532, 0.60969755, 0.59398125, 0.58187281,\n",
       "        0.64461248, 0.62192178, 0.6027668 , 0.66161137, 0.65740741,\n",
       "        0.60770751, 0.67553444, 0.65373699, 0.57775541, 0.60158573,\n",
       "        0.5840357 , 0.59679037, 0.62756598, 0.5980198 , 0.63663664,\n",
       "        0.65623472, 0.65562914, 0.63992075, 0.66699219, 0.64073894,\n",
       "        0.58553616, 0.59086395, 0.6037001 , 0.6088221 , 0.62341463,\n",
       "        0.61173049, 0.61222494, 0.6615087 , 0.66509656, 0.61290323,\n",
       "        0.65662943, 0.66089466, 0.54638647, 0.59763314, 0.60983926,\n",
       "        0.56407669, 0.62445202, 0.61081344, 0.58097166, 0.65240642,\n",
       "        0.6572902 , 0.58739837, 0.65629558, 0.65395615, 0.69995105,\n",
       "        0.67298106, 0.66305419, 0.71266634, 0.68278244, 0.68439193,\n",
       "        0.71002445, 0.68938272, 0.68242245, 0.71449488, 0.6990099 ,\n",
       "        0.68641975, 0.70749633, 0.66899302, 0.66831194, 0.74143921,\n",
       "        0.70524767, 0.69124877, 0.73059812, 0.70657507, 0.69162562,\n",
       "        0.74656189, 0.71400394, 0.69980315, 0.71400588, 0.67163438,\n",
       "        0.67127345, 0.74793589, 0.70576735, 0.69526627, 0.75798645,\n",
       "        0.71043606, 0.69753695, 0.76015474, 0.72113503, 0.70403147,\n",
       "        0.70912738, 0.68329177, 0.66998507, 0.73156342, 0.71555996,\n",
       "        0.69482152, 0.74571289, 0.72843765, 0.70050251, 0.74647202,\n",
       "        0.73545232, 0.7057634 , 0.69742063, 0.6861167 , 0.67555556,\n",
       "        0.73658296, 0.72293942, 0.70210632, 0.74517566, 0.72997033,\n",
       "        0.7146402 , 0.74591382, 0.73679012, 0.71499503, 0.71231527,\n",
       "        0.6999003 , 0.67419038, 0.74074074, 0.72878862, 0.70313277,\n",
       "        0.75729115, 0.73902319, 0.7202381 , 0.76068796, 0.73838631,\n",
       "        0.72178218, 0.68602904, 0.70158103, 0.68479355, 0.71449488,\n",
       "        0.71907862, 0.71450075, 0.72879684, 0.74430129, 0.70795569,\n",
       "        0.73497537, 0.75061485, 0.70795569, 0.68728522, 0.70243418,\n",
       "        0.68511066, 0.7252208 , 0.73584906, 0.71751129, 0.73715415,\n",
       "        0.74263261, 0.7201995 , 0.74484789, 0.75984252, 0.7201995 ,\n",
       "        0.70219124, 0.70124688, 0.68740592, 0.71442688, 0.73039216,\n",
       "        0.71177945, 0.73441335, 0.73979341, 0.71177945, 0.73647984,\n",
       "        0.73917323, 0.71177945, 0.67168675, 0.69517653, 0.67989822,\n",
       "        0.70243418, 0.72296296, 0.702     , 0.70124688, 0.72602066,\n",
       "        0.702     , 0.71647699, 0.73251834, 0.702     , 0.66966068,\n",
       "        0.69405941, 0.68522267, 0.69625247, 0.70710059, 0.71591472,\n",
       "        0.7055336 , 0.72129539, 0.71591472, 0.70951914, 0.72129539,\n",
       "        0.71591472, 0.68608739, 0.71084929, 0.67698452, 0.71450075,\n",
       "        0.72630044, 0.69474727, 0.72344689, 0.73755437, 0.69474727,\n",
       "        0.7234886 , 0.73755437, 0.69474727, 0.6854096 , 0.65134865,\n",
       "        0.63479145, 0.69641981, 0.6744868 , 0.65810277, 0.70628349,\n",
       "        0.68621701, 0.67219512, 0.7079646 , 0.6895537 , 0.6718903 ,\n",
       "        0.72490347, 0.66699556, 0.64296597, 0.73199412, 0.68534907,\n",
       "        0.66336634, 0.74201474, 0.7055365 , 0.67871094, 0.75319567,\n",
       "        0.70397644, 0.67613915, 0.70980392, 0.66633712, 0.64072764,\n",
       "        0.73046875, 0.68472906, 0.66732673, 0.73352999, 0.7       ,\n",
       "        0.68208661, 0.7407045 , 0.70103093, 0.68470241, 0.70986745,\n",
       "        0.68378651, 0.65494726, 0.73379175, 0.72427572, 0.69649212,\n",
       "        0.73865878, 0.73446894, 0.70308548, 0.74792784, 0.73809524,\n",
       "        0.71077694, 0.69436202, 0.69816012, 0.66900351, 0.72859216,\n",
       "        0.72440945, 0.70342772, 0.73499268, 0.74324988, 0.71385842,\n",
       "        0.75292969, 0.7483998 , 0.71385842, 0.70272953, 0.71053934,\n",
       "        0.66935078, 0.74103194, 0.73271211, 0.70967742, 0.75346535,\n",
       "        0.75219512, 0.72087258, 0.75651746, 0.75499269, 0.7243083 ,\n",
       "        0.71925527, 0.70294558, 0.68704663, 0.72763029, 0.72646624,\n",
       "        0.70588235, 0.73859735, 0.73855244, 0.70588235, 0.73425197,\n",
       "        0.74063116, 0.70588235, 0.6834817 , 0.71      , 0.68926554,\n",
       "        0.71456888, 0.74482759, 0.71995978, 0.72458045, 0.75147348,\n",
       "        0.71995978, 0.7306163 , 0.75824717, 0.71995978, 0.70302429,\n",
       "        0.70676692, 0.69261477, 0.73549656, 0.74019608, 0.70194708,\n",
       "        0.74459725, 0.75098425, 0.70194708, 0.74582924, 0.758485  ,\n",
       "        0.70194708, 0.67099351, 0.69875931, 0.68787276, 0.70119522,\n",
       "        0.72628993, 0.70711089, 0.70588235, 0.72575906, 0.70711089,\n",
       "        0.71342685, 0.73046875, 0.70711089, 0.66930302, 0.709     ,\n",
       "        0.69277108, 0.70384995, 0.74274662, 0.7077378 , 0.72949219,\n",
       "        0.74341463, 0.7077378 , 0.73170732, 0.74341463, 0.7077378 ,\n",
       "        0.65958523, 0.70184171, 0.7077378 , 0.68806419, 0.72359329,\n",
       "        0.71030777, 0.70658683, 0.72346786, 0.71030777, 0.71022444,\n",
       "        0.72346786, 0.71030777, 0.68822394, 0.65187377, 0.62815884,\n",
       "        0.70334928, 0.66305952, 0.65208748, 0.70815244, 0.67447535,\n",
       "        0.6617719 , 0.71276596, 0.68062317, 0.66893866, 0.69513991,\n",
       "        0.65942744, 0.63519754, 0.72895377, 0.67221135, 0.65775136,\n",
       "        0.73362445, 0.67741935, 0.66927211, 0.7411019 , 0.68615984,\n",
       "        0.6731423 , 0.70201276, 0.66206897, 0.63206578, 0.72860636,\n",
       "        0.67768595, 0.6637037 , 0.73011225, 0.68874817, 0.67026497,\n",
       "        0.74135412, 0.69573643, 0.67414634, 0.69456901, 0.69196649,\n",
       "        0.66901763, 0.7343519 , 0.71774592, 0.69902913, 0.74268716,\n",
       "        0.7338908 , 0.71573604, 0.7562777 , 0.74400392, 0.71825999,\n",
       "        0.70888562, 0.68962076, 0.66599088, 0.74372848, 0.71794872,\n",
       "        0.70310933, 0.75636008, 0.74269006, 0.72111554, 0.75707317,\n",
       "        0.74842462, 0.72111554, 0.70996529, 0.69440317, 0.68559838,\n",
       "        0.72861357, 0.71794872, 0.7166998 , 0.74023437, 0.7377129 ,\n",
       "        0.72763223, 0.74975657, 0.74008811, 0.72763223, 0.70219124,\n",
       "        0.68542714, 0.68188737, 0.72763029, 0.71095335, 0.72463768,\n",
       "        0.73917323, 0.73580247, 0.72463768, 0.74263261, 0.73736373,\n",
       "        0.72463768, 0.69404106, 0.71250623, 0.68245968, 0.7308642 ,\n",
       "        0.73076923, 0.71082551, 0.74482759, 0.74263261, 0.71082551,\n",
       "        0.74581281, 0.74609375, 0.71082551, 0.71343284, 0.71421446,\n",
       "        0.69164557, 0.7379891 , 0.73519334, 0.71428571, 0.74877331,\n",
       "        0.75061244, 0.71428571, 0.75501222, 0.75061244, 0.71428571,\n",
       "        0.68743719, 0.69950249, 0.6877193 , 0.70663391, 0.72816008,\n",
       "        0.69776675, 0.71336951, 0.74647202, 0.69776675, 0.71351879,\n",
       "        0.74452555, 0.69776675, 0.69461078, 0.69447236, 0.7038835 ,\n",
       "        0.71414441, 0.72346786, 0.70732907, 0.72951629, 0.72899055,\n",
       "        0.70732907, 0.72970297, 0.72899055, 0.70732907, 0.69634026,\n",
       "        0.69277108, 0.69139466, 0.71562346, 0.7185332 , 0.70039683,\n",
       "        0.72557683, 0.71845621, 0.70039683, 0.72249752, 0.71845621,\n",
       "        0.70039683]),\n",
       " 'mean_test_score': array([0.58027276, 0.5521378 , 0.44935637, 0.59985206, 0.5471544 ,\n",
       "        0.36513764, 0.62599848, 0.58363139, 0.42856485, 0.6238242 ,\n",
       "        0.58234655, 0.40927907, 0.57388844, 0.57968728, 0.48035815,\n",
       "        0.60419433, 0.59211694, 0.43336476, 0.62749457, 0.61920583,\n",
       "        0.49300483, 0.63526158, 0.6199857 , 0.47678219, 0.57549428,\n",
       "        0.58651718, 0.48970588, 0.60161308, 0.60355981, 0.45724423,\n",
       "        0.63021649, 0.62571367, 0.51091496, 0.63068704, 0.62627311,\n",
       "        0.50270994, 0.58434995, 0.57980609, 0.51739588, 0.61266126,\n",
       "        0.60765951, 0.56341924, 0.64202781, 0.64846321, 0.63437883,\n",
       "        0.65030902, 0.65581282, 0.63682442, 0.57228327, 0.59640975,\n",
       "        0.54232715, 0.60987226, 0.62500703, 0.59063824, 0.6379404 ,\n",
       "        0.66126337, 0.64800848, 0.64252703, 0.6661351 , 0.65121626,\n",
       "        0.57413145, 0.60340731, 0.54493428, 0.60347485, 0.63225007,\n",
       "        0.6035972 , 0.62884876, 0.6674787 , 0.64993206, 0.6342687 ,\n",
       "        0.66926315, 0.65429896, 0.58688316, 0.59008783, 0.54686724,\n",
       "        0.61577636, 0.62048642, 0.58962407, 0.64230854, 0.65628758,\n",
       "        0.64273097, 0.64304763, 0.66178326, 0.64241872, 0.56733648,\n",
       "        0.60492324, 0.57536121, 0.60048618, 0.62769273, 0.6009053 ,\n",
       "        0.62660689, 0.65748331, 0.65017754, 0.63264474, 0.66038709,\n",
       "        0.64946192, 0.56945751, 0.602385  , 0.57766079, 0.5933168 ,\n",
       "        0.62574139, 0.60784214, 0.61595256, 0.65544336, 0.65503788,\n",
       "        0.62328088, 0.66146832, 0.65299788, 0.58193568, 0.59909679,\n",
       "        0.57551679, 0.60150767, 0.61540809, 0.59246933, 0.62475111,\n",
       "        0.64928116, 0.64451015, 0.627189  , 0.65434966, 0.63731282,\n",
       "        0.57834084, 0.59316529, 0.58618535, 0.59354339, 0.6098437 ,\n",
       "        0.60146739, 0.60931905, 0.64830896, 0.6446847 , 0.61508382,\n",
       "        0.64376278, 0.63680554, 0.5620152 , 0.59364583, 0.58758873,\n",
       "        0.58296757, 0.6145679 , 0.60332843, 0.60053196, 0.64829806,\n",
       "        0.64664602, 0.60228366, 0.65001059, 0.64334781, 0.69476629,\n",
       "        0.67599965, 0.66379597, 0.70779393, 0.69186307, 0.68239527,\n",
       "        0.71404993, 0.68954148, 0.68222489, 0.7190059 , 0.69879973,\n",
       "        0.68717344, 0.70180944, 0.68003327, 0.66513715, 0.73434547,\n",
       "        0.70216954, 0.68946021, 0.74066711, 0.70057304, 0.69035948,\n",
       "        0.74747631, 0.70873465, 0.69882568, 0.70438819, 0.68182099,\n",
       "        0.66914028, 0.73691141, 0.70509508, 0.69307055, 0.74947066,\n",
       "        0.70723572, 0.69512087, 0.75564384, 0.71572401, 0.70110265,\n",
       "        0.71340899, 0.68567691, 0.67112014, 0.73819456, 0.71792509,\n",
       "        0.70064828, 0.74733291, 0.7273879 , 0.70947137, 0.75067246,\n",
       "        0.73292693, 0.71523767, 0.70808734, 0.69558124, 0.67980811,\n",
       "        0.73354077, 0.72695741, 0.70891561, 0.74692297, 0.73701636,\n",
       "        0.72151258, 0.7540002 , 0.74188661, 0.72523139, 0.70037052,\n",
       "        0.70273574, 0.68078928, 0.73020522, 0.73342903, 0.71539014,\n",
       "        0.74594891, 0.74341703, 0.72689066, 0.74982668, 0.74786798,\n",
       "        0.72904836, 0.69932163, 0.70177447, 0.68948706, 0.71473645,\n",
       "        0.73273871, 0.7147194 , 0.73126731, 0.74574754, 0.72068911,\n",
       "        0.73547938, 0.74560786, 0.72068911, 0.68957532, 0.70638183,\n",
       "        0.69610356, 0.72008448, 0.7349158 , 0.72246158, 0.73477104,\n",
       "        0.74496398, 0.72493068, 0.74186165, 0.7524783 , 0.72493068,\n",
       "        0.68816269, 0.71328275, 0.6948542 , 0.72252034, 0.73676048,\n",
       "        0.72226415, 0.73394108, 0.74538671, 0.72463733, 0.74011305,\n",
       "        0.7488709 , 0.72463733, 0.67626702, 0.69844473, 0.68768644,\n",
       "        0.69645591, 0.72098684, 0.71072563, 0.7057849 , 0.73076601,\n",
       "        0.71072563, 0.71667785, 0.73109458, 0.71072563, 0.66891713,\n",
       "        0.70200138, 0.69481368, 0.69862061, 0.71427169, 0.71730154,\n",
       "        0.70856082, 0.72036566, 0.71730154, 0.71325441, 0.72036566,\n",
       "        0.71730154, 0.67673216, 0.70288694, 0.69718599, 0.70643267,\n",
       "        0.72319424, 0.71565348, 0.71840699, 0.7280416 , 0.71565348,\n",
       "        0.72474467, 0.72791671, 0.71565348, 0.69057126, 0.66570227,\n",
       "        0.64420253, 0.70313162, 0.68171256, 0.6662083 , 0.70943474,\n",
       "        0.69010474, 0.67692331, 0.71308902, 0.69353038, 0.68081634,\n",
       "        0.71037089, 0.6752896 , 0.65372334, 0.73208924, 0.69223416,\n",
       "        0.67410413, 0.74134518, 0.70208856, 0.68505392, 0.74613185,\n",
       "        0.70437581, 0.68790922, 0.70842481, 0.67493377, 0.65463825,\n",
       "        0.73641046, 0.69404883, 0.67629852, 0.74708404, 0.70449534,\n",
       "        0.68872329, 0.74846323, 0.70910705, 0.69076766, 0.70867769,\n",
       "        0.68954601, 0.67058269, 0.73352329, 0.72085637, 0.70376428,\n",
       "        0.74571765, 0.73022878, 0.71429421, 0.75263336, 0.73734353,\n",
       "        0.71918379, 0.7059849 , 0.70468557, 0.67914582, 0.73401442,\n",
       "        0.7314074 , 0.71096062, 0.74568173, 0.74646775, 0.72587975,\n",
       "        0.75240729, 0.74959657, 0.72618509, 0.71153176, 0.71017094,\n",
       "        0.6795548 , 0.74339008, 0.73684835, 0.71940421, 0.75223911,\n",
       "        0.75249288, 0.73077954, 0.75638243, 0.75498683, 0.73188647,\n",
       "        0.70778949, 0.70268965, 0.68753803, 0.72846825, 0.73241513,\n",
       "        0.70936419, 0.73096933, 0.74188448, 0.71108668, 0.73710758,\n",
       "        0.74537327, 0.71108668, 0.69422512, 0.71168143, 0.69977256,\n",
       "        0.72475033, 0.73717277, 0.7224599 , 0.73908313, 0.7485452 ,\n",
       "        0.7224599 , 0.74481798, 0.7521732 , 0.7224599 , 0.70218437,\n",
       "        0.71307135, 0.700112  , 0.7314508 , 0.74065434, 0.71982849,\n",
       "        0.74110047, 0.74607187, 0.71982849, 0.74576874, 0.74710817,\n",
       "        0.71982849, 0.67888791, 0.70186538, 0.69554145, 0.69721294,\n",
       "        0.72210465, 0.70688676, 0.70605324, 0.72772736, 0.70688676,\n",
       "        0.7118094 , 0.72911474, 0.70688676, 0.67603356, 0.70724573,\n",
       "        0.69598045, 0.69688689, 0.7258834 , 0.71230515, 0.71224066,\n",
       "        0.72930072, 0.71230515, 0.71475924, 0.72930072, 0.71230515,\n",
       "        0.67496196, 0.70039984, 0.70498787, 0.70025965, 0.72144623,\n",
       "        0.71108402, 0.71065731, 0.72310109, 0.71057097, 0.71766968,\n",
       "        0.72310109, 0.71057097, 0.68969657, 0.66524801, 0.64449339,\n",
       "        0.7003629 , 0.67687657, 0.662609  , 0.70311391, 0.68556011,\n",
       "        0.6715751 , 0.70767289, 0.68998731, 0.67700077, 0.70224441,\n",
       "        0.67121326, 0.64944142, 0.72579179, 0.68718734, 0.66893487,\n",
       "        0.73315035, 0.6921467 , 0.67596573, 0.73857583, 0.69650176,\n",
       "        0.68167446, 0.70342172, 0.67371199, 0.64898337, 0.72966663,\n",
       "        0.69475516, 0.67297767, 0.73833246, 0.69812068, 0.67853117,\n",
       "        0.7442599 , 0.70239757, 0.68377391, 0.70564071, 0.69005528,\n",
       "        0.67598013, 0.73334207, 0.72056777, 0.69865931, 0.7453385 ,\n",
       "        0.73343384, 0.71348808, 0.75221153, 0.740196  , 0.71731799,\n",
       "        0.71537606, 0.7022478 , 0.68066321, 0.73882297, 0.72789917,\n",
       "        0.71223114, 0.75137802, 0.74080106, 0.72089709, 0.75366956,\n",
       "        0.74818132, 0.72089709, 0.71423484, 0.70302251, 0.67944233,\n",
       "        0.74030119, 0.73086015, 0.71775522, 0.75397796, 0.74310781,\n",
       "        0.7290922 , 0.75940016, 0.74850617, 0.7290922 , 0.70604597,\n",
       "        0.70333273, 0.68642126, 0.72024391, 0.7280371 , 0.71804036,\n",
       "        0.72917532, 0.73810451, 0.71946478, 0.73650812, 0.74289489,\n",
       "        0.71946478, 0.69871946, 0.70892451, 0.69748765, 0.73123786,\n",
       "        0.73602007, 0.71740082, 0.74755871, 0.7424308 , 0.71740082,\n",
       "        0.7502488 , 0.74684776, 0.71740082, 0.70665913, 0.71149525,\n",
       "        0.69831604, 0.73304265, 0.73348713, 0.71832764, 0.74318408,\n",
       "        0.7462095 , 0.71832764, 0.74657033, 0.74820647, 0.71832764,\n",
       "        0.68415112, 0.69951206, 0.69508574, 0.69484711, 0.71529189,\n",
       "        0.70929223, 0.70987141, 0.73186873, 0.70929223, 0.71480419,\n",
       "        0.72988305, 0.70929223, 0.68798194, 0.70295487, 0.70563492,\n",
       "        0.70510093, 0.72373962, 0.70707653, 0.71765102, 0.72657784,\n",
       "        0.70707653, 0.72174442, 0.72657784, 0.70707653, 0.69562528,\n",
       "        0.69718206, 0.69225972, 0.71486563, 0.72491619, 0.70377023,\n",
       "        0.7223516 , 0.72698147, 0.70377023, 0.72229997, 0.72698147,\n",
       "        0.70377023]),\n",
       " 'std_test_score': array([0.01795917, 0.02510397, 0.01309267, 0.00504204, 0.0202505 ,\n",
       "        0.02524086, 0.01059227, 0.01698641, 0.0300623 , 0.01056796,\n",
       "        0.01615739, 0.03738793, 0.00985042, 0.01564989, 0.00819102,\n",
       "        0.00831429, 0.00854964, 0.01602152, 0.00946181, 0.01040923,\n",
       "        0.01061991, 0.01297515, 0.00919371, 0.01891184, 0.0138303 ,\n",
       "        0.01837854, 0.00990454, 0.00697242, 0.00694847, 0.01540753,\n",
       "        0.0071304 , 0.0078762 , 0.01044681, 0.00921908, 0.01072448,\n",
       "        0.01521552, 0.01414568, 0.02317693, 0.01993333, 0.00585076,\n",
       "        0.01169112, 0.01413505, 0.0085167 , 0.01045059, 0.01146916,\n",
       "        0.01150437, 0.01358887, 0.01107265, 0.01150519, 0.01126371,\n",
       "        0.01837362, 0.01150579, 0.01393327, 0.01463212, 0.00911005,\n",
       "        0.00965917, 0.01651228, 0.00599071, 0.01047648, 0.01440015,\n",
       "        0.00583899, 0.01367897, 0.01231706, 0.00666803, 0.01164345,\n",
       "        0.01447575, 0.01096901, 0.01302513, 0.01282782, 0.01055739,\n",
       "        0.01064901, 0.0148842 , 0.00983926, 0.02259503, 0.01416399,\n",
       "        0.01336971, 0.01195246, 0.01124098, 0.01197147, 0.01002429,\n",
       "        0.0120067 , 0.01608282, 0.01428869, 0.00997019, 0.01703795,\n",
       "        0.00645653, 0.01297533, 0.01052179, 0.01218035, 0.01385999,\n",
       "        0.01385382, 0.00794536, 0.00756995, 0.01133844, 0.00633424,\n",
       "        0.00792789, 0.01125972, 0.01175331, 0.01485392, 0.00749102,\n",
       "        0.01542918, 0.01080272, 0.00772827, 0.00657328, 0.00404882,\n",
       "        0.0087319 , 0.01037661, 0.00839991, 0.00975944, 0.00627628,\n",
       "        0.00531842, 0.01108177, 0.01411433, 0.0064785 , 0.00954557,\n",
       "        0.00689777, 0.00844436, 0.00787594, 0.00814743, 0.00544476,\n",
       "        0.00932736, 0.01047323, 0.01213784, 0.01018609, 0.01062151,\n",
       "        0.01149826, 0.00455632, 0.00784821, 0.01258901, 0.00304516,\n",
       "        0.00913885, 0.0136479 , 0.01077503, 0.0062838 , 0.01604784,\n",
       "        0.01565094, 0.00850449, 0.01404245, 0.01376682, 0.0048322 ,\n",
       "        0.01445548, 0.01241907, 0.01095743, 0.01404362, 0.01265763,\n",
       "        0.0085367 , 0.00614046, 0.01020778, 0.01071101, 0.00723233,\n",
       "        0.00721454, 0.01346905, 0.004295  , 0.00648006, 0.01699281,\n",
       "        0.00751451, 0.01022162, 0.0077604 , 0.005976  , 0.01223498,\n",
       "        0.00848724, 0.00628638, 0.01374716, 0.00888741, 0.00539081,\n",
       "        0.01032678, 0.01193977, 0.00594156, 0.00998147, 0.00695261,\n",
       "        0.00630951, 0.01491135, 0.00976555, 0.0076523 , 0.0121886 ,\n",
       "        0.01029789, 0.00676084, 0.01061798, 0.01519479, 0.00657819,\n",
       "        0.00693566, 0.00494957, 0.00712192, 0.00604488, 0.01171623,\n",
       "        0.00718027, 0.00449509, 0.00939022, 0.01096521, 0.00538372,\n",
       "        0.00845516, 0.01040813, 0.01238036, 0.00894187, 0.00461374,\n",
       "        0.01337742, 0.00750998, 0.00779503, 0.01374729, 0.00561643,\n",
       "        0.00701849, 0.01010352, 0.00740606, 0.01005596, 0.01043625,\n",
       "        0.00245611, 0.00860129, 0.00932391, 0.00458979, 0.00909068,\n",
       "        0.01420322, 0.00600688, 0.00814799, 0.0145563 , 0.00658206,\n",
       "        0.00687577, 0.00807338, 0.00948448, 0.00946694, 0.0061035 ,\n",
       "        0.01348107, 0.01030254, 0.00868268, 0.01094461, 0.00912592,\n",
       "        0.0125416 , 0.01192002, 0.00912592, 0.00614173, 0.00621468,\n",
       "        0.0085255 , 0.00732173, 0.00820833, 0.0107316 , 0.00744122,\n",
       "        0.00596474, 0.0088992 , 0.00791665, 0.0084212 , 0.0088992 ,\n",
       "        0.01544035, 0.0083218 , 0.00664593, 0.00743731, 0.00967965,\n",
       "        0.00925358, 0.00717906, 0.0052315 , 0.01035802, 0.00916379,\n",
       "        0.00797952, 0.01035802, 0.00730341, 0.0122997 , 0.00788748,\n",
       "        0.00798235, 0.0089306 , 0.00895178, 0.00644564, 0.00949974,\n",
       "        0.00895178, 0.00940854, 0.00662243, 0.00895178, 0.00701275,\n",
       "        0.00793976, 0.00721845, 0.00760276, 0.00900564, 0.00379463,\n",
       "        0.00330002, 0.01163878, 0.00379463, 0.00434946, 0.01163878,\n",
       "        0.00379463, 0.0082994 , 0.01279572, 0.01458815, 0.01260251,\n",
       "        0.00693234, 0.01536126, 0.00924061, 0.00729758, 0.01536126,\n",
       "        0.00940059, 0.0071742 , 0.01536126, 0.01176532, 0.00925675,\n",
       "        0.00888216, 0.01069856, 0.01209455, 0.00805497, 0.00931687,\n",
       "        0.01229768, 0.00956098, 0.00765851, 0.01232095, 0.00894236,\n",
       "        0.01330327, 0.00710792, 0.01007906, 0.01532322, 0.00956427,\n",
       "        0.00960666, 0.01365861, 0.01087599, 0.00888513, 0.01458209,\n",
       "        0.01189809, 0.0100628 , 0.00658606, 0.00701348, 0.01287154,\n",
       "        0.01329442, 0.00778514, 0.00970999, 0.01137996, 0.00895338,\n",
       "        0.00739025, 0.01108869, 0.00976156, 0.00759413, 0.00808992,\n",
       "        0.00897084, 0.00932095, 0.01150919, 0.01140775, 0.00824499,\n",
       "        0.00962389, 0.01547381, 0.00827466, 0.00996326, 0.01516703,\n",
       "        0.00921289, 0.01211247, 0.00664849, 0.00861312, 0.01008613,\n",
       "        0.00736277, 0.00933545, 0.01208772, 0.0038495 , 0.00664841,\n",
       "        0.01178684, 0.00800353, 0.00695382, 0.00762091, 0.00412812,\n",
       "        0.00637925, 0.00623329, 0.0065905 , 0.00690686, 0.008645  ,\n",
       "        0.0077461 , 0.00864791, 0.00920322, 0.00970906, 0.00819079,\n",
       "        0.00964748, 0.01039774, 0.01230187, 0.01122679, 0.00857029,\n",
       "        0.01011475, 0.01074514, 0.01120187, 0.01249177, 0.00573388,\n",
       "        0.01185103, 0.01249177, 0.0084151 , 0.00935045, 0.00913004,\n",
       "        0.00972655, 0.01128155, 0.00683949, 0.01297997, 0.01045401,\n",
       "        0.00683949, 0.01179616, 0.01021082, 0.00683949, 0.00704127,\n",
       "        0.01306267, 0.00830529, 0.01102173, 0.00928379, 0.01530987,\n",
       "        0.01034941, 0.00899871, 0.01530987, 0.0100732 , 0.01215317,\n",
       "        0.01530987, 0.00855756, 0.00884255, 0.00612729, 0.00666497,\n",
       "        0.0090183 , 0.00740556, 0.00709522, 0.0074111 , 0.00740556,\n",
       "        0.01166954, 0.00941047, 0.00740556, 0.00397684, 0.00714595,\n",
       "        0.00957624, 0.01057842, 0.01211196, 0.00529742, 0.01434047,\n",
       "        0.00852416, 0.00529742, 0.01350897, 0.00852416, 0.00529742,\n",
       "        0.01218643, 0.01289486, 0.01053231, 0.01401078, 0.0086563 ,\n",
       "        0.00527348, 0.00936159, 0.0088566 , 0.0062025 , 0.01136608,\n",
       "        0.0088566 , 0.0062025 , 0.01399668, 0.01388614, 0.01175747,\n",
       "        0.0069833 , 0.01438283, 0.00873724, 0.00822288, 0.0131235 ,\n",
       "        0.00765189, 0.00716057, 0.01331867, 0.00836578, 0.00859116,\n",
       "        0.00913423, 0.01162573, 0.00680061, 0.01113201, 0.0082522 ,\n",
       "        0.0104327 , 0.01407193, 0.00511521, 0.00754639, 0.01327909,\n",
       "        0.00761908, 0.00543554, 0.00881137, 0.01210676, 0.01150702,\n",
       "        0.00943031, 0.00657266, 0.01140716, 0.00800657, 0.00730451,\n",
       "        0.01073718, 0.01060906, 0.00741272, 0.01136948, 0.01548985,\n",
       "        0.00632543, 0.01159531, 0.01276147, 0.00979815, 0.01052606,\n",
       "        0.01153244, 0.00764441, 0.01333957, 0.00821287, 0.0067134 ,\n",
       "        0.00931396, 0.01309321, 0.01285634, 0.00937635, 0.00765489,\n",
       "        0.00696455, 0.01068758, 0.01468747, 0.00118547, 0.01305307,\n",
       "        0.0139604 , 0.00118547, 0.0086679 , 0.00937484, 0.00662488,\n",
       "        0.01139322, 0.00866344, 0.0055958 , 0.01192298, 0.00844876,\n",
       "        0.00679037, 0.0100106 , 0.01184544, 0.00679037, 0.00993388,\n",
       "        0.01739153, 0.01094796, 0.01571388, 0.01251022, 0.01192329,\n",
       "        0.01307626, 0.011949  , 0.01262887, 0.01106189, 0.00892662,\n",
       "        0.01262887, 0.00691441, 0.00895455, 0.01160099, 0.01104249,\n",
       "        0.00795382, 0.00809682, 0.01282045, 0.01230987, 0.00809682,\n",
       "        0.01180393, 0.01245097, 0.00809682, 0.01403864, 0.0082746 ,\n",
       "        0.01481944, 0.01174908, 0.01212105, 0.00764153, 0.00846374,\n",
       "        0.00909485, 0.00764153, 0.00833208, 0.01051434, 0.00764153,\n",
       "        0.00614496, 0.00459538, 0.01196158, 0.01011392, 0.00938386,\n",
       "        0.00824194, 0.00401798, 0.01400714, 0.00824194, 0.01015691,\n",
       "        0.01261666, 0.00824194, 0.00913   , 0.01383086, 0.00691708,\n",
       "        0.0101113 , 0.00743816, 0.00447775, 0.01049529, 0.00963202,\n",
       "        0.00447775, 0.009893  , 0.00963202, 0.00447775, 0.01233066,\n",
       "        0.01121278, 0.01015418, 0.0114201 , 0.00735322, 0.00831596,\n",
       "        0.01212986, 0.01133548, 0.00831596, 0.01063418, 0.01133548,\n",
       "        0.00831596]),\n",
       " 'rank_test_score': array([544, 559, 572, 523, 560, 576, 487, 540, 574, 492, 542, 575, 553,\n",
       "        546, 569, 509, 531, 573, 483, 496, 567, 474, 495, 570, 550, 537,\n",
       "        568, 517, 511, 571, 480, 489, 565, 479, 486, 566, 539, 545, 564,\n",
       "        502, 507, 557, 469, 453, 475, 445, 436, 472, 554, 525, 563, 503,\n",
       "        490, 532, 470, 432, 456, 466, 424, 444, 552, 513, 562, 512, 478,\n",
       "        510, 481, 422, 448, 476, 418, 441, 536, 533, 561, 498, 494, 534,\n",
       "        468, 435, 465, 464, 430, 467, 556, 508, 551, 522, 482, 520, 485,\n",
       "        434, 446, 477, 433, 449, 555, 515, 548, 528, 488, 506, 497, 437,\n",
       "        438, 493, 431, 443, 543, 524, 549, 518, 499, 530, 491, 451, 459,\n",
       "        484, 440, 471, 547, 529, 538, 527, 504, 519, 505, 454, 458, 500,\n",
       "        462, 473, 558, 526, 535, 541, 501, 514, 521, 455, 457, 516, 447,\n",
       "        463, 347, 405, 428, 261, 356, 383, 218, 366, 384, 184, 323, 376,\n",
       "        309, 391, 427,  88, 305, 368,  64, 313, 359,  29, 256, 322, 286,\n",
       "        385, 419,  79, 282, 352,  20, 265, 342,   3, 202, 311, 220, 378,\n",
       "        416,  73, 190, 312,  30, 131, 247,  16,  99, 209, 260, 340, 392,\n",
       "         91, 134, 255,  33,  78, 164,   5,  58, 142, 315, 299, 389, 115,\n",
       "         95, 206,  40,  52, 135,  18,  27, 124, 321, 310, 367, 213, 100,\n",
       "        214, 107,  42, 170,  85,  45, 170, 364, 274, 337, 176,  86, 155,\n",
       "         87,  49, 143,  60,  10, 143, 370, 221, 344, 154,  81, 161,  90,\n",
       "         46, 148,  68,  21, 148, 403, 327, 373, 336, 166, 238, 278, 113,\n",
       "        238, 201, 109, 238, 421, 307, 346, 326, 216, 198, 258, 173, 198,\n",
       "        222, 173, 198, 401, 298, 332, 273, 151, 203, 185, 126, 203, 147,\n",
       "        128, 203, 358, 425, 461, 294, 386, 423, 248, 360, 399, 223, 351,\n",
       "        388, 244, 408, 442, 102, 354, 411,  61, 306, 380,  38, 287, 372,\n",
       "        259, 410, 439,  83, 350, 402,  32, 285, 369,  24, 253, 357, 257,\n",
       "        365, 417,  92, 169, 291,  43, 114, 215,   8,  75, 183, 277, 284,\n",
       "        395,  89, 106, 237,  44,  36, 140,  11,  19, 138, 232, 245, 393,\n",
       "         53,  80, 182,  12,   9, 112,   2,   4, 103, 262, 300, 374, 125,\n",
       "        101, 249, 110,  59, 234,  77,  47, 234, 349, 231, 319, 146,  76,\n",
       "        156,  69,  22, 156,  50,  14, 156, 304, 224, 318, 105,  65, 177,\n",
       "         62,  39, 177,  41,  31, 177, 396, 308, 341, 331, 162, 269, 275,\n",
       "        130, 269, 230, 121, 269, 404, 264, 338, 334, 139, 225, 228, 118,\n",
       "        225, 212, 118, 225, 409, 314, 283, 317, 165, 236, 241, 152, 242,\n",
       "        192, 152, 242, 363, 426, 460, 316, 400, 429, 295, 379, 414, 263,\n",
       "        362, 398, 303, 415, 450, 141, 375, 420,  97, 355, 407,  71, 335,\n",
       "        387, 292, 412, 452, 117, 348, 413,  72, 329, 397,  51, 301, 382,\n",
       "        279, 361, 406,  96, 172, 325,  48,  94, 219,  13,  67, 197, 207,\n",
       "        302, 390,  70, 129, 229,  15,  63, 167,   7,  26, 167, 217, 296,\n",
       "        394,  66, 111, 191,   6,  55, 122,   1,  23, 122, 276, 293, 377,\n",
       "        175, 127, 189, 120,  74, 180,  82,  56, 180, 324, 254, 330, 108,\n",
       "         84, 194,  28,  57, 194,  17,  34, 194, 272, 233, 328,  98,  93,\n",
       "        186,  54,  37, 186,  35,  25, 186, 381, 320, 343, 345, 208, 250,\n",
       "        246, 104, 250, 211, 116, 250, 371, 297, 280, 281, 150, 266, 193,\n",
       "        136, 266, 163, 136, 266, 339, 333, 353, 210, 145, 288, 159, 132,\n",
       "        288, 160, 132, 288], dtype=int32),\n",
       " 'split0_train_score': array([0.71245186, 0.64209305, 0.50649961, 0.75439939, 0.66123865,\n",
       "        0.47917015, 0.79113275, 0.69299163, 0.52586771, 0.79605179,\n",
       "        0.69662921, 0.51823352, 0.89455023, 0.68563057, 0.55216741,\n",
       "        0.93866317, 0.70319176, 0.55100775, 0.95239337, 0.72443077,\n",
       "        0.59975853, 0.95597812, 0.72977625, 0.60695309, 0.96745259,\n",
       "        0.6988595 , 0.57400932, 0.98192924, 0.7140913 , 0.58040389,\n",
       "        0.98708534, 0.73828267, 0.63263818, 0.98772618, 0.74300868,\n",
       "        0.63444821, 0.72960284, 0.65983708, 0.56903884, 0.80899451,\n",
       "        0.69824518, 0.64135575, 0.87624141, 0.74452555, 0.70178878,\n",
       "        0.90112672, 0.7674113 , 0.709983  , 0.93283678, 0.6881278 ,\n",
       "        0.60754304, 0.98799747, 0.73592445, 0.67963618, 0.99711743,\n",
       "        0.78578199, 0.72438672, 0.99949987, 0.8052506 , 0.73098507,\n",
       "        0.98825313, 0.70247435, 0.62433012, 0.99962495, 0.74762077,\n",
       "        0.69639539, 0.99974994, 0.79541657, 0.73549001, 0.99975   ,\n",
       "        0.82229132, 0.74123066, 0.74590892, 0.65903778, 0.60173218,\n",
       "        0.84730311, 0.71552673, 0.66749318, 0.91351826, 0.75862904,\n",
       "        0.70972389, 0.94408225, 0.78383299, 0.70951282, 0.95871967,\n",
       "        0.70317766, 0.63483575, 0.99837358, 0.75584226, 0.68774799,\n",
       "        0.99975006, 0.80821752, 0.72564284, 0.99975   , 0.82895533,\n",
       "        0.72629824, 0.99787207, 0.71368497, 0.65032516, 0.99975   ,\n",
       "        0.76631932, 0.69859423, 0.99975   , 0.81776268, 0.73434379,\n",
       "        0.99974994, 0.84355231, 0.74263493, 0.74891665, 0.66788856,\n",
       "        0.62033169, 0.87224114, 0.72680476, 0.65970037, 0.94378365,\n",
       "        0.77102231, 0.71068054, 0.97192279, 0.79416852, 0.71406156,\n",
       "        0.97415307, 0.69378698, 0.63139418, 0.99975   , 0.76575799,\n",
       "        0.68185634, 0.99974994, 0.81840491, 0.72679489, 0.99975   ,\n",
       "        0.8378144 , 0.73359743, 0.9995    , 0.70171119, 0.63553045,\n",
       "        0.99974994, 0.78037497, 0.69354452, 0.99975006, 0.83080715,\n",
       "        0.73990385, 0.99974994, 0.85050655, 0.74602979, 0.76226142,\n",
       "        0.69906542, 0.68898462, 0.78812416, 0.72880935, 0.70742465,\n",
       "        0.78820507, 0.72758749, 0.70544919, 0.80196199, 0.73868974,\n",
       "        0.7118017 , 0.93354903, 0.73415184, 0.70444746, 0.96983409,\n",
       "        0.76907756, 0.73544909, 0.97361809, 0.77145684, 0.73211437,\n",
       "        0.97836478, 0.78690139, 0.74094981, 0.98871049, 0.7530648 ,\n",
       "        0.7192638 , 0.99535701, 0.79693107, 0.75289811, 0.99674185,\n",
       "        0.79729064, 0.74868017, 0.99736941, 0.81603138, 0.76160157,\n",
       "        0.8092828 , 0.72891865, 0.71311271, 0.88933101, 0.77286793,\n",
       "        0.74670644, 0.93945459, 0.79901112, 0.75932883, 0.95721791,\n",
       "        0.81465891, 0.77391956, 0.98029866, 0.78093132, 0.7386865 ,\n",
       "        0.99924962, 0.83708282, 0.7852291 , 0.99974994, 0.86248923,\n",
       "        0.80875263, 0.99974994, 0.88141026, 0.81226148, 0.99849812,\n",
       "        0.80108055, 0.75904963, 0.99975   , 0.86134091, 0.80608083,\n",
       "        0.99974994, 0.8892197 , 0.82574917, 0.99974994, 0.90489198,\n",
       "        0.82574917, 0.83393637, 0.75474453, 0.72819623, 0.93956386,\n",
       "        0.80427046, 0.77328036, 0.98225887, 0.83949092, 0.78067466,\n",
       "        0.99361302, 0.86088461, 0.78067466, 0.99436302, 0.80088441,\n",
       "        0.7593478 , 0.99974994, 0.86594876, 0.80724673, 0.99975006,\n",
       "        0.9001114 , 0.81531642, 0.99974994, 0.91607959, 0.81531642,\n",
       "        0.99962486, 0.81761787, 0.76909679, 0.99975   , 0.88762235,\n",
       "        0.81925028, 0.99975006, 0.91964286, 0.828196  , 0.99975006,\n",
       "        0.93389662, 0.828196  , 0.86516153, 0.76691167, 0.74090795,\n",
       "        0.98037745, 0.83737713, 0.77825   , 0.99937477, 0.87624008,\n",
       "        0.77825   , 0.99974994, 0.89101767, 0.77825   , 0.99975   ,\n",
       "        0.82259663, 0.77831897, 0.99975   , 0.9017025 , 0.81880847,\n",
       "        0.99975006, 0.92342118, 0.81880847, 0.99975   , 0.92342118,\n",
       "        0.81880847, 0.99975006, 0.83645911, 0.79253731, 0.99975   ,\n",
       "        0.91558765, 0.81775586, 0.99975006, 0.94004736, 0.81775586,\n",
       "        0.99974994, 0.94004736, 0.81775586, 0.74413569, 0.6863496 ,\n",
       "        0.67361628, 0.76490466, 0.70557749, 0.69407497, 0.77263337,\n",
       "        0.71544515, 0.70261122, 0.77891905, 0.72279921, 0.70629625,\n",
       "        0.93084388, 0.72217418, 0.68578554, 0.95985037, 0.75154055,\n",
       "        0.71563636, 0.97062516, 0.75736016, 0.72285369, 0.97463586,\n",
       "        0.76947614, 0.7290856 , 0.98313554, 0.73753411, 0.69641077,\n",
       "        0.99398647, 0.77268269, 0.72784656, 0.99598796, 0.78000489,\n",
       "        0.73658714, 0.99711743, 0.79314985, 0.74302425, 0.79829891,\n",
       "        0.72385252, 0.7044279 , 0.89231909, 0.78103277, 0.74792518,\n",
       "        0.94389027, 0.8060184 , 0.76526931, 0.96615461, 0.82154799,\n",
       "        0.7716555 , 0.97847456, 0.7771932 , 0.72975038, 0.99912445,\n",
       "        0.83738225, 0.78102551, 0.99975   , 0.871314  , 0.80418977,\n",
       "        0.99975   , 0.88715084, 0.80418977, 0.99862448, 0.8       ,\n",
       "        0.74622736, 0.99975   , 0.86728926, 0.80098462, 0.99975   ,\n",
       "        0.89914825, 0.82941249, 0.99975006, 0.91046627, 0.83236852,\n",
       "        0.83482587, 0.7560915 , 0.72006141, 0.93733583, 0.81128357,\n",
       "        0.76691167, 0.98096669, 0.84585956, 0.77327028, 0.99412427,\n",
       "        0.86194537, 0.77327028, 0.99649474, 0.80004919, 0.75935695,\n",
       "        0.99975   , 0.86609898, 0.8088581 , 0.99974994, 0.90108268,\n",
       "        0.8088581 , 0.99974994, 0.91487787, 0.8088581 , 0.99962505,\n",
       "        0.82738909, 0.76638781, 0.99975   , 0.89667988, 0.82138458,\n",
       "        0.99975   , 0.92942051, 0.82138458, 0.99974994, 0.93735269,\n",
       "        0.82138458, 0.86805642, 0.77069267, 0.7382667 , 0.97786116,\n",
       "        0.84376171, 0.77377377, 0.99949987, 0.8794926 , 0.77377377,\n",
       "        0.99975   , 0.88897178, 0.77377377, 0.99962486, 0.82313682,\n",
       "        0.76913503, 0.99975006, 0.89733085, 0.79085373, 0.99975   ,\n",
       "        0.91603621, 0.79085373, 0.99974994, 0.91603621, 0.79085373,\n",
       "        0.99975006, 0.855999  , 0.79246219, 0.99975   , 0.92994266,\n",
       "        0.80896372, 0.99975   , 0.93619143, 0.80896372, 0.99975   ,\n",
       "        0.93619143, 0.80896372, 0.73726893, 0.68004896, 0.67051172,\n",
       "        0.74809345, 0.69756634, 0.68665448, 0.75525325, 0.70589672,\n",
       "        0.69124424, 0.76272628, 0.71089932, 0.69928545, 0.93110236,\n",
       "        0.70989255, 0.68283674, 0.94833582, 0.73695812, 0.70179699,\n",
       "        0.95972066, 0.74432788, 0.71097635, 0.96725   , 0.75094386,\n",
       "        0.7185716 , 0.98064516, 0.72711464, 0.68667408, 0.99098422,\n",
       "        0.75706768, 0.71300122, 0.99386196, 0.76535972, 0.72319806,\n",
       "        0.99574042, 0.77503346, 0.7355402 , 0.8045779 , 0.72878   ,\n",
       "        0.70132467, 0.89374922, 0.77714709, 0.7400327 , 0.94256883,\n",
       "        0.81047043, 0.76387676, 0.96544842, 0.82574257, 0.77218642,\n",
       "        0.98253493, 0.77731196, 0.72944396, 0.99937477, 0.83858268,\n",
       "        0.78876377, 0.99975   , 0.87419832, 0.80152484, 0.99975   ,\n",
       "        0.89400124, 0.80152484, 0.99912489, 0.79975385, 0.73833671,\n",
       "        0.99974994, 0.86641975, 0.80482647, 0.99975   , 0.90014793,\n",
       "        0.82288963, 0.99974994, 0.91650246, 0.82288963, 0.83676269,\n",
       "        0.75587029, 0.71557994, 0.94719801, 0.81511772, 0.7666001 ,\n",
       "        0.98500375, 0.84931846, 0.7666001 , 0.99424712, 0.86668316,\n",
       "        0.7666001 , 0.99363057, 0.80833744, 0.76300869, 0.99974994,\n",
       "        0.872103  , 0.79792105, 0.99975   , 0.90717822, 0.79792105,\n",
       "        0.99975   , 0.92059553, 0.79792105, 0.99975006, 0.82514881,\n",
       "        0.77334005, 0.99975006, 0.89415248, 0.81211219, 0.99975   ,\n",
       "        0.93412434, 0.81211219, 0.99974994, 0.93787973, 0.81211219,\n",
       "        0.86894658, 0.78131582, 0.74959248, 0.98047071, 0.84953329,\n",
       "        0.77061951, 0.9995    , 0.88468244, 0.77061951, 0.99975   ,\n",
       "        0.89306143, 0.77061951, 0.99975   , 0.82690132, 0.78693817,\n",
       "        0.99974994, 0.90168645, 0.79806859, 0.99975   , 0.90775733,\n",
       "        0.79806859, 0.99975   , 0.90775733, 0.79806859, 0.99975006,\n",
       "        0.85084404, 0.80130457, 0.99974994, 0.92972838, 0.81259296,\n",
       "        0.99975006, 0.93703148, 0.81259296, 0.99975   , 0.93703148,\n",
       "        0.81259296]),\n",
       " 'split1_train_score': array([0.7062621 , 0.61175833, 0.49330427, 0.75560134, 0.63742849,\n",
       "        0.4568112 , 0.78180178, 0.66845063, 0.53017596, 0.791095  ,\n",
       "        0.67580516, 0.50256235, 0.88563514, 0.65995612, 0.5409348 ,\n",
       "        0.93792741, 0.69145074, 0.54082748, 0.9503268 , 0.71416051,\n",
       "        0.60991412, 0.95376176, 0.71339641, 0.59571847, 0.96596018,\n",
       "        0.67633338, 0.56564774, 0.97985208, 0.70534726, 0.56661097,\n",
       "        0.98309394, 0.72346676, 0.63283756, 0.98399797, 0.72316522,\n",
       "        0.62445871, 0.73763195, 0.65123301, 0.57718306, 0.81129196,\n",
       "        0.69552349, 0.63534137, 0.87048994, 0.74207163, 0.70662539,\n",
       "        0.89994947, 0.76437266, 0.71903472, 0.93509492, 0.6863012 ,\n",
       "        0.61220707, 0.98992189, 0.73537872, 0.67680223, 0.99774606,\n",
       "        0.78374525, 0.72547825, 0.99962495, 0.79765746, 0.73194144,\n",
       "        0.98725874, 0.69915152, 0.62382652, 0.99924962, 0.75119389,\n",
       "        0.69411474, 0.99974994, 0.79805179, 0.73405797, 0.99975   ,\n",
       "        0.81435407, 0.73700636, 0.74163592, 0.66335897, 0.60107852,\n",
       "        0.841306  , 0.71508652, 0.65829528, 0.90625   , 0.76854742,\n",
       "        0.7075506 , 0.93484848, 0.78394535, 0.71231884, 0.95319257,\n",
       "        0.68811942, 0.63392289, 0.99737139, 0.75365795, 0.67858905,\n",
       "        0.99975   , 0.80192887, 0.72744539, 0.99975   , 0.82232762,\n",
       "        0.73616718, 0.99460409, 0.69553517, 0.64690299, 0.99975006,\n",
       "        0.76646128, 0.68415247, 0.99975   , 0.81325082, 0.73546856,\n",
       "        0.99974994, 0.83756283, 0.7434242 , 0.74965457, 0.66796875,\n",
       "        0.62021169, 0.86715402, 0.72449229, 0.6635    , 0.93977415,\n",
       "        0.77088685, 0.7097006 , 0.96766107, 0.79028862, 0.7053637 ,\n",
       "        0.97351575, 0.69870789, 0.64677479, 0.99975006, 0.76527966,\n",
       "        0.68460316, 0.99975   , 0.81393643, 0.73664122, 0.99974994,\n",
       "        0.83575059, 0.74098683, 0.99962495, 0.70250368, 0.65658168,\n",
       "        0.99975   , 0.77824063, 0.69184987, 0.99975006, 0.82839218,\n",
       "        0.73638762, 0.99975   , 0.84777228, 0.74164354, 0.7582553 ,\n",
       "        0.70040938, 0.66991162, 0.78859975, 0.73238735, 0.70085036,\n",
       "        0.79270397, 0.73023371, 0.70049474, 0.79866518, 0.73794132,\n",
       "        0.71387101, 0.93552434, 0.73362445, 0.69372129, 0.96630064,\n",
       "        0.77005215, 0.72830331, 0.9718239 , 0.76661284, 0.7311285 ,\n",
       "        0.97485786, 0.7809902 , 0.74339386, 0.98421119, 0.74405804,\n",
       "        0.70619672, 0.99358733, 0.7947609 , 0.7479287 , 0.99547966,\n",
       "        0.79823161, 0.74736578, 0.99598394, 0.8098401 , 0.76156051,\n",
       "        0.80913978, 0.71426769, 0.69461685, 0.88592777, 0.7714215 ,\n",
       "        0.74051821, 0.93520491, 0.79713191, 0.75745105, 0.96052137,\n",
       "        0.81853091, 0.76522175, 0.97514436, 0.77246323, 0.72484237,\n",
       "        0.99924981, 0.83509202, 0.77865418, 0.99975   , 0.86248923,\n",
       "        0.80221811, 0.99975006, 0.88118812, 0.81203008, 0.99736875,\n",
       "        0.79823378, 0.74244713, 0.99975   , 0.85574213, 0.79692727,\n",
       "        0.99975006, 0.88057129, 0.82148538, 0.99975006, 0.90013579,\n",
       "        0.82148538, 0.83414089, 0.75024704, 0.7251066 , 0.94311377,\n",
       "        0.80490524, 0.76668738, 0.98323743, 0.83896424, 0.77396245,\n",
       "        0.99323647, 0.85529908, 0.77396245, 0.99185566, 0.80009787,\n",
       "        0.76218612, 0.99975006, 0.86402162, 0.80398917, 0.99975006,\n",
       "        0.89564036, 0.80398917, 0.99974994, 0.91344246, 0.80398917,\n",
       "        0.99962495, 0.81837161, 0.77428147, 0.99974994, 0.88776016,\n",
       "        0.81792441, 0.99974994, 0.92170244, 0.81792441, 0.99975   ,\n",
       "        0.9340632 , 0.81792441, 0.86859296, 0.7728343 , 0.73149087,\n",
       "        0.97873405, 0.83792201, 0.77094419, 0.99887401, 0.87440817,\n",
       "        0.77094419, 0.99975006, 0.88905454, 0.77094419, 0.99949987,\n",
       "        0.81649331, 0.77097055, 0.99975   , 0.89550019, 0.7990608 ,\n",
       "        0.99975   , 0.9255993 , 0.7990608 , 0.99974994, 0.9255993 ,\n",
       "        0.7990608 , 0.99975006, 0.84177533, 0.79388364, 0.99975006,\n",
       "        0.91826863, 0.83983218, 0.99975   , 0.93197109, 0.83983218,\n",
       "        0.99975006, 0.93608555, 0.83983218, 0.75033345, 0.67832605,\n",
       "        0.64743674, 0.77281078, 0.7011208 , 0.67464299, 0.77622206,\n",
       "        0.71398593, 0.6887568 , 0.77940995, 0.72417623, 0.69613539,\n",
       "        0.92833436, 0.7053104 , 0.66640696, 0.95887338, 0.74126394,\n",
       "        0.70065494, 0.96743487, 0.75442043, 0.7167003 , 0.973258  ,\n",
       "        0.76249229, 0.72301297, 0.98211829, 0.72215973, 0.68087855,\n",
       "        0.99310345, 0.76347009, 0.71611581, 0.99535934, 0.77770974,\n",
       "        0.73254348, 0.99586103, 0.78713969, 0.74022873, 0.79672211,\n",
       "        0.7260612 , 0.68882639, 0.88750622, 0.77987888, 0.73489081,\n",
       "        0.94447199, 0.81017661, 0.75813367, 0.9626401 , 0.8247652 ,\n",
       "        0.76573427, 0.97218742, 0.7717311 , 0.718923  , 0.99912467,\n",
       "        0.83600098, 0.78356639, 0.99974994, 0.87173193, 0.79995057,\n",
       "        0.99975   , 0.88084054, 0.80202795, 0.99737073, 0.79324374,\n",
       "        0.73795712, 0.99975   , 0.85633387, 0.80341246, 0.99974994,\n",
       "        0.88927208, 0.82866658, 0.99974994, 0.9065513 , 0.82866658,\n",
       "        0.83942608, 0.74729377, 0.71898224, 0.93877551, 0.80814603,\n",
       "        0.76542593, 0.98270677, 0.84797048, 0.77708748, 0.99311209,\n",
       "        0.85985924, 0.77708748, 0.9979995 , 0.79067473, 0.75473108,\n",
       "        0.99974994, 0.8645975 , 0.80464026, 0.99975006, 0.89981424,\n",
       "        0.80464026, 0.99975006, 0.91783717, 0.80464026, 0.99962495,\n",
       "        0.82168437, 0.77701437, 0.99975006, 0.89084594, 0.82000739,\n",
       "        0.99975006, 0.92777086, 0.82000739, 0.99975006, 0.93088473,\n",
       "        0.82000739, 0.86779237, 0.77099237, 0.73853841, 0.97725   ,\n",
       "        0.84027778, 0.75159714, 0.99937477, 0.87475149, 0.75159714,\n",
       "        0.99975   , 0.88968944, 0.75159714, 0.99962505, 0.81343656,\n",
       "        0.77095529, 0.99975   , 0.89122114, 0.78551946, 0.99975006,\n",
       "        0.9165116 , 0.78551946, 0.99975006, 0.9165116 , 0.78551946,\n",
       "        0.99975   , 0.83155382, 0.77654477, 0.99975   , 0.91179039,\n",
       "        0.79625702, 0.99975   , 0.9197554 , 0.79625702, 0.99975   ,\n",
       "        0.9197554 , 0.79625702, 0.74009128, 0.67939872, 0.63979782,\n",
       "        0.75146056, 0.6984869 , 0.66241707, 0.76089085, 0.70730812,\n",
       "        0.67813017, 0.76700576, 0.71299241, 0.68344505, 0.93067591,\n",
       "        0.69456901, 0.64772275, 0.95863444, 0.73098696, 0.68329886,\n",
       "        0.96225944, 0.74233658, 0.69946606, 0.97069138, 0.75039785,\n",
       "        0.70579288, 0.98265024, 0.70934858, 0.65250863, 0.99172102,\n",
       "        0.75183016, 0.69534974, 0.99334756, 0.76663405, 0.71274023,\n",
       "        0.99447791, 0.77483201, 0.71868687, 0.79417572, 0.72324909,\n",
       "        0.69703121, 0.88466791, 0.78030021, 0.73841143, 0.94286786,\n",
       "        0.80834161, 0.76028935, 0.96312961, 0.82738169, 0.76783042,\n",
       "        0.97398699, 0.77177289, 0.72565707, 0.99937477, 0.8386698 ,\n",
       "        0.78033031, 0.99975006, 0.87203557, 0.79526394, 0.99974994,\n",
       "        0.89100581, 0.79526394, 0.99762113, 0.80014896, 0.72519665,\n",
       "        0.99974994, 0.86447791, 0.79995021, 0.99974994, 0.8982925 ,\n",
       "        0.82050018, 0.99975   , 0.91142999, 0.82050018, 0.84094018,\n",
       "        0.75299401, 0.71966527, 0.94409006, 0.81493386, 0.77004008,\n",
       "        0.98574287, 0.84983232, 0.77342971, 0.99549662, 0.87018933,\n",
       "        0.77342971, 0.99574362, 0.81093731, 0.75356161, 0.99975   ,\n",
       "        0.87253937, 0.79935715, 0.99975   , 0.90724206, 0.79935715,\n",
       "        0.99974994, 0.91514474, 0.79935715, 0.99962495, 0.82835543,\n",
       "        0.7852291 , 0.99975   , 0.89925558, 0.82039911, 0.99975   ,\n",
       "        0.93095327, 0.82039911, 0.99975006, 0.93597409, 0.82039911,\n",
       "        0.87200599, 0.7747996 , 0.74794521, 0.98247371, 0.84462646,\n",
       "        0.75767748, 0.99925   , 0.88626182, 0.75767748, 0.99974994,\n",
       "        0.89259075, 0.75767748, 0.99975   , 0.82606008, 0.77924669,\n",
       "        0.99975   , 0.90309789, 0.78554576, 0.99975   , 0.93383271,\n",
       "        0.78554576, 0.99975   , 0.93383271, 0.78554576, 0.99975   ,\n",
       "        0.84497487, 0.80145674, 0.99975   , 0.92151962, 0.81005726,\n",
       "        0.99975   , 0.9317219 , 0.81005726, 0.99975   , 0.9317219 ,\n",
       "        0.81005726]),\n",
       " 'split2_train_score': array([0.70784771, 0.60083333, 0.52141757, 0.75243715, 0.64415863,\n",
       "        0.44693128, 0.7759928 , 0.68085665, 0.49991498, 0.78672006,\n",
       "        0.68769634, 0.48402305, 0.89729446, 0.65815603, 0.56465075,\n",
       "        0.93908696, 0.6976287 , 0.56182213, 0.95526281, 0.71657885,\n",
       "        0.59821703, 0.96078941, 0.71856287, 0.58967136, 0.96831302,\n",
       "        0.67452173, 0.58579797, 0.98129057, 0.70703408, 0.59223737,\n",
       "        0.9861868 , 0.72341446, 0.63206152, 0.98798229, 0.72738138,\n",
       "        0.62061701, 0.72302758, 0.63859468, 0.57276463, 0.81134497,\n",
       "        0.69532987, 0.65409793, 0.87349321, 0.7453565 , 0.70645673,\n",
       "        0.90414081, 0.75968246, 0.71542325, 0.9369069 , 0.68137316,\n",
       "        0.60799563, 0.98597245, 0.73151377, 0.68820719, 0.99736941,\n",
       "        0.77686541, 0.72900809, 0.998999  , 0.79661017, 0.74415257,\n",
       "        0.98787879, 0.68726731, 0.62288931, 0.99937477, 0.7453565 ,\n",
       "        0.69924158, 0.99975006, 0.79286139, 0.73918773, 0.99975   ,\n",
       "        0.81016909, 0.74832054, 0.7352678 , 0.64543041, 0.59959596,\n",
       "        0.84086593, 0.70711757, 0.66321767, 0.90720869, 0.76279182,\n",
       "        0.71555662, 0.94253741, 0.78205907, 0.71854985, 0.95337966,\n",
       "        0.69169675, 0.64384615, 0.99824869, 0.75264373, 0.69452981,\n",
       "        0.99975   , 0.79942036, 0.73504678, 0.99975   , 0.82288497,\n",
       "        0.7361697 , 0.99498495, 0.6957905 , 0.65603877, 0.99975   ,\n",
       "        0.76365394, 0.70563776, 0.99974994, 0.81467181, 0.74559849,\n",
       "        0.99974994, 0.83809756, 0.74685214, 0.75502211, 0.66715417,\n",
       "        0.62573909, 0.87235375, 0.72458732, 0.66799502, 0.94636591,\n",
       "        0.77089745, 0.71588313, 0.9715    , 0.79747774, 0.72667638,\n",
       "        0.97999245, 0.68858848, 0.6414719 , 0.99975   , 0.76201007,\n",
       "        0.69008568, 0.99975   , 0.81271648, 0.73221309, 0.99974994,\n",
       "        0.8375093 , 0.73845779, 0.99962505, 0.69974462, 0.6515377 ,\n",
       "        0.99975   , 0.76487183, 0.70234032, 0.99974994, 0.82944968,\n",
       "        0.7464046 , 0.99975   , 0.84934227, 0.7618932 , 0.7576244 ,\n",
       "        0.69204502, 0.67958495, 0.7823712 , 0.71971796, 0.70214614,\n",
       "        0.78641378, 0.72003025, 0.70196709, 0.79687112, 0.72968198,\n",
       "        0.71449704, 0.93441802, 0.73329985, 0.69199799, 0.96793663,\n",
       "        0.76361371, 0.72471979, 0.97522947, 0.76556457, 0.72554348,\n",
       "        0.97885729, 0.77651183, 0.73787841, 0.98755812, 0.7523511 ,\n",
       "        0.70453689, 0.99511217, 0.791019  , 0.74568807, 0.99623966,\n",
       "        0.79267236, 0.74346706, 0.99724311, 0.80725286, 0.75567554,\n",
       "        0.80242634, 0.72965879, 0.70983873, 0.89150943, 0.78153542,\n",
       "        0.74357695, 0.93752348, 0.8055763 , 0.76002514, 0.9600701 ,\n",
       "        0.82112309, 0.7768492 , 0.98055451, 0.77102102, 0.73043917,\n",
       "        0.99887458, 0.83613445, 0.78107974, 0.99975006, 0.86824786,\n",
       "        0.80564846, 0.99974994, 0.88607909, 0.80565897, 0.99812242,\n",
       "        0.79955374, 0.75050454, 0.99975006, 0.85893803, 0.80154479,\n",
       "        0.99975   , 0.89117757, 0.82613015, 0.99975   , 0.90701711,\n",
       "        0.82613015, 0.83387541, 0.74819247, 0.71545535, 0.93714857,\n",
       "        0.81048337, 0.76255534, 0.97723292, 0.84684015, 0.77493447,\n",
       "        0.98974744, 0.86064053, 0.77493447, 0.99373591, 0.79845924,\n",
       "        0.74639227, 0.99975   , 0.86467748, 0.8088911 , 0.99975   ,\n",
       "        0.90349894, 0.81630125, 0.99975006, 0.92063098, 0.81630125,\n",
       "        0.99962495, 0.82421681, 0.7610708 , 0.99975   , 0.89330699,\n",
       "        0.82541246, 0.99975   , 0.92494126, 0.82541246, 0.99974994,\n",
       "        0.93853634, 0.82541246, 0.86009717, 0.76894502, 0.73710629,\n",
       "        0.98141171, 0.83540527, 0.77017084, 0.99962495, 0.87400596,\n",
       "        0.77017084, 0.99975   , 0.88999502, 0.77017084, 0.99975   ,\n",
       "        0.82099227, 0.77161642, 0.99975006, 0.89509453, 0.8064956 ,\n",
       "        0.99975006, 0.918286  , 0.8064956 , 0.99975   , 0.918286  ,\n",
       "        0.8064956 , 0.99975   , 0.83362435, 0.79447624, 0.99975006,\n",
       "        0.91348189, 0.83780111, 0.99975006, 0.9473027 , 0.83780111,\n",
       "        0.99975   , 0.9473027 , 0.83780111, 0.74193149, 0.68085646,\n",
       "        0.65992318, 0.76637125, 0.70469043, 0.69140673, 0.77258104,\n",
       "        0.71565814, 0.70333988, 0.78180915, 0.72254841, 0.70928948,\n",
       "        0.93137497, 0.72131556, 0.68184117, 0.95370831, 0.74962631,\n",
       "        0.71358268, 0.96847636, 0.75412706, 0.72338858, 0.97432686,\n",
       "        0.75960456, 0.73024456, 0.98418234, 0.73541588, 0.69368687,\n",
       "        0.99210229, 0.7713826 , 0.7253746 , 0.99498747, 0.77934272,\n",
       "        0.73743427, 0.99686756, 0.78444004, 0.74467042, 0.78852782,\n",
       "        0.72873882, 0.70199011, 0.88566108, 0.78280318, 0.74123413,\n",
       "        0.93823456, 0.81026535, 0.7590858 , 0.96415142, 0.82522702,\n",
       "        0.76758372, 0.98252621, 0.77390549, 0.72396819, 0.99949987,\n",
       "        0.83746149, 0.78256563, 0.99975006, 0.86819024, 0.80435051,\n",
       "        0.99975006, 0.88504754, 0.80435051, 0.99874937, 0.799704  ,\n",
       "        0.7398568 , 0.99974994, 0.86367569, 0.80516578, 0.99974994,\n",
       "        0.8962963 , 0.81938109, 0.99974994, 0.91006079, 0.81938109,\n",
       "        0.8314467 , 0.75407389, 0.72697327, 0.93917822, 0.81531978,\n",
       "        0.76923077, 0.98621899, 0.8497769 , 0.77287459, 0.99423992,\n",
       "        0.86903729, 0.77287459, 0.99147442, 0.80681253, 0.75871583,\n",
       "        0.99975006, 0.87669879, 0.80319083, 0.99975   , 0.91233726,\n",
       "        0.80319083, 0.99975   , 0.92704075, 0.80319083, 0.99974994,\n",
       "        0.82220295, 0.77507867, 0.99974994, 0.89303483, 0.81705503,\n",
       "        0.99974994, 0.92682321, 0.81705503, 0.99974994, 0.94322709,\n",
       "        0.81705503, 0.87304565, 0.77820721, 0.74588031, 0.98174087,\n",
       "        0.84091764, 0.76823981, 0.99912489, 0.88481091, 0.76823981,\n",
       "        0.99975   , 0.89813665, 0.76823981, 0.99949987, 0.83190883,\n",
       "        0.76835176, 0.99975006, 0.90217526, 0.80177405, 0.99975   ,\n",
       "        0.92290558, 0.80177405, 0.99974994, 0.92290558, 0.80177405,\n",
       "        0.99974994, 0.84280185, 0.79050106, 0.99974994, 0.91979141,\n",
       "        0.8388445 , 0.99974994, 0.93118012, 0.8452616 , 0.99975   ,\n",
       "        0.93118012, 0.8452616 , 0.73746751, 0.68415165, 0.65356959,\n",
       "        0.75086334, 0.70110608, 0.68499627, 0.76536797, 0.70809321,\n",
       "        0.6943996 , 0.77094695, 0.71446353, 0.7022354 , 0.93111166,\n",
       "        0.7172709 , 0.67066021, 0.95331757, 0.73747195, 0.70389255,\n",
       "        0.96321321, 0.74498567, 0.71585713, 0.97121402, 0.75208152,\n",
       "        0.72287372, 0.98350825, 0.73760641, 0.67708067, 0.99198598,\n",
       "        0.76555734, 0.71634377, 0.99448345, 0.77250774, 0.72749783,\n",
       "        0.99574148, 0.7789891 , 0.73409622, 0.80478385, 0.72293438,\n",
       "        0.71108328, 0.89466484, 0.77539306, 0.74297894, 0.94645081,\n",
       "        0.81108753, 0.76565834, 0.96351824, 0.8281522 , 0.76714036,\n",
       "        0.98472709, 0.77153275, 0.73542601, 0.99937461, 0.84475732,\n",
       "        0.78486647, 0.99974994, 0.8755418 , 0.80708271, 0.99974994,\n",
       "        0.89166461, 0.80708271, 0.9984985 , 0.8035    , 0.74727204,\n",
       "        0.99975   , 0.86856436, 0.81001363, 0.99974994, 0.90356083,\n",
       "        0.82584478, 0.99974994, 0.92035617, 0.82584478, 0.84461059,\n",
       "        0.75751856, 0.72204473, 0.94593242, 0.82073704, 0.7685    ,\n",
       "        0.98561241, 0.85561497, 0.7685    , 0.99524762, 0.87203674,\n",
       "        0.7685    , 0.99674512, 0.80243388, 0.7587346 , 0.99975006,\n",
       "        0.87473723, 0.80332961, 0.99975   , 0.91033114, 0.80332961,\n",
       "        0.99975006, 0.91806124, 0.80332961, 0.99962486, 0.83644047,\n",
       "        0.77571157, 0.99975006, 0.90161893, 0.82567451, 0.99975   ,\n",
       "        0.92897127, 0.82567451, 0.99975   , 0.92897127, 0.82567451,\n",
       "        0.87623577, 0.78093583, 0.75181386, 0.98560881, 0.84675909,\n",
       "        0.77620047, 0.99962486, 0.88635519, 0.77620047, 0.99975006,\n",
       "        0.8909294 , 0.77620047, 0.99975006, 0.83182504, 0.79088405,\n",
       "        0.99975   , 0.90328988, 0.81402287, 0.99975006, 0.91319531,\n",
       "        0.81402287, 0.99975006, 0.91319531, 0.81402287, 0.99974994,\n",
       "        0.8592574 , 0.80034744, 0.99975   , 0.92791449, 0.82122147,\n",
       "        0.99975   , 0.93851133, 0.82122147, 0.99975   , 0.93851133,\n",
       "        0.82122147]),\n",
       " 'split3_train_score': array([0.70924715, 0.618246  , 0.50632911, 0.75690323, 0.63573215,\n",
       "        0.43584305, 0.79202352, 0.67141513, 0.53592423, 0.7954866 ,\n",
       "        0.66927857, 0.51175285, 0.89229962, 0.66589387, 0.54783788,\n",
       "        0.93810903, 0.69341195, 0.5255829 , 0.95345188, 0.71117855,\n",
       "        0.59434683, 0.95671447, 0.71238047, 0.59197531, 0.96627773,\n",
       "        0.68525089, 0.56755162, 0.98011219, 0.70838548, 0.55858099,\n",
       "        0.98644368, 0.72646514, 0.6181872 , 0.98772618, 0.72508295,\n",
       "        0.61570743, 0.72748179, 0.64564911, 0.57417621, 0.8220969 ,\n",
       "        0.70054536, 0.65422269, 0.87677543, 0.74671489, 0.71139615,\n",
       "        0.90419314, 0.76358794, 0.71709234, 0.93348683, 0.6785005 ,\n",
       "        0.61255411, 0.98811931, 0.73195122, 0.68129388, 0.99824737,\n",
       "        0.7807309 , 0.72645848, 0.99949975, 0.80327671, 0.73176017,\n",
       "        0.98559151, 0.69001233, 0.62687359, 0.99949987, 0.74661792,\n",
       "        0.68983493, 0.99975   , 0.79374925, 0.73427162, 0.99974994,\n",
       "        0.82108317, 0.74130538, 0.73883422, 0.65365731, 0.61313676,\n",
       "        0.84767368, 0.71079455, 0.66957388, 0.9121074 , 0.76136501,\n",
       "        0.70721699, 0.93821857, 0.77813346, 0.7074616 , 0.95020325,\n",
       "        0.68807906, 0.646286  , 0.99799649, 0.75560429, 0.68485072,\n",
       "        0.99975006, 0.80271449, 0.72825048, 0.99975   , 0.82352941,\n",
       "        0.73239096, 0.99585999, 0.70247628, 0.65724293, 0.99975006,\n",
       "        0.7644901 , 0.69390244, 0.99974994, 0.81848024, 0.73619048,\n",
       "        0.99975   , 0.83767436, 0.73938085, 0.75203252, 0.66420027,\n",
       "        0.62197581, 0.86971831, 0.71834305, 0.66137828, 0.9429324 ,\n",
       "        0.77032148, 0.71023755, 0.97264743, 0.79157261, 0.70726916,\n",
       "        0.97767745, 0.6956199 , 0.64814815, 0.99975   , 0.76379417,\n",
       "        0.68229863, 0.99975   , 0.81301211, 0.72960961, 0.99974994,\n",
       "        0.83223643, 0.73018983, 0.99975006, 0.70303477, 0.65044031,\n",
       "        0.99974994, 0.77175   , 0.68843735, 0.99974994, 0.82994862,\n",
       "        0.73789889, 0.99974994, 0.84878656, 0.73659788, 0.74429742,\n",
       "        0.70021604, 0.67772992, 0.77894474, 0.72261013, 0.70988202,\n",
       "        0.78302122, 0.72405511, 0.71089209, 0.78906446, 0.73045241,\n",
       "        0.71806558, 0.93129771, 0.73447185, 0.70731707, 0.96499812,\n",
       "        0.76582915, 0.73983638, 0.96899711, 0.7662191 , 0.7416604 ,\n",
       "        0.97292532, 0.77700918, 0.74795726, 0.98531811, 0.75402765,\n",
       "        0.71665828, 0.99434886, 0.79116215, 0.75147114, 0.99497866,\n",
       "        0.79362708, 0.75202593, 0.99548306, 0.80533732, 0.76468388,\n",
       "        0.80894711, 0.7283237 , 0.70201189, 0.88816856, 0.78109885,\n",
       "        0.74217288, 0.93976506, 0.80761573, 0.76441764, 0.96177466,\n",
       "        0.82164577, 0.77623861, 0.9754509 , 0.77405754, 0.73428144,\n",
       "        0.99887401, 0.82990196, 0.77425817, 0.99974994, 0.86146971,\n",
       "        0.80962369, 0.99975   , 0.88002981, 0.81822672, 0.99737007,\n",
       "        0.79737591, 0.75101626, 0.99974994, 0.86148939, 0.79701863,\n",
       "        0.99975006, 0.88795553, 0.83180806, 0.99975006, 0.9049505 ,\n",
       "        0.8371749 , 0.82693755, 0.75122196, 0.72264631, 0.93954534,\n",
       "        0.81416149, 0.77026858, 0.98071142, 0.84497952, 0.78063954,\n",
       "        0.99311553, 0.8603352 , 0.78063954, 0.99311381, 0.7992528 ,\n",
       "        0.75721818, 0.99975   , 0.86692203, 0.80639564, 0.99975   ,\n",
       "        0.9013702 , 0.80610725, 0.99975   , 0.91672844, 0.80610725,\n",
       "        0.99949975, 0.81960687, 0.77209186, 0.99975   , 0.89059787,\n",
       "        0.8379819 , 0.99975   , 0.9259995 , 0.84550212, 0.99975   ,\n",
       "        0.93192547, 0.84550212, 0.86231432, 0.77455357, 0.7411853 ,\n",
       "        0.97910672, 0.83895782, 0.78522572, 0.99912511, 0.87708385,\n",
       "        0.78522572, 0.99975006, 0.89007444, 0.78522572, 0.99975   ,\n",
       "        0.81537696, 0.77496871, 0.99975006, 0.89636273, 0.81503462,\n",
       "        0.99975   , 0.92394155, 0.81503462, 0.99975006, 0.92394155,\n",
       "        0.81503462, 0.99975006, 0.83719193, 0.79965135, 0.99975   ,\n",
       "        0.91765292, 0.81235272, 0.99975006, 0.93082856, 0.81235272,\n",
       "        0.99975   , 0.93082856, 0.81235272, 0.73165829, 0.6843913 ,\n",
       "        0.665106  , 0.75979047, 0.70707326, 0.68510693, 0.76907681,\n",
       "        0.72168569, 0.69904137, 0.77467085, 0.72401615, 0.70448515,\n",
       "        0.92448112, 0.7199186 , 0.68899522, 0.951466  , 0.7547217 ,\n",
       "        0.71134942, 0.96296296, 0.76028334, 0.72518892, 0.96786342,\n",
       "        0.767998  , 0.73128865, 0.98126405, 0.74245295, 0.69989696,\n",
       "        0.9908349 , 0.77517448, 0.72741117, 0.99448068, 0.78494223,\n",
       "        0.74366692, 0.99548646, 0.79301746, 0.74921433, 0.79461529,\n",
       "        0.72834104, 0.70151728, 0.88747664, 0.77741855, 0.74544312,\n",
       "        0.94036582, 0.80929308, 0.75967823, 0.95983034, 0.82712536,\n",
       "        0.77287379, 0.97677323, 0.77199103, 0.72752917, 0.99912445,\n",
       "        0.83380629, 0.78206413, 0.99974994, 0.86992071, 0.8130667 ,\n",
       "        0.99975   , 0.88630459, 0.8130667 , 0.99787367, 0.79821185,\n",
       "        0.74755183, 0.99974994, 0.86653485, 0.80637133, 0.99975006,\n",
       "        0.89521696, 0.83063219, 0.99975   , 0.90900099, 0.83063219,\n",
       "        0.82859625, 0.74940632, 0.72459388, 0.94172086, 0.81120797,\n",
       "        0.76973436, 0.97818455, 0.84708224, 0.77846842, 0.99260929,\n",
       "        0.86293028, 0.77846842, 0.99486794, 0.80664353, 0.76198823,\n",
       "        0.99975   , 0.87109134, 0.80421836, 0.99975   , 0.90557621,\n",
       "        0.80421836, 0.99974994, 0.92087939, 0.80421836, 0.99962495,\n",
       "        0.8249066 , 0.77410468, 0.99975006, 0.89406517, 0.8219818 ,\n",
       "        0.99975   , 0.92908505, 0.8219818 , 0.99974994, 0.93108158,\n",
       "        0.8219818 , 0.87264563, 0.77100945, 0.74909994, 0.98232861,\n",
       "        0.84571429, 0.78675924, 0.99912489, 0.88024845, 0.78675924,\n",
       "        0.99962505, 0.8975028 , 0.78675924, 0.99975006, 0.8189741 ,\n",
       "        0.78279302, 0.99975   , 0.89623462, 0.82457879, 0.99975   ,\n",
       "        0.92177379, 0.82457879, 0.99975   , 0.92177379, 0.82457879,\n",
       "        0.99974994, 0.84561404, 0.80193476, 0.99974994, 0.91395175,\n",
       "        0.8364725 , 0.99975   , 0.92579681, 0.8364725 , 0.99975   ,\n",
       "        0.92579681, 0.8364725 , 0.72844881, 0.67958689, 0.65835282,\n",
       "        0.74340468, 0.70201005, 0.68019916, 0.75408609, 0.70999127,\n",
       "        0.69042821, 0.75899863, 0.71433901, 0.6952465 , 0.92032762,\n",
       "        0.71207272, 0.67359397, 0.94862545, 0.74343987, 0.70171865,\n",
       "        0.96252023, 0.74599826, 0.71010101, 0.9659915 , 0.75185736,\n",
       "        0.71585043, 0.9753596 , 0.73066122, 0.68191268, 0.98972946,\n",
       "        0.76249689, 0.71401395, 0.99285087, 0.76639242, 0.72375204,\n",
       "        0.9942298 , 0.77195609, 0.73056604, 0.80725568, 0.72615923,\n",
       "        0.69443739, 0.89309743, 0.78161206, 0.74401009, 0.94105894,\n",
       "        0.8107436 , 0.76300578, 0.96      , 0.82844356, 0.77434679,\n",
       "        0.97923962, 0.76715073, 0.72882218, 0.99887401, 0.83847864,\n",
       "        0.782663  , 0.99975006, 0.87887813, 0.80887457, 0.99975006,\n",
       "        0.89127219, 0.80887457, 0.99812242, 0.79469536, 0.7401655 ,\n",
       "        0.99975   , 0.86399604, 0.81025896, 0.99974994, 0.9013702 ,\n",
       "        0.83870168, 0.99975   , 0.91684224, 0.83870168, 0.84064492,\n",
       "        0.76654527, 0.72795875, 0.94536817, 0.82133068, 0.77125658,\n",
       "        0.98310176, 0.85272092, 0.77600399, 0.99437148, 0.86767086,\n",
       "        0.77600399, 0.99649386, 0.80622881, 0.76227552, 0.99974994,\n",
       "        0.87476636, 0.80064388, 0.99974994, 0.91032945, 0.80064388,\n",
       "        0.99975   , 0.91625372, 0.80064388, 0.99962495, 0.82703379,\n",
       "        0.77621323, 0.99974994, 0.89971418, 0.82896195, 0.99975   ,\n",
       "        0.93048659, 0.82896195, 0.99974994, 0.93048659, 0.82896195,\n",
       "        0.87614048, 0.77554585, 0.7464559 , 0.98823235, 0.84498632,\n",
       "        0.77433022, 0.99962486, 0.87562313, 0.77433022, 0.99975   ,\n",
       "        0.87562313, 0.77433022, 0.99975006, 0.83079611, 0.78548347,\n",
       "        0.99974994, 0.90489771, 0.79403727, 0.99975006, 0.92353599,\n",
       "        0.79403727, 0.99975   , 0.92353599, 0.79403727, 0.99975006,\n",
       "        0.84851535, 0.80219233, 0.99975   , 0.92944709, 0.81573396,\n",
       "        0.99975   , 0.93981019, 0.81573396, 0.99975006, 0.93981019,\n",
       "        0.81573396]),\n",
       " 'split4_train_score': array([0.71563674, 0.6281407 , 0.5364969 , 0.75706215, 0.64156545,\n",
       "        0.48033755, 0.78596043, 0.68066788, 0.55275414, 0.78752237,\n",
       "        0.68359068, 0.54504139, 0.90645035, 0.67784643, 0.57965954,\n",
       "        0.94319822, 0.69934071, 0.57579462, 0.95720047, 0.72417178,\n",
       "        0.62724174, 0.9571112 , 0.71952866, 0.62825829, 0.96528671,\n",
       "        0.68944176, 0.5988604 , 0.98076188, 0.71109484, 0.60629227,\n",
       "        0.98644368, 0.73113094, 0.65709362, 0.98657208, 0.72681098,\n",
       "        0.65863454, 0.72989165, 0.64853239, 0.5931694 , 0.81756843,\n",
       "        0.69581101, 0.66692044, 0.87663976, 0.74059027, 0.7105295 ,\n",
       "        0.90521805, 0.75653007, 0.7156791 , 0.92861753, 0.6892489 ,\n",
       "        0.62539767, 0.99041614, 0.7313112 , 0.69971982, 0.99824693,\n",
       "        0.77483444, 0.7384506 , 0.99949975, 0.79310345, 0.73830775,\n",
       "        0.986872  , 0.69823356, 0.63713576, 0.99974994, 0.74881066,\n",
       "        0.71075372, 1.        , 0.79607519, 0.74661007, 1.        ,\n",
       "        0.81232828, 0.74686242, 0.7445181 , 0.65813347, 0.6151008 ,\n",
       "        0.84781784, 0.70286134, 0.6758485 , 0.91140196, 0.7536162 ,\n",
       "        0.71555877, 0.93807311, 0.77222899, 0.71756658, 0.95848673,\n",
       "        0.69245983, 0.64900662, 0.99924944, 0.74846033, 0.70077407,\n",
       "        1.        , 0.79980646, 0.73696711, 1.        , 0.81864654,\n",
       "        0.73488711, 0.99623872, 0.70364539, 0.66148148, 1.        ,\n",
       "        0.7628492 , 0.70004792, 1.        , 0.8172147 , 0.74209966,\n",
       "        1.        , 0.83812295, 0.74151057, 0.75464074, 0.66502826,\n",
       "        0.64172448, 0.86610616, 0.72391732, 0.65565628, 0.94054459,\n",
       "        0.76626565, 0.70431655, 0.96846337, 0.78566113, 0.71024605,\n",
       "        0.97283019, 0.69078867, 0.66189494, 1.        , 0.75956485,\n",
       "        0.66982048, 1.        , 0.80722004, 0.72683744, 1.        ,\n",
       "        0.82937098, 0.72565725, 0.99987498, 0.69687691, 0.66366183,\n",
       "        1.        , 0.77260475, 0.67861902, 1.        , 0.82009605,\n",
       "        0.72692492, 1.        , 0.8473528 , 0.72687279, 0.76060458,\n",
       "        0.70711089, 0.68579134, 0.78989329, 0.725846  , 0.70845624,\n",
       "        0.79048313, 0.72507999, 0.70910655, 0.79487179, 0.73507647,\n",
       "        0.71285908, 0.93217175, 0.73967862, 0.70217205, 0.96374357,\n",
       "        0.771509  , 0.7393865 , 0.97047786, 0.76951354, 0.73548861,\n",
       "        0.97711843, 0.78045187, 0.74458661, 0.98192317, 0.75681006,\n",
       "        0.71223463, 0.99397439, 0.79112844, 0.75354523, 0.99560798,\n",
       "        0.79217125, 0.75241236, 0.99585999, 0.80577299, 0.76191646,\n",
       "        0.79847159, 0.72362685, 0.7121456 , 0.88460102, 0.77776405,\n",
       "        0.75071207, 0.93351662, 0.80407812, 0.76368128, 0.95626018,\n",
       "        0.82135827, 0.76930705, 0.9797145 , 0.76991588, 0.73833819,\n",
       "        0.99962495, 0.83261697, 0.78357381, 1.        , 0.86613399,\n",
       "        0.80226128, 1.        , 0.88106378, 0.80211536, 0.99774549,\n",
       "        0.7965016 , 0.75657254, 1.        , 0.86314496, 0.80554533,\n",
       "        1.        , 0.89138484, 0.82531085, 1.        , 0.90789799,\n",
       "        0.82915438, 0.83836368, 0.75246305, 0.72407338, 0.93959397,\n",
       "        0.8116553 , 0.77012922, 0.9827111 , 0.84121873, 0.78128103,\n",
       "        0.9943757 , 0.86207322, 0.78128103, 0.99172932, 0.80159224,\n",
       "        0.76028756, 1.        , 0.86952216, 0.81203749, 1.        ,\n",
       "        0.90169993, 0.81450419, 1.        , 0.9180571 , 0.81450419,\n",
       "        0.99987498, 0.82550667, 0.77378019, 1.        , 0.88902589,\n",
       "        0.82191108, 1.        , 0.92410548, 0.82191108, 1.        ,\n",
       "        0.93422686, 0.82191108, 0.86371018, 0.76510903, 0.74000752,\n",
       "        0.97628905, 0.83559951, 0.77564741, 0.99925019, 0.87453601,\n",
       "        0.77564741, 0.99987498, 0.89551869, 0.77564741, 0.99962486,\n",
       "        0.81119403, 0.77936032, 1.        , 0.89868061, 0.82601185,\n",
       "        1.        , 0.93128819, 0.82601185, 1.        , 0.93128819,\n",
       "        0.82601185, 1.        , 0.82805881, 0.80009945, 1.        ,\n",
       "        0.91223272, 0.82088812, 1.        , 0.93364929, 0.82088812,\n",
       "        1.        , 0.93364929, 0.82088812, 0.74958104, 0.68750775,\n",
       "        0.66903013, 0.76924934, 0.70873074, 0.69359606, 0.77441241,\n",
       "        0.71927681, 0.70453157, 0.77806216, 0.72386322, 0.70682041,\n",
       "        0.92759961, 0.71603727, 0.6876647 , 0.95023405, 0.74439571,\n",
       "        0.71246006, 0.96104218, 0.75953079, 0.72487483, 0.96796709,\n",
       "        0.76727406, 0.72927906, 0.98237776, 0.73001471, 0.69611041,\n",
       "        0.99274637, 0.76737675, 0.72646842, 0.99524048, 0.77962375,\n",
       "        0.73693252, 0.9962406 , 0.78969852, 0.74167793, 0.7982958 ,\n",
       "        0.733716  , 0.70171058, 0.89568123, 0.78049988, 0.74623522,\n",
       "        0.9426087 , 0.80881625, 0.76202342, 0.96042941, 0.82583992,\n",
       "        0.76965106, 0.97353308, 0.77748918, 0.7331415 , 0.99912423,\n",
       "        0.83803681, 0.78692165, 1.        , 0.86918069, 0.80688384,\n",
       "        1.        , 0.88718646, 0.80688384, 0.99737073, 0.80788177,\n",
       "        0.74802656, 1.        , 0.86608353, 0.8097585 , 1.        ,\n",
       "        0.89846838, 0.8300123 , 1.        , 0.91540935, 0.83394834,\n",
       "        0.83807413, 0.75049652, 0.71994917, 0.94114726, 0.8190523 ,\n",
       "        0.76641335, 0.98201349, 0.85084913, 0.76641335, 0.99186788,\n",
       "        0.86944685, 0.76641335, 0.99449312, 0.80593294, 0.75647148,\n",
       "        1.        , 0.87164326, 0.80343369, 1.        , 0.90410282,\n",
       "        0.80343369, 1.        , 0.91485686, 0.80343369, 0.99974994,\n",
       "        0.83102425, 0.78543918, 1.        , 0.88847949, 0.81233112,\n",
       "        1.        , 0.9271441 , 0.81233112, 1.        , 0.93632866,\n",
       "        0.81233112, 0.86581709, 0.777188  , 0.74512241, 0.98321223,\n",
       "        0.84383494, 0.77867129, 0.99975   , 0.88117457, 0.77867129,\n",
       "        1.        , 0.89934219, 0.77867129, 1.        , 0.82379518,\n",
       "        0.78003743, 1.        , 0.90256028, 0.80394575, 1.        ,\n",
       "        0.92403642, 0.80394575, 1.        , 0.92403642, 0.80394575,\n",
       "        1.        , 0.84074212, 0.79741326, 1.        , 0.92070158,\n",
       "        0.82309125, 1.        , 0.92601787, 0.82309125, 1.        ,\n",
       "        0.92601787, 0.82309125, 0.74049271, 0.68251232, 0.65888989,\n",
       "        0.75527626, 0.69863348, 0.68762376, 0.76362333, 0.70618306,\n",
       "        0.6927593 , 0.77163462, 0.71266037, 0.69970703, 0.92656193,\n",
       "        0.70251603, 0.66930037, 0.94685039, 0.72481752, 0.70399605,\n",
       "        0.9597133 , 0.7411464 , 0.71208037, 0.96555996, 0.74945322,\n",
       "        0.71767294, 0.97819623, 0.7103287 , 0.67178503, 0.98978066,\n",
       "        0.74386391, 0.71310769, 0.99411838, 0.76396309, 0.72353445,\n",
       "        0.99549662, 0.77093207, 0.73202934, 0.78484438, 0.73326772,\n",
       "        0.70882905, 0.89174176, 0.78416379, 0.74614517, 0.93589424,\n",
       "        0.81214374, 0.76465442, 0.95846327, 0.83150769, 0.76949826,\n",
       "        0.98051948, 0.76688024, 0.72823887, 0.99937461, 0.83795048,\n",
       "        0.78050605, 1.        , 0.87294728, 0.80286031, 1.        ,\n",
       "        0.89296711, 0.80286031, 0.99899925, 0.78984972, 0.74385349,\n",
       "        1.        , 0.86363081, 0.80853689, 1.        , 0.90161708,\n",
       "        0.81506091, 1.        , 0.9151072 , 0.81506091, 0.83531746,\n",
       "        0.75916035, 0.73164019, 0.93808397, 0.81329035, 0.76753681,\n",
       "        0.98423029, 0.85738938, 0.76753681, 0.99524525, 0.87236923,\n",
       "        0.76753681, 0.99712176, 0.80577849, 0.76228387, 1.        ,\n",
       "        0.87802469, 0.79431748, 1.        , 0.91120168, 0.79431748,\n",
       "        1.        , 0.9240475 , 0.79431748, 0.99974994, 0.83074244,\n",
       "        0.78043912, 1.        , 0.89980159, 0.81948566, 1.        ,\n",
       "        0.93411238, 0.81948566, 1.        , 0.93411238, 0.81948566,\n",
       "        0.87384462, 0.77412935, 0.75295876, 0.98537317, 0.84746601,\n",
       "        0.76368591, 0.99974994, 0.88800793, 0.76368591, 1.        ,\n",
       "        0.88808844, 0.76368591, 1.        , 0.82549919, 0.78791585,\n",
       "        1.        , 0.90408922, 0.79975278, 1.        , 0.92063098,\n",
       "        0.79975278, 1.        , 0.92063098, 0.79975278, 1.        ,\n",
       "        0.85873606, 0.794859  , 1.        , 0.92641016, 0.82062112,\n",
       "        1.        , 0.93027888, 0.82062112, 1.        , 0.93027888,\n",
       "        0.82062112]),\n",
       " 'mean_train_score': array([0.71028911, 0.62021428, 0.51280949, 0.75528065, 0.64402468,\n",
       "        0.45981865, 0.78538226, 0.67887639, 0.5289274 , 0.79137517,\n",
       "        0.68259999, 0.51232263, 0.89524596, 0.6694966 , 0.55705008,\n",
       "        0.93939696, 0.69700477, 0.55100698, 0.95372707, 0.71810409,\n",
       "        0.60589565, 0.95687099, 0.71872893, 0.6025153 , 0.96665804,\n",
       "        0.68488145, 0.57837341, 0.98078919, 0.70919059, 0.5808251 ,\n",
       "        0.98585069, 0.728552  , 0.63456362, 0.98680094, 0.72908984,\n",
       "        0.63077318, 0.72952716, 0.64876925, 0.57726643, 0.81425935,\n",
       "        0.69709098, 0.65038764, 0.87472795, 0.74385177, 0.70735931,\n",
       "        0.90292564, 0.76231689, 0.71544248, 0.93338859, 0.68471031,\n",
       "        0.61313951, 0.98848545, 0.73321587, 0.68513186, 0.99774544,\n",
       "        0.7803916 , 0.72875643, 0.99942467, 0.79917968, 0.7354294 ,\n",
       "        0.98717083, 0.69542781, 0.62701106, 0.99949983, 0.74791995,\n",
       "        0.69806807, 0.99979999, 0.79523084, 0.73792348, 0.99979999,\n",
       "        0.81604518, 0.74294507, 0.74123299, 0.65592359, 0.60612885,\n",
       "        0.84499331, 0.71027734, 0.6668857 , 0.91009726, 0.7609899 ,\n",
       "        0.71112137, 0.93955196, 0.78003997, 0.71308194, 0.95479638,\n",
       "        0.69270654, 0.64157948, 0.99824792, 0.75324171, 0.68929833,\n",
       "        0.99980002, 0.80241754, 0.73067052, 0.9998    , 0.82326877,\n",
       "        0.73318264, 0.99591197, 0.70222646, 0.65439827, 0.99980002,\n",
       "        0.76475477, 0.69646696, 0.99979997, 0.81627605, 0.7387402 ,\n",
       "        0.99979996, 0.839002  , 0.74276054, 0.75205332, 0.666448  ,\n",
       "        0.62599655, 0.86951468, 0.72362895, 0.66164599, 0.94268014,\n",
       "        0.76987875, 0.71016367, 0.97043893, 0.79183372, 0.71272337,\n",
       "        0.97563378, 0.69349838, 0.64593679, 0.99980001, 0.76328135,\n",
       "        0.68173285, 0.99979999, 0.81305799, 0.73041925, 0.99979996,\n",
       "        0.83453634, 0.73377783, 0.99967501, 0.70077424, 0.65155039,\n",
       "        0.99979997, 0.77356844, 0.69095822, 0.9998    , 0.82773873,\n",
       "        0.73750398, 0.99979997, 0.84875209, 0.74260744, 0.75660862,\n",
       "        0.69976935, 0.68040049, 0.78558663, 0.72587416, 0.70575188,\n",
       "        0.78816543, 0.72539731, 0.70558193, 0.79628691, 0.73436838,\n",
       "        0.71421888, 0.93339217, 0.73504532, 0.69993117, 0.96656261,\n",
       "        0.76801631, 0.73353901, 0.97202929, 0.76787338, 0.73318707,\n",
       "        0.97642474, 0.78037289, 0.74295319, 0.98554422, 0.75206233,\n",
       "        0.71177806, 0.99447595, 0.79300031, 0.75030625, 0.99580956,\n",
       "        0.79479859, 0.74879026, 0.9963879 , 0.80884693, 0.76108759,\n",
       "        0.80565353, 0.72495914, 0.70634516, 0.88790756, 0.77693755,\n",
       "        0.74473731, 0.93709293, 0.80268264, 0.76098079, 0.95916884,\n",
       "        0.81946339, 0.77230723, 0.97823259, 0.7736778 , 0.73331753,\n",
       "        0.9991746 , 0.83416565, 0.780559  , 0.99979999, 0.864166  ,\n",
       "        0.80570083, 0.99979999, 0.88195421, 0.81005852, 0.99782097,\n",
       "        0.79854912, 0.75191802, 0.9998    , 0.86013108, 0.80142337,\n",
       "        0.99980001, 0.88806178, 0.82609672, 0.99980001, 0.90497867,\n",
       "        0.8279388 , 0.83345078, 0.75137381, 0.72309557, 0.9397931 ,\n",
       "        0.80909517, 0.76858418, 0.98123035, 0.84229871, 0.77829843,\n",
       "        0.99281763, 0.85984653, 0.77829843, 0.99295954, 0.80005731,\n",
       "        0.75708638, 0.9998    , 0.86621841, 0.80771203, 0.99980002,\n",
       "        0.90046417, 0.81124366, 0.99979999, 0.91698771, 0.81124366,\n",
       "        0.9996499 , 0.82106396, 0.77006422, 0.99979999, 0.88966265,\n",
       "        0.82449603, 0.9998    , 0.92327831, 0.82778921, 0.9998    ,\n",
       "        0.9345297 , 0.82778921, 0.86397523, 0.76967072, 0.73813958,\n",
       "        0.9791838 , 0.83705235, 0.77604763, 0.99924981, 0.87525481,\n",
       "        0.77604763, 0.99977501, 0.89113207, 0.77604763, 0.99967495,\n",
       "        0.81733064, 0.77504699, 0.99980002, 0.89746811, 0.81308227,\n",
       "        0.99980002, 0.92450725, 0.81308227, 0.9998    , 0.92450725,\n",
       "        0.81308227, 0.99980004, 0.83542191, 0.7961296 , 0.99980002,\n",
       "        0.91544476, 0.825726  , 0.99980004, 0.9367598 , 0.825726  ,\n",
       "        0.9998    , 0.93758269, 0.825726  , 0.74352799, 0.68348623,\n",
       "        0.66302247, 0.7666253 , 0.70543855, 0.68776554, 0.77298514,\n",
       "        0.71721035, 0.69965617, 0.77857423, 0.72348064, 0.70460534,\n",
       "        0.92852679, 0.7169512 , 0.68213872, 0.95482642, 0.74830964,\n",
       "        0.71073669, 0.96610831, 0.75714436, 0.72260126, 0.97161025,\n",
       "        0.76536901, 0.72858217, 0.9826156 , 0.73351548, 0.69339671,\n",
       "        0.9925547 , 0.77001732, 0.72464331, 0.99521119, 0.78032467,\n",
       "        0.73743286, 0.99631462, 0.78948911, 0.74376313, 0.79529199,\n",
       "        0.72814192, 0.69969445, 0.88972885, 0.78032665, 0.74314569,\n",
       "        0.94191427, 0.80891394, 0.76083809, 0.96264118, 0.8249011 ,\n",
       "        0.76949967, 0.9766989 , 0.774462  , 0.72666245, 0.99919954,\n",
       "        0.83653757, 0.78322866, 0.99979999, 0.87006751, 0.80568828,\n",
       "        0.99980001, 0.88530599, 0.80610375, 0.9979978 , 0.79980827,\n",
       "        0.74392394, 0.99979997, 0.86398344, 0.80513854, 0.99979999,\n",
       "        0.89568039, 0.82762093, 0.99979999, 0.91029774, 0.82899934,\n",
       "        0.8344738 , 0.7514724 , 0.72211199, 0.93963154, 0.81300193,\n",
       "        0.76754322, 0.9820181 , 0.84830766, 0.77362283, 0.99319069,\n",
       "        0.86464381, 0.77362283, 0.99506595, 0.80202258, 0.75825271,\n",
       "        0.9998    , 0.87002597, 0.80486825, 0.9998    , 0.90458264,\n",
       "        0.80486825, 0.99979999, 0.91909841, 0.80486825, 0.99967497,\n",
       "        0.82544145, 0.77560494, 0.99980001, 0.89262106, 0.81855199,\n",
       "        0.9998    , 0.92804874, 0.81855199, 0.99979997, 0.93577495,\n",
       "        0.81855199, 0.86947143, 0.77361794, 0.74338155, 0.98047857,\n",
       "        0.84290127, 0.77180825, 0.99937488, 0.8800956 , 0.77180825,\n",
       "        0.99977501, 0.89472857, 0.77180825, 0.99969997, 0.8222503 ,\n",
       "        0.77425451, 0.99980002, 0.89790443, 0.80133436, 0.99980001,\n",
       "        0.92025272, 0.80133436, 0.99979999, 0.92025272, 0.80133436,\n",
       "        0.99979999, 0.84334217, 0.79177121, 0.99979997, 0.91923556,\n",
       "        0.8207258 , 0.99979999, 0.92778833, 0.82200922, 0.9998    ,\n",
       "        0.92778833, 0.82200922, 0.73675385, 0.68113971, 0.65622437,\n",
       "        0.74981966, 0.69956057, 0.68037815, 0.7598443 , 0.70749447,\n",
       "        0.6893923 , 0.76626245, 0.71307093, 0.69598389, 0.9279559 ,\n",
       "        0.70726424, 0.66882281, 0.95115273, 0.73473488, 0.69894062,\n",
       "        0.96148537, 0.74375896, 0.70969618, 0.96814137, 0.75094676,\n",
       "        0.71615231, 0.9800719 , 0.72301191, 0.67399222, 0.99084027,\n",
       "        0.7561632 , 0.71036327, 0.99373244, 0.7669714 , 0.72214452,\n",
       "        0.99513725, 0.77434854, 0.73018373, 0.79912751, 0.72687808,\n",
       "        0.70254112, 0.89158423, 0.77972324, 0.74231566, 0.94176814,\n",
       "        0.81055738, 0.76349693, 0.96211191, 0.82824554, 0.77020045,\n",
       "        0.98020162, 0.77092971, 0.72951762, 0.99927455, 0.83968778,\n",
       "        0.78342592, 0.99980001, 0.87472022, 0.80312127, 0.99979999,\n",
       "        0.89218219, 0.80312127, 0.99847324, 0.79758958, 0.73896488,\n",
       "        0.99979997, 0.86541777, 0.80671723, 0.99979996, 0.90099771,\n",
       "        0.82459944, 0.99979997, 0.91604761, 0.82459944, 0.83965517,\n",
       "        0.7584177 , 0.72337777, 0.94413452, 0.81708193, 0.76878671,\n",
       "        0.98473822, 0.85297521, 0.77041412, 0.99492162, 0.86978986,\n",
       "        0.77041412, 0.99594699, 0.80674319, 0.75997286, 0.99979999,\n",
       "        0.87443413, 0.79911383, 0.99979999, 0.90925651, 0.79911383,\n",
       "        0.9998    , 0.91882055, 0.79911383, 0.99967495, 0.82954419,\n",
       "        0.77818661, 0.99980001, 0.89890855, 0.82132668, 0.9998    ,\n",
       "        0.93172957, 0.82132668, 0.99979999, 0.93348481, 0.82132668,\n",
       "        0.87343469, 0.77734529, 0.74975324, 0.98443175, 0.84667423,\n",
       "        0.76850272, 0.99954993, 0.8841861 , 0.76850272, 0.9998    ,\n",
       "        0.88805863, 0.76850272, 0.99980002, 0.82821635, 0.78609365,\n",
       "        0.99979997, 0.90341223, 0.79828545, 0.99980002, 0.91979047,\n",
       "        0.79828545, 0.99980001, 0.91979047, 0.79828545, 0.99980001,\n",
       "        0.85246555, 0.80003201, 0.99979999, 0.92700395, 0.81604535,\n",
       "        0.99980001, 0.93547076, 0.81604535, 0.99980001, 0.93547076,\n",
       "        0.81604535]),\n",
       " 'std_train_score': array([3.36266415e-03, 1.40890540e-02, 1.48152359e-02, 1.71880515e-03,\n",
       "        9.10672751e-03, 1.75810366e-02, 5.97478934e-03, 8.61089789e-03,\n",
       "        1.71424038e-02, 3.88245261e-03, 9.45941989e-03, 2.00095471e-02,\n",
       "        6.80099167e-03, 1.06078603e-02, 1.36920785e-02, 1.94434732e-03,\n",
       "        4.19238148e-03, 1.72127891e-02, 2.35963276e-03, 5.34200245e-03,\n",
       "        1.18473984e-02, 2.27599358e-03, 6.18787531e-03, 1.41749838e-02,\n",
       "        1.08466332e-03, 8.90743776e-03, 1.24333401e-02, 7.60047364e-04,\n",
       "        3.08911032e-03, 1.71804317e-02, 1.40998551e-03, 5.62042274e-03,\n",
       "        1.25597389e-02, 1.48436881e-03, 7.11340071e-03, 1.52299587e-02,\n",
       "        4.73823163e-03, 6.95528900e-03, 8.37162103e-03, 4.84550796e-03,\n",
       "        2.02295806e-03, 1.10433030e-02, 2.43405364e-03, 2.22359652e-03,\n",
       "        3.42725405e-03, 2.02148575e-03, 3.80058547e-03, 3.01624051e-03,\n",
       "        2.77009849e-03, 4.11113841e-03, 6.46922617e-03, 1.57976499e-03,\n",
       "        2.00691515e-03, 8.20465171e-03, 4.55899232e-04, 4.09229742e-03,\n",
       "        5.08255230e-03, 2.18283711e-04, 4.46053432e-03, 5.09430490e-03,\n",
       "        9.23456279e-04, 5.78468869e-03, 5.23156004e-03, 1.76891665e-04,\n",
       "        1.99339395e-03, 7.05002401e-03, 1.00006265e-04, 1.81739235e-03,\n",
       "        4.71710020e-03, 1.00006254e-04, 4.80831223e-03, 4.12643453e-03,\n",
       "        3.85113569e-03, 6.08455975e-03, 6.58971948e-03, 3.19778147e-03,\n",
       "        4.81524997e-03, 5.91906945e-03, 2.84926057e-03, 4.90813239e-03,\n",
       "        3.72315685e-03, 3.33238661e-03, 4.43535621e-03, 4.35704341e-03,\n",
       "        3.30717630e-03, 5.53498238e-03, 6.10823759e-03, 6.08334489e-04,\n",
       "        2.67346408e-03, 7.69330218e-03, 9.99875070e-05, 3.15492017e-03,\n",
       "        4.47960464e-03, 1.00000000e-04, 3.31067899e-03, 3.70828747e-03,\n",
       "        1.14204884e-03, 6.62673570e-03, 5.17155024e-03, 9.99875070e-05,\n",
       "        1.43339321e-03, 7.20660298e-03, 1.00012507e-04, 1.98369304e-03,\n",
       "        4.35557492e-03, 1.00018758e-04, 2.28604084e-03, 2.45691429e-03,\n",
       "        2.49405788e-03, 1.54625506e-03, 8.11363875e-03, 2.55836236e-03,\n",
       "        2.82040702e-03, 4.08610574e-03, 2.36100095e-03, 1.82275792e-03,\n",
       "        3.67118869e-03, 1.99120576e-03, 3.94620686e-03, 7.57004130e-03,\n",
       "        2.74638865e-03, 3.55451106e-03, 9.91642653e-03, 9.99937545e-05,\n",
       "        2.27237865e-03, 6.63699373e-03, 1.00006254e-04, 3.56543389e-03,\n",
       "        3.70274595e-03, 1.00018758e-04, 3.25665857e-03, 5.52669477e-03,\n",
       "        1.27477944e-04, 2.24624654e-03, 9.27223789e-03, 1.00012507e-04,\n",
       "        5.44104188e-03, 7.68956297e-03, 1.00000022e-04, 3.90035180e-03,\n",
       "        6.29760631e-03, 1.00012507e-04, 1.12655933e-03, 1.15609388e-02,\n",
       "        6.37543578e-03, 4.78890625e-03, 6.64171680e-03, 4.20758909e-03,\n",
       "        4.46297095e-03, 3.58318612e-03, 3.33174861e-03, 3.43205949e-03,\n",
       "        3.98961083e-03, 4.29814662e-03, 3.72125271e-03, 2.13013047e-03,\n",
       "        1.51652880e-03, 2.35202565e-03, 6.02441660e-03, 2.14726498e-03,\n",
       "        2.88570015e-03, 6.04465862e-03, 2.20946515e-03, 2.24443855e-03,\n",
       "        5.30861996e-03, 2.22994411e-03, 3.72104663e-03, 3.39617002e-03,\n",
       "        2.40919607e-03, 4.27926050e-03, 5.72096745e-03, 6.69089648e-04,\n",
       "        2.42317553e-03, 3.01814582e-03, 6.15471138e-04, 2.48161693e-03,\n",
       "        3.28532851e-03, 7.68810565e-04, 3.92199289e-03, 2.94612906e-03,\n",
       "        4.43098814e-03, 5.74746476e-03, 7.04244919e-03, 2.44624604e-03,\n",
       "        4.15038797e-03, 3.61382832e-03, 2.41899482e-03, 3.97357947e-03,\n",
       "        2.65352225e-03, 2.08318442e-03, 2.64803901e-03, 4.42470173e-03,\n",
       "        2.41374967e-03, 3.88441505e-03, 5.19760078e-03, 2.80880143e-04,\n",
       "        2.60083369e-03, 3.86002209e-03, 1.00006265e-04, 2.58563886e-03,\n",
       "        3.11977153e-03, 1.00006265e-04, 2.11629850e-03, 5.61977562e-03,\n",
       "        4.38843465e-04, 1.61775041e-03, 5.74514277e-03, 1.00000011e-04,\n",
       "        2.57209876e-03, 3.95762213e-03, 9.99937654e-05, 3.95500717e-03,\n",
       "        3.30525098e-03, 9.99937654e-05, 2.68847638e-03, 5.22485573e-03,\n",
       "        3.67290510e-03, 2.18914484e-03, 4.23301204e-03, 1.90651485e-03,\n",
       "        3.87252625e-03, 3.66669195e-03, 2.16921792e-03, 3.09774598e-03,\n",
       "        3.16670673e-03, 1.59690362e-03, 2.34879775e-03, 3.16670673e-03,\n",
       "        1.03230747e-03, 1.11738920e-03, 5.58122212e-03, 1.00000011e-04,\n",
       "        1.93302415e-03, 2.67988611e-03, 9.99875070e-05, 2.64391407e-03,\n",
       "        5.13435733e-03, 1.00006265e-04, 2.36125744e-03, 5.13435733e-03,\n",
       "        1.22539597e-04, 3.19141327e-03, 4.84814159e-03, 1.00006254e-04,\n",
       "        2.11393419e-03, 7.21241461e-03, 1.00000011e-04, 2.30453449e-03,\n",
       "        9.50187558e-03, 1.00000011e-04, 2.17003888e-03, 9.50187558e-03,\n",
       "        2.84952629e-03, 3.54396547e-03, 3.62498184e-03, 1.73067483e-03,\n",
       "        1.36500732e-03, 5.47411403e-03, 2.50247357e-04, 1.19241250e-03,\n",
       "        5.47411403e-03, 4.99875254e-05, 2.27961090e-03, 5.47411403e-03,\n",
       "        1.00057833e-04, 4.08201335e-03, 3.39714462e-03, 9.99875070e-05,\n",
       "        2.45473745e-03, 9.42083631e-03, 9.99875070e-05, 4.18036499e-03,\n",
       "        9.42083631e-03, 1.00000011e-04, 4.18036499e-03, 9.42083631e-03,\n",
       "        9.99812576e-05, 4.51710464e-03, 3.12553767e-03, 9.99875070e-05,\n",
       "        2.32554216e-03, 1.10504799e-02, 9.99812576e-05, 6.16031330e-03,\n",
       "        1.10504799e-02, 1.00000011e-04, 5.72511032e-03, 1.10504799e-02,\n",
       "        6.73441298e-03, 3.42735735e-03, 9.00202901e-03, 4.35627669e-03,\n",
       "        2.55862410e-03, 7.29837707e-03, 2.36967624e-03, 2.83623344e-03,\n",
       "        5.74884174e-03, 2.31433042e-03, 6.70877882e-04, 4.50469669e-03,\n",
       "        2.47972374e-03, 6.18831452e-03, 8.22707799e-03, 3.87944479e-03,\n",
       "        4.86307763e-03, 5.23628930e-03, 3.55888070e-03, 2.53450793e-03,\n",
       "        3.07798605e-03, 3.05158064e-03, 3.71251282e-03, 2.89287545e-03,\n",
       "        9.85451302e-04, 6.94042804e-03, 6.56491333e-03, 1.05358447e-03,\n",
       "        4.13287332e-03, 4.34755039e-03, 4.91909748e-04, 2.43792936e-03,\n",
       "        3.57029593e-03, 6.07836168e-04, 3.37363204e-03, 3.09595796e-03,\n",
       "        3.64202174e-03, 3.29225138e-03, 5.53494192e-03, 3.70673564e-03,\n",
       "        1.75039136e-03, 4.67897549e-03, 2.31804682e-03, 1.54648482e-03,\n",
       "        2.56009494e-03, 2.34169991e-03, 1.85454861e-03, 2.60401040e-03,\n",
       "        3.67401294e-03, 2.46967397e-03, 4.88718145e-03, 1.50168949e-04,\n",
       "        1.52069962e-03, 2.01999313e-03, 1.00006265e-04, 1.31576323e-03,\n",
       "        4.30875484e-03, 9.99937545e-05, 2.36405714e-03, 3.80632453e-03,\n",
       "        5.93202425e-04, 4.70894197e-03, 4.18194510e-03, 1.00012507e-04,\n",
       "        4.01146329e-03, 2.93594921e-03, 1.00006265e-04, 3.50598150e-03,\n",
       "        4.17095738e-03, 1.00006265e-04, 2.89587611e-03, 5.12116529e-03,\n",
       "        4.03287204e-03, 3.18702952e-03, 3.11351166e-03, 1.60444406e-03,\n",
       "        3.78761570e-03, 1.66176209e-03, 2.60473876e-03, 1.80145731e-03,\n",
       "        4.19867168e-03, 9.01827078e-04, 3.88540174e-03, 4.19867168e-03,\n",
       "        2.18694174e-03, 6.20090030e-03, 2.48847883e-03, 1.00000011e-04,\n",
       "        4.31663530e-03, 2.06227682e-03, 1.00000011e-04, 4.38950793e-03,\n",
       "        2.06227682e-03, 1.00006265e-04, 4.55295875e-03, 2.06227682e-03,\n",
       "        6.12142844e-05, 3.45978419e-03, 6.10216369e-03, 9.99937654e-05,\n",
       "        2.79407716e-03, 3.54529124e-03, 1.00000011e-04, 1.03471717e-03,\n",
       "        3.54529124e-03, 1.00012518e-04, 4.56666698e-03, 3.54529124e-03,\n",
       "        2.86448199e-03, 3.34848489e-03, 4.28000038e-03, 2.43981253e-03,\n",
       "        2.01706009e-03, 1.17959238e-02, 2.37203777e-04, 3.23557733e-03,\n",
       "        1.17959238e-02, 1.22463007e-04, 4.45264550e-03, 1.17959238e-02,\n",
       "        1.69599746e-04, 6.08071091e-03, 5.97134259e-03, 9.99875070e-05,\n",
       "        4.18809237e-03, 1.34704912e-02, 9.99937545e-05, 3.32994408e-03,\n",
       "        1.34704912e-02, 1.00006265e-04, 3.32994408e-03, 1.34704912e-02,\n",
       "        1.00006265e-04, 7.89143790e-03, 8.59145893e-03, 1.00012507e-04,\n",
       "        6.33165461e-03, 1.62413592e-02, 1.00006254e-04, 5.54517706e-03,\n",
       "        1.78010738e-02, 1.00000000e-04, 5.54517706e-03, 1.78010738e-02,\n",
       "        4.35581800e-03, 1.87551798e-03, 9.92615781e-03, 3.94177977e-03,\n",
       "        1.69574565e-03, 9.33649536e-03, 4.47486158e-03, 1.47647154e-03,\n",
       "        5.79220169e-03, 4.82959231e-03, 1.29907648e-03, 6.65747279e-03,\n",
       "        4.18069998e-03, 7.92309316e-03, 1.15582959e-02, 4.32422713e-03,\n",
       "        6.33422798e-03, 7.88187666e-03, 1.47715906e-03, 1.77153079e-03,\n",
       "        5.47904678e-03, 2.36744142e-03, 9.64245161e-04, 5.67012580e-03,\n",
       "        2.98358274e-03, 1.12773598e-02, 1.18280864e-02, 9.45068141e-04,\n",
       "        7.73603698e-03, 7.60250759e-03, 5.75332871e-04, 2.92395418e-03,\n",
       "        4.95428286e-03, 6.50575119e-04, 2.81544791e-03, 5.99549139e-03,\n",
       "        8.44151640e-03, 3.83885005e-03, 6.48098685e-03, 3.58685040e-03,\n",
       "        3.13068708e-03, 2.77240330e-03, 3.42880641e-03, 1.24494818e-03,\n",
       "        1.82677592e-03, 2.52660090e-03, 1.88152278e-03, 2.70593573e-03,\n",
       "        3.62138876e-03, 3.80753710e-03, 3.22295410e-03, 2.00269041e-04,\n",
       "        2.54707656e-03, 3.13873519e-03, 9.99937654e-05, 2.39107769e-03,\n",
       "        4.75705430e-03, 1.00006265e-04, 1.13180310e-03, 4.75705430e-03,\n",
       "        5.56979773e-04, 4.78433961e-03, 7.54065861e-03, 1.00012507e-04,\n",
       "        1.84406270e-03, 3.90099706e-03, 1.00018758e-04, 1.73966903e-03,\n",
       "        7.88841075e-03, 1.00012507e-04, 2.88474782e-03, 7.88841075e-03,\n",
       "        3.29755178e-03, 4.54583494e-03, 5.75532156e-03, 3.18592065e-03,\n",
       "        3.29423823e-03, 1.67910341e-03, 9.77754582e-04, 3.15491448e-03,\n",
       "        3.65595376e-03, 5.09752232e-04, 2.28048127e-03, 3.65595376e-03,\n",
       "        1.24284482e-03, 2.82480770e-03, 3.53544136e-03, 1.00006265e-04,\n",
       "        2.10326593e-03, 2.98666156e-03, 1.00006254e-04, 1.70100078e-03,\n",
       "        2.98666156e-03, 1.00000011e-04, 3.19838603e-03, 2.98666156e-03,\n",
       "        6.12755420e-05, 3.89946309e-03, 4.19951410e-03, 9.99937654e-05,\n",
       "        2.51130825e-03, 5.76814727e-03, 1.00000000e-04, 2.05759949e-03,\n",
       "        5.76814727e-03, 1.00006265e-04, 3.32466690e-03, 5.76814727e-03,\n",
       "        2.73933184e-03, 3.12147042e-03, 2.39536256e-03, 2.69221401e-03,\n",
       "        1.78049280e-03, 6.90048309e-03, 1.69518619e-04, 4.40887130e-03,\n",
       "        6.90048309e-03, 1.00000011e-04, 6.45680930e-03, 6.90048309e-03,\n",
       "        9.99875070e-05, 2.58609773e-03, 3.85317128e-03, 1.00012507e-04,\n",
       "        1.07303876e-03, 9.27591921e-03, 9.99875070e-05, 8.94664582e-03,\n",
       "        9.27591921e-03, 9.99937545e-05, 8.94664582e-03, 9.27591921e-03,\n",
       "        9.99937654e-05, 5.65317825e-03, 2.65238672e-03, 1.00006254e-04,\n",
       "        2.98834370e-03, 4.37271254e-03, 9.99937545e-05, 3.78209076e-03,\n",
       "        4.37271254e-03, 9.99937545e-05, 3.78209076e-03, 4.37271254e-03])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'learning_rate' : [0.01,0.2,0.4,0.9], \n",
    "    'colsample_bytree': [0.01,0.3,0.4,0.6],\n",
    "    'max_depth': [5,9,12],\n",
    "    'reg_alpha': [0,50,100],\n",
    "    'n_estimators': [10,25,40,50]\n",
    "}\n",
    "\n",
    "\n",
    "clf = grid_search(parameters)\n",
    "clf.cv_results_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.2, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=50, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.759400158071584"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "saveModel(clf, \"XGB_LSA_1500\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
