{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librería LSA: https://radimrehurek.com/gensim/models/lsimodel.html\n",
    "\n",
    "Librería Corpora Dic: https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "\n",
    "Ejemplo:\n",
    "- https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "- https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n",
    "\n",
    "Wikipedia:\n",
    "https://en.wikipedia.org/wiki/Latent_semantic_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTable(name):\n",
    "    path = '../../Tables/'\n",
    "    path += name + '.h5'\n",
    "    return pd.read_hdf(path, 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table(\"../../Dataset/10k_1Col_NoCarEsp_LSA.csv\", sep=',')\n",
    "#data['Diagnoses'] = data['Interventions'] + data['Diagnoses']\n",
    "#data = data.drop(columns=[\"Interventions\",\"Eligible\"])\n",
    "#data['Diagnoses']\n",
    "#wordss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Input  : path and file_name\n",
    "    Purpose: loading text file\n",
    "    Output : list of paragraphs/documents and\n",
    "             title(initial 100 words considred as title of document)\n",
    "    \"\"\"\n",
    "    titles = []\n",
    "    titles.append( data['Diagnoses'][0:min(len(data['Diagnoses']),2)] )\n",
    "    return data['Diagnoses'],titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    \n",
    "    dictionary.filter_extremes(no_below=10)\n",
    "    dictionary.filter_tokens(bad_ids=[3,2])\n",
    "        \n",
    "    return dictionary,doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words=10):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, column):\n",
    "    return df[column].fillna('').apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for number_of_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(doc_clean,start, stop, step):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDocTopicMatrix(model, df, column, num_topics):    \n",
    "    tokens = tokenize(data, 'Diagnoses')\n",
    "    sentence_embedding = np.zeros((len(tok),num_topics))\n",
    "    dic,a = prepare_corpus(tok)\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        sentence = dic.doc2bow(token)\n",
    "        sentence_embedding[i,:]=np.asarray(model[sentence])[:,1].transpose()\n",
    "        \n",
    "    return pd.DataFrame(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDf(df, name):\n",
    "    path = '../../Tables/'\n",
    "    path += name + '.h5'\n",
    "    df.to_hdf(path, key='df', index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok = tokenize(data, 'Diagnoses')\n",
    "#dic, mat = prepare_corpus(tok)\n",
    "#x = dic.cfs\n",
    "#sorted_dic = {dic.get(k): v for k, v in sorted(x.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted_words_dic = {dic.get(k): v for k, v in sorted(dic.cfs.items(), key=lambda item: item[1], reverse=True)}\n",
    "#dic.token2id['cancer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum([sum(map((1).__eq__, sorted_words_dic.values())),sum(map((2).__eq__, sorted_words_dic.values())),sum(map((3).__eq__, sorted_words_dic.values())),sum(map((4).__eq__, sorted_words_dic.values())),sum(map((5).__eq__, sorted_words_dic.values())),sum(map((6).__eq__, sorted_words_dic.values())),sum(map((7).__eq__, sorted_words_dic.values())),sum(map((8).__eq__, sorted_words_dic.values())),sum(map((9).__eq__, sorted_words_dic.values())),sum(map((10).__eq__, sorted_words_dic.values()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x,y = zip(*sorted_dic.items()) # unpack a list of pairs into two tuples\n",
    "#plt.plot(y)\n",
    "#plt.xlim(0,40)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,stop,step=2,6,1\n",
    "tok = tokenize(data, 'Diagnoses')\n",
    "\n",
    "plot_graph(tok,start,stop,step) ## Hallar y graficar el número de tópicos óptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenize(data, 'Diagnoses')\n",
    "model = create_gensim_lsa_model(tok,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenize(data, 'Diagnoses')\n",
    "dic,a = prepare_corpus(tok)\n",
    "d = []\n",
    "for i in range(len(dic.keys())):\n",
    "    d.append(dic.get(i))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = loadTable('testBoW')\n",
    "\n",
    "pd.DataFrame(model.get_topics(), columns=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildDocTopicMatrix(model, data, 'Diagnoses', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = buildDocTopicMatrix(model, data, 'Diagnoses', 4)\n",
    "b = np.zeros((4,5))\n",
    "for i in a.values.T:\n",
    "    i = np.argsort(i)[::-1]\n",
    "    i = i[:5]\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Diagnoses'][8])\n",
    "model.print_topic(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
