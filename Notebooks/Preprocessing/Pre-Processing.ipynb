{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In any Machine Learning process, Data Preprocessing is that step in which the data gets transformed, or Encoded, to bring it to such a state that now the machine can easily parse it. In other words, the features of the data can now be easily interpreted by the algorithm.\" - https://towardsdatascience.com/data-preprocessing-concepts-fa946d11c825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Juan\n",
      "[nltk_data]     José\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re # Expresiones regulares\n",
    "from sklearn import preprocessing # LabelEncoder\n",
    "\n",
    "###############################StopWords#####################################\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Se debe descargar el conjunto de 'Stop Words' la primera vez\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Se carga la librería de lematización\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Se carga la librería de stemming y se inicializa el stemmer\n",
    "from nltk import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "###############################StopWords#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table(\"../Dataset/labeledEligibilitySample1M.csv\", header = None)\n",
    "uniqueInterventions = 0\n",
    "uniqueDiagnoses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeRandomSamples(n):\n",
    "    a = data[data[0] == \"__label__0\"].sample(n)\n",
    "    b = data[data[0] == \"__label__1\"].sample(n)\n",
    "    \n",
    "    return pd.concat([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrame(table, n, specialCharacters=True):\n",
    "    df = pd.DataFrame(np.array(table).reshape(n*2,2), columns=['Label', 'Description'])\n",
    "    \n",
    "    df['Interventions'], df['Diagnoses'] = df['Description'].str.split('.', 1).str\n",
    "    df['Eligible'] = df['Label'].str.extract('(\\d)', expand=True)\n",
    "\n",
    "    df = df.drop(['Description'], axis=1)\n",
    "    df = df.drop(['Label'], axis=1)\n",
    "    \n",
    "    df['Interventions'] = df['Interventions'].str.replace(\"study interventions are\\s\", \"\")\n",
    "    \n",
    "    # Se transforma todo a minúsculas\n",
    "    df['Interventions'] = df['Interventions'].str.lower() ##Todo a minúscula\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.lower()     ##Todo a minúscula\n",
    "\n",
    "    #Se reemplazan vocales con tildes en ambas variables\n",
    "    df['Interventions'] = df['Interventions'].str.replace('[áäâà]', 'a', regex=True)\n",
    "    df['Interventions'] = df['Interventions'].str.replace('[éêèë]', 'a', regex=True)\n",
    "    df['Interventions'] = df['Interventions'].str.replace('[íïìî]', 'a', regex=True)\n",
    "    df['Interventions'] = df['Interventions'].str.replace('[óôòö]', 'a', regex=True)\n",
    "    df['Interventions'] = df['Interventions'].str.replace('[úûùü]', 'a', regex=True)\n",
    "\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.replace('[áäâà]', 'a', regex=True)\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.replace('[éêèë]', 'a', regex=True)\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.replace('[íïìî]', 'a', regex=True)\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.replace('[óôòö]', 'a', regex=True)\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.replace('[úûùü]', 'a', regex=True)\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.replace('^\\s', '', regex=True)\n",
    "    \n",
    "    if not specialCharacters:\n",
    "        df['Interventions'] = df['Interventions'].str.replace('[^a-zA-Z# +]', '', regex=True)\n",
    "        df['Diagnoses'] = df['Diagnoses'].str.replace('[^a-zA-Z# +]', '', regex=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueInterventions(df):\n",
    "    return len(np.unique(df['Interventions'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueInterventions(df):\n",
    "    return len(np.unique(df['Diagnoses'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInterventionsCount(df):\n",
    "    return df['Interventions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDiagnosesCount(df):\n",
    "    return df['Diagnoses'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformInterventions(df):\n",
    "    labels = np.unique(df['Interventions'])\n",
    "\n",
    "    lb_interventions = preprocessing.LabelEncoder()\n",
    "    lb_interventions.fit(labels)\n",
    "    df['Interventions'] = lb_interventions.transform(df['Interventions']) \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(df, column):\n",
    "    samples = df[column]\n",
    "\n",
    "    samples = [next(nlp(sample).sents).lemma_ for sample in samples]\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(df, column, stemmer='porter'):\n",
    "    samples = df[column]\n",
    "    \n",
    "    if stemmer = 'snowball':\n",
    "        samples = [snowball.stem(sample) for sample in samples]\n",
    "    elif stemmer = 'lancaster':\n",
    "        samples = [lancaster.stem(sample) for sample in samples]\n",
    "    else: \n",
    "        samples = [porter.stem(sample) for sample in samples]\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, column):\n",
    "    return df[column].fillna('').apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(tokens):\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(df, column):\n",
    "    samples = df[column]\n",
    "    stop_words = stopwords.words('english')\n",
    "     \n",
    "    tokens = tokenize(df, column)\n",
    "    samples = tokens.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        samples[i] = detokenize(sample)\n",
    "        \n",
    "    return samples\n",
    "        \n",
    "    #return tokens.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interventions</th>\n",
       "      <th>Diagnoses</th>\n",
       "      <th>Eligible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>afatinib</td>\n",
       "      <td>head neck neoplasm diagnosi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>pv</td>\n",
       "      <td>stage iv squamous cell carcinoma hypopharynx d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>lenalidomid</td>\n",
       "      <td>follicular lymphoma diagnosis man must also co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>antibodies monoclon</td>\n",
       "      <td>stage iv pancreatic cancer diagnosis currently...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>docetaxel</td>\n",
       "      <td>sarcoma diagnosis patient must great equal two...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>bevacizumab</td>\n",
       "      <td>renal cancer diagnosis -pron- qualify trial st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>pioglitazon</td>\n",
       "      <td>advanced melanoma diagnosis know time entry ga...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>pembrolizumab</td>\n",
       "      <td>stage ivb hepatocellular carcinoma diagnosis p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>cisplatin</td>\n",
       "      <td>metastatic malignant neoplasm lung diagnosis d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>cyclophosphamid</td>\n",
       "      <td>colorectal cancer diagnosis patient know sensi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Interventions                                          Diagnoses  \\\n",
       "0             afatinib                        head neck neoplasm diagnosi   \n",
       "1                   pv  stage iv squamous cell carcinoma hypopharynx d...   \n",
       "2          lenalidomid  follicular lymphoma diagnosis man must also co...   \n",
       "3  antibodies monoclon  stage iv pancreatic cancer diagnosis currently...   \n",
       "4            docetaxel  sarcoma diagnosis patient must great equal two...   \n",
       "5          bevacizumab  renal cancer diagnosis -pron- qualify trial st...   \n",
       "6          pioglitazon  advanced melanoma diagnosis know time entry ga...   \n",
       "7        pembrolizumab  stage ivb hepatocellular carcinoma diagnosis p...   \n",
       "8            cisplatin  metastatic malignant neoplasm lung diagnosis d...   \n",
       "9      cyclophosphamid  colorectal cancer diagnosis patient know sensi...   \n",
       "\n",
       "  Eligible  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = takeRandomSamples(5)\n",
    "df = createDataFrame(table, 5, specialCharacters = False)\n",
    "\n",
    "df['Interventions'] = lemmatize(df, 'Interventions')\n",
    "df['Diagnoses'] = lemmatize(df, 'Diagnoses')\n",
    "\n",
    "df['Interventions'] = stem(df, 'Interventions')\n",
    "df['Diagnoses'] = stem(df, 'Diagnoses')\n",
    "\n",
    "\n",
    "df['Interventions'] = removeStopWords(df, 'Interventions')\n",
    "df['Diagnoses'] = removeStopWords(df, 'Diagnoses')\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
