{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In any Machine Learning process, Data Preprocessing is that step in which the data gets transformed, or Encoded, to bring it to such a state that now the machine can easily parse it. In other words, the features of the data can now be easily interpreted by the algorithm.\" - https://towardsdatascience.com/data-preprocessing-concepts-fa946d11c825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Juan\n",
      "[nltk_data]     José\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re # Expresiones regulares\n",
    "from sklearn import preprocessing # LabelEncoder\n",
    "\n",
    "###############################StopWords#####################################\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Se debe descargar el conjunto de 'Stop Words' la primera vez\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Se carga la librería de lematización\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Se carga la librería de stemming y se inicializa el stemmer\n",
    "from nltk import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "###############################StopWords#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table(\"../../Dataset/labeledEligibilitySample1M.csv\", header = None)\n",
    "uniqueInterventions = 0\n",
    "uniqueDiagnoses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeRandomSamples(n):\n",
    "    a = data[data[0] == \"__label__0\"].sample(n)\n",
    "    b = data[data[0] == \"__label__1\"].sample(n)\n",
    "    \n",
    "    return pd.concat([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrame(table, n, specialCharacters=True):\n",
    "    df = pd.DataFrame(np.array(table).reshape(n*2,2), columns=['Label', 'Description'])\n",
    "    \n",
    "    df['Interventions'], df['Diagnoses'] = df['Description'].str.split('.', 1).str\n",
    "    df['Eligible'] = df['Label'].str.extract('(\\d)', expand=True)\n",
    "\n",
    "    df = df.drop(['Description'], axis=1)\n",
    "    df = df.drop(['Label'], axis=1)\n",
    "    \n",
    "    df['Interventions'] = df['Interventions'].str.replace(\"study interventions are\\s\", \"\")\n",
    "    \n",
    "    # Se transforma todo a minúsculas\n",
    "    df['Interventions'] = df['Interventions'].str.lower() ##Todo a minúscula\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.lower()     ##Todo a minúscula\n",
    "\n",
    "    #Se reemplazan vocales con tildes en ambas variables\n",
    "    df['Interventions'] = df['Interventions'].str.replace('[áäâà]', 'a', regex=True)\n",
    "    df['Interventions'] = df['Interventions'].str.replace('[éêèë]', 'a', regex=True)\n",
    "    df['Interventions'] = df['Interventions'].str.replace('[íïìî]', 'a', regex=True)\n",
    "    df['Interventions'] = df['Interventions'].str.replace('[óôòö]', 'a', regex=True)\n",
    "    df['Interventions'] = df['Interventions'].str.replace('[úûùü]', 'a', regex=True)\n",
    "\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.replace('[áäâà]', 'a', regex=True)\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.replace('[éêèë]', 'a', regex=True)\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.replace('[íïìî]', 'a', regex=True)\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.replace('[óôòö]', 'a', regex=True)\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.replace('[úûùü]', 'a', regex=True)\n",
    "    df['Diagnoses'] = df['Diagnoses'].str.replace('^\\s', '', regex=True)\n",
    "    \n",
    "    if not specialCharacters:\n",
    "        df['Interventions'] = df['Interventions'].str.replace('[^a-zA-Z# +]', '', regex=True)\n",
    "        df['Diagnoses'] = df['Diagnoses'].str.replace('[^a-zA-Z# +]', '', regex=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueInterventions(df):\n",
    "    return len(np.unique(df['Interventions'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueInterventions(df):\n",
    "    return len(np.unique(df['Diagnoses'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInterventionsCount(df):\n",
    "    return df['Interventions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDiagnosesCount(df):\n",
    "    return df['Diagnoses'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformInterventions(df):\n",
    "    labels = np.unique(df['Interventions'])\n",
    "\n",
    "    lb_interventions = preprocessing.LabelEncoder()\n",
    "    lb_interventions.fit(labels)\n",
    "    df['Interventions'] = lb_interventions.transform(df['Interventions']) \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinColumns(df):\n",
    "    df['Diagnoses'] = df['Interventions'] + df['Diagnoses']\n",
    "    df = df.drop(columns=[\"Interventions\",\"Eligible\"])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(df, column):\n",
    "    samples = df[column]\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        nlp_sample = nlp(sample)\n",
    "        for next_sample in nlp_sample:\n",
    "            sentence_sample = next_sample\n",
    "            lemma = sentence_sample.lemma_\n",
    "            samples[i] = sample\n",
    "    #samples = [next(nlp(sample).sents).lemma_ for sample in samples]\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(df, column, stemmer='porter'):\n",
    "    samples = df[column]\n",
    "    \n",
    "    if stemmer == 'snowball':\n",
    "        samples = [snowball.stem(sample) for sample in samples]\n",
    "    elif stemmer == 'lancaster':\n",
    "        samples = [lancaster.stem(sample) for sample in samples]\n",
    "    else: \n",
    "        samples = [porter.stem(sample) for sample in samples]\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, column):\n",
    "    return df[column].fillna('').apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(tokens):\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(df, column):\n",
    "    samples = df[column]\n",
    "    stop_words = stopwords.words('english')\n",
    "     \n",
    "    tokens = tokenize(df, column)\n",
    "    samples = tokens.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        samples[i] = detokenize(sample)\n",
    "        \n",
    "    return samples\n",
    "        \n",
    "    #return tokens.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToCSV(df, name):\n",
    "    path = '../../Dataset/'\n",
    "    path += name + '.csv'\n",
    "    df.to_csv(path, index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "table = takeRandomSamples(5000)\n",
    "df = createDataFrame(table, 5000, specialCharacters = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interventions</th>\n",
       "      <th>Diagnoses</th>\n",
       "      <th>Eligible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>adjuvant therapy</td>\n",
       "      <td>breast cancer diagnosis and ecog performance s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>dacarbazine</td>\n",
       "      <td>childhood central nervous system mixed germ ce...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bortezomib</td>\n",
       "      <td>unspecified adult solid tumor protocol specifi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>antibodies monoclonal</td>\n",
       "      <td>unspecified adult solid tumor protocol specifi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>isophosphamide mustard</td>\n",
       "      <td>fibrohistiocytic neoplasm diagnosis and inelig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>cyclosporine</td>\n",
       "      <td>recurrent childhood lymphoblastic lymphoma dia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>albuminbound paclitaxel</td>\n",
       "      <td>stage iv adenoid cystic carcinoma of the oral ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>calcium dietary</td>\n",
       "      <td>adenomas diagnosis and use of phenytoin in the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>bevacizumab</td>\n",
       "      <td>patients may have received previous adjuvant c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>thalidomide</td>\n",
       "      <td>large cell lymphoma arising in kshv associated...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Interventions  \\\n",
       "0            adjuvant therapy    \n",
       "1                 dacarbazine    \n",
       "2                  bortezomib    \n",
       "3       antibodies monoclonal    \n",
       "4      isophosphamide mustard    \n",
       "...                        ...   \n",
       "9995             cyclosporine    \n",
       "9996  albuminbound paclitaxel    \n",
       "9997          calcium dietary    \n",
       "9998              bevacizumab    \n",
       "9999              thalidomide    \n",
       "\n",
       "                                              Diagnoses Eligible  \n",
       "0     breast cancer diagnosis and ecog performance s...        0  \n",
       "1     childhood central nervous system mixed germ ce...        0  \n",
       "2     unspecified adult solid tumor protocol specifi...        0  \n",
       "3     unspecified adult solid tumor protocol specifi...        0  \n",
       "4     fibrohistiocytic neoplasm diagnosis and inelig...        0  \n",
       "...                                                 ...      ...  \n",
       "9995  recurrent childhood lymphoblastic lymphoma dia...        1  \n",
       "9996  stage iv adenoid cystic carcinoma of the oral ...        1  \n",
       "9997  adenomas diagnosis and use of phenytoin in the...        1  \n",
       "9998  patients may have received previous adjuvant c...        1  \n",
       "9999  large cell lymphoma arising in kshv associated...        1  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinColumns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Diagnoses'] = lemmatize(df, 'Diagnoses')\n",
    "\n",
    "df['Diagnoses'] = stem(df, 'Diagnoses')\n",
    "\n",
    "df['Diagnoses'] = removeStopWords(df, 'Diagnoses')\n",
    "\n",
    "saveToCSV(df, '10k_1Col_NoCarEsp_LSA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
